{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interim-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "framed-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(annotation_file, shuffle=True):\n",
    "    with open(annotation_file) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(int(time.time()))\n",
    "        np.random.shuffle(lines)\n",
    "        #np.random.seed(None)\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_model(model_path):\n",
    "    # support of tflite model\n",
    "    if model_path.endswith('.tflite'):\n",
    "        from tensorflow.lite.python import interpreter as interpreter_wrapper\n",
    "        model = interpreter_wrapper.Interpreter(model_path=model_path)\n",
    "        model.allocate_tensors()\n",
    "        model_format = 'TFLITE'\n",
    "\n",
    "    # normal keras h5 model\n",
    "    elif model_path.endswith('.h5'):\n",
    "        custom_object_dict = get_custom_objects()\n",
    "\n",
    "        model = load_model(model_path, compile=False, custom_objects=custom_object_dict)\n",
    "        model_format = 'H5'\n",
    "        K.set_learning_phase(0)\n",
    "    else:\n",
    "        raise ValueError('invalid model file')\n",
    "\n",
    "    return model, model_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "purple-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_parse(annotation_lines, class_names):\n",
    "    '''\n",
    "    parse annotation lines to get image dict and ground truth class dict\n",
    "\n",
    "    image dict would be like:\n",
    "    annotation_records = {\n",
    "        '/path/to/000001.jpg': {'100,120,200,235':'dog', '85,63,156,128':'car', ...},\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    ground truth class dict would be like:\n",
    "    classes_records = {\n",
    "        'car': [\n",
    "                ['000001.jpg','100,120,200,235'],\n",
    "                ['000002.jpg','85,63,156,128'],\n",
    "                ...\n",
    "               ],\n",
    "        ...\n",
    "    }\n",
    "    '''\n",
    "    annotation_records = OrderedDict()\n",
    "    classes_records = OrderedDict({class_name: [] for class_name in class_names})\n",
    "\n",
    "    for line in annotation_lines:\n",
    "        box_records = {}\n",
    "        image_name = line.split(' ')[0]\n",
    "        boxes = line.split(' ')[1:]\n",
    "        for box in boxes:\n",
    "            # strip box coordinate and class\n",
    "            class_name = class_names[int(box.split(',')[-1])]\n",
    "            coordinate = ','.join(box.split(',')[:-1])\n",
    "            box_records[coordinate] = class_name\n",
    "            # append or add ground truth class item\n",
    "            record = [os.path.basename(image_name), coordinate]\n",
    "            if class_name in classes_records:\n",
    "                classes_records[class_name].append(record)\n",
    "            else:\n",
    "                classes_records[class_name] = list([record])\n",
    "        annotation_records[image_name] = box_records\n",
    "\n",
    "    return annotation_records, classes_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_predict_keras(model, image, anchors, num_classes, model_image_size, conf_threshold, elim_grid_sense,\n",
    "                       v5_decode):\n",
    "    image_data = preprocess_image(image, model_image_size)\n",
    "    # origin image shape, in (height, width) format\n",
    "    image_shape = tuple(reversed(image.size))\n",
    "\n",
    "    prediction = model.predict([image_data])\n",
    "    if len(anchors) == 5:\n",
    "        # YOLOv2 use 5 anchors\n",
    "        pred_boxes, pred_classes, pred_scores = yolo2_postprocess_np(prediction, image_shape, anchors, num_classes,\n",
    "                                                                     model_image_size, max_boxes=100,\n",
    "                                                                     confidence=conf_threshold,\n",
    "                                                                     elim_grid_sense=elim_grid_sense)\n",
    "    else:\n",
    "        if v5_decode:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo5_postprocess_np(prediction, image_shape, anchors, num_classes,\n",
    "                                                                         model_image_size, max_boxes=100,\n",
    "                                                                         confidence=conf_threshold,\n",
    "                                                                         elim_grid_sense=True)  # enable \"elim_grid_sense\" by default\n",
    "        else:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo3_postprocess_np(prediction, image_shape, anchors, num_classes,\n",
    "                                                                         model_image_size, max_boxes=100,\n",
    "                                                                         confidence=conf_threshold,\n",
    "                                                                         elim_grid_sense=elim_grid_sense)\n",
    "\n",
    "    return pred_boxes, pred_classes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_class_records(model, model_format, annotation_records, anchors, class_names, model_image_size,\n",
    "                                 conf_threshold, elim_grid_sense, v5_decode, save_result):\n",
    "    '''\n",
    "    Do the predict with YOLO model on annotation images to get predict class dict\n",
    "\n",
    "    predict class dict would contain image_name, coordinary and score, and\n",
    "    sorted by score:\n",
    "    pred_classes_records = {\n",
    "        'car': [\n",
    "                ['000001.jpg','94,115,203,232',0.98],\n",
    "                ['000002.jpg','82,64,154,128',0.93],\n",
    "                ...\n",
    "               ],\n",
    "        ...\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    # create txt file to save prediction result, with\n",
    "    # save format as annotation file but adding score, like:\n",
    "    #\n",
    "    # path/to/img1.jpg 50,100,150,200,0,0.86 30,50,200,120,3,0.95\n",
    "    #\n",
    "    os.makedirs('result', exist_ok=True)\n",
    "    result_file = open(os.path.join('result', 'detection_result.txt'), 'w')\n",
    "\n",
    "    pred_classes_records = OrderedDict()\n",
    "    pbar = tqdm(total=len(annotation_records), desc='Eval model')\n",
    "    for (image_name, gt_records) in annotation_records.items():\n",
    "        image = Image.open(image_name)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        image_array = np.array(image, dtype='uint8')\n",
    "\n",
    "        # normal keras h5 model\n",
    "        if model_format == 'H5':\n",
    "            pred_boxes, pred_classes, pred_scores = yolo_predict_keras(model, image, anchors, len(class_names),\n",
    "                                                                       model_image_size, conf_threshold,\n",
    "                                                                       elim_grid_sense, v5_decode)\n",
    "        else:\n",
    "            raise ValueError('invalid model format')\n",
    "\n",
    "        # print('Found {} boxes for {}'.format(len(pred_boxes), image_name))\n",
    "        pbar.update(1)\n",
    "\n",
    "        # save prediction result to txt\n",
    "        result_file.write(image_name)\n",
    "        for box, cls, score in zip(pred_boxes, pred_classes, pred_scores):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            box_annotation = \" %d,%d,%d,%d,%d,%f\" % (\n",
    "                xmin, ymin, xmax, ymax, cls, score)\n",
    "            result_file.write(box_annotation)\n",
    "        result_file.write('\\n')\n",
    "        result_file.flush()\n",
    "\n",
    "        if save_result:\n",
    "\n",
    "            gt_boxes, gt_classes, gt_scores = transform_gt_record(gt_records, class_names)\n",
    "\n",
    "            result_dir = os.path.join('result', 'detection')\n",
    "            os.makedirs(result_dir, exist_ok=True)\n",
    "            colors = get_colors(class_names)\n",
    "            image_array = draw_boxes(image_array, gt_boxes, gt_classes, gt_scores, class_names, colors=None,\n",
    "                                     show_score=False)\n",
    "            image_array = draw_boxes(image_array, pred_boxes, pred_classes, pred_scores, class_names, colors)\n",
    "            image = Image.fromarray(image_array)\n",
    "            # here we handle the RGBA image\n",
    "            if (len(image.split()) == 4):\n",
    "                r, g, b, a = image.split()\n",
    "                image = Image.merge(\"RGB\", (r, g, b))\n",
    "            image.save(os.path.join(result_dir, image_name.split(os.path.sep)[-1]))\n",
    "\n",
    "        # Nothing detected\n",
    "        if pred_boxes is None or len(pred_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        for box, cls, score in zip(pred_boxes, pred_classes, pred_scores):\n",
    "            pred_class_name = class_names[cls]\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            coordinate = \"{},{},{},{}\".format(xmin, ymin, xmax, ymax)\n",
    "\n",
    "            # append or add predict class item\n",
    "            record = [os.path.basename(image_name), coordinate, score]\n",
    "            if pred_class_name in pred_classes_records:\n",
    "                pred_classes_records[pred_class_name].append(record)\n",
    "            else:\n",
    "                pred_classes_records[pred_class_name] = list([record])\n",
    "\n",
    "    # sort pred_classes_records for each class according to score\n",
    "    for pred_class_list in pred_classes_records.values():\n",
    "        pred_class_list.sort(key=lambda ele: ele[2], reverse=True)\n",
    "\n",
    "    pbar.close()\n",
    "    result_file.close()\n",
    "    return pred_classes_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comparative-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(annotation_file, shuffle=True):\n",
    "    with open(annotation_file) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(int(time.time()))\n",
    "        np.random.shuffle(lines)\n",
    "        #np.random.seed(None)\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_AP(model, model_format, annotation_lines, anchors, class_names, model_image_size, eval_type, iou_threshold,\n",
    "            conf_threshold, elim_grid_sense, v5_decode, save_result, class_filter=None):\n",
    "    '''\n",
    "    Compute AP for detection model on annotation dataset\n",
    "    '''\n",
    "    annotation_records, gt_classes_records = annotation_parse(annotation_lines, class_names)\n",
    "    pred_classes_records = get_prediction_class_records(model, model_format, annotation_records, anchors, class_names,\n",
    "                                                        model_image_size, conf_threshold, elim_grid_sense, v5_decode,\n",
    "                                                        save_result)\n",
    "    AP = 0.0\n",
    "\n",
    "    if eval_type == 'VOC':\n",
    "        AP, APs = compute_mAP_PascalVOC(annotation_records, gt_classes_records, pred_classes_records, class_names,\n",
    "                                        iou_threshold)\n",
    "\n",
    "        if class_filter is not None:\n",
    "            get_filter_class_mAP(APs, class_filter)\n",
    "\n",
    "    elif eval_type == 'COCO':\n",
    "        AP, _ = compute_AP_COCO(annotation_records, gt_classes_records, pred_classes_records, class_names, class_filter)\n",
    "        # get AP for different scale: small, medium, large\n",
    "        scale_gt_classes_records = get_scale_gt_dict(gt_classes_records, class_names)\n",
    "        compute_AP_COCO_Scale(annotation_records, scale_gt_classes_records, pred_classes_records, class_names)\n",
    "    else:\n",
    "        raise ValueError('Unsupported evaluation type')\n",
    "\n",
    "    return AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "architectural-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('result', exist_ok=True)\n",
    "result_file = open(os.path.join('result', 'detection_result.txt'), 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sweet-swift",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\xia\\\\Documents\\\\datasets\\\\VOCdevkit/VOC2007/JPEGImages/000001.jpg 48,240,195,371,11 8,12,352,498,14',\n",
       " 'C:\\\\Users\\\\xia\\\\Documents\\\\datasets\\\\VOCdevkit/VOC2007/JPEGImages/000002.jpg 139,200,207,301,18']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_lines = get_dataset('2007_test.txt', shuffle=False)\n",
    "annotation_lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "northern-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "annotation_records, gt_classes_records = annotation_parse(annotation_lines,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ordinary-protein",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval model:   0%|                                                                             | 0/4952 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pbar = tqdm(total=len(annotation_records), desc='Eval model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dirty-appraisal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xia\\Documents\\datasets\\VOCdevkit/VOC2007/JPEGImages/000001.jpg\n",
      "[[[[0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   ...\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]]\n",
      "\n",
      "  [[0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   ...\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]]\n",
      "\n",
      "  [[0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   ...\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   ...\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]]\n",
      "\n",
      "  [[0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   ...\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]]\n",
      "\n",
      "  [[0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   ...\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]\n",
      "   [0.5019608 0.5019608 0.5019608]]]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "model_image_size = (416,416)\n",
    "\n",
    "for (image_name, gt_records) in annotation_records.items():\n",
    "    print(image_name)\n",
    "    image = Image.open(image_name)\n",
    "    if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "#     image.show()\n",
    "    image_array = np.array(image, dtype='uint8')\n",
    "    image_data = preprocess_image(image, model_image_size)\n",
    "    image_shape = tuple(reversed(image.size))\n",
    "    print(image_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "exact-trust",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval model:   0%|                                                                | 1/4952 [04:11<346:14:43, 251.76s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "negative-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "approximate-addition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 416)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_image_size = (416,416)\n",
    "tuple(reversed(model_image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "subtle-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Concatenate, MaxPooling2D, BatchNormalization, Activation, UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "def mish(x):\n",
    "    return x * K.tanh(K.softplus(x))\n",
    "def get_custom_objects():\n",
    "    '''\n",
    "    form up a custom_objects dict so that the customized\n",
    "    layer/function call could be correctly parsed when keras\n",
    "    .h5 model is loading or converting\n",
    "    '''\n",
    "    custom_objects_dict = {\n",
    "        'tf': tf,\n",
    "        'mish': mish\n",
    "    }\n",
    "\n",
    "    return custom_objects_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "several-express",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-45-29c25056caf7>:6: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\n",
      "Instructions for updating:\n",
      "Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "custom_object_dict = get_custom_objects()\n",
    "model_path = r'C:\\Users\\xia\\Documents\\codes\\20210403_目标检测\\target_detection\\2_yolov4\\yolov4_pycharm\\weights\\yolov4.h5'\n",
    "model = load_model(model_path, compile=False, custom_objects=custom_object_dict)\n",
    "model_format = 'H5'\n",
    "K.set_learning_phase(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "virtual-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict([image_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "opened-hospital",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-c0bcebfb69c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-romania",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
