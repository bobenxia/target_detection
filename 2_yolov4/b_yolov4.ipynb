{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "large-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Ipynb_importer\n",
    "from a_CSPdarknet53 import DarknetConv2D, darknet_body,compose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "speaking-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import (Add, BatchNormalization, Concatenate,\n",
    "                                     Conv2D, LeakyReLU, MaxPooling2D,\n",
    "                                     UpSampling2D, ZeroPadding2D)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-cotton",
   "metadata": {},
   "source": [
    "### 定义CBL 基本模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appointed-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DarknetConv2D_BN_LeakyReLU(*args, **kwargs):\n",
    "    \"\"\"Darknet Convolution2D followed by BatchNormalization and LeakyReLU.\"\"\"\n",
    "    \n",
    "    no_bias_kwargs = {'use_bias': False}  # 没懂为啥用 no_bias\n",
    "    no_bias_kwargs.update(kwargs)\n",
    "    return compose(\n",
    "        DarknetConv2D(*args, **no_bias_kwargs),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-qatar",
   "metadata": {},
   "source": [
    "### 定义 yolo 主体\n",
    "<img src=\"https://pic3.zhimg.com/80/v2-5251e9c0784871a37c693d53f7d57f92_1440w.jpg\" alt=\"img\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_body(inputs, num_anchors, num_classes):\n",
    "    # 1、通过CSPdarknet_body 获得三个有效特征层\n",
    "    feat1, feat2, feat3 = darknet_body(inputs)\n",
    "    \n",
    "    # 2、获得 p5、p4、p3\n",
    "    P5 = DarknetConv2D_BN_Leaky(512, (1, 1))(feat3)\n",
    "    P4 = DarknetConv2D_BN_Leaky(256, (1,1))(feat2)\n",
    "    P3 = DarknetConv2D_BN_Leaky(128, (1,1))(feat1)\n",
    "    \n",
    "    # 3.1、p5 neck部分处理\n",
    "    P5 = DarknetConv2D_BN_Leaky(1024, (3, 3))(P5)\n",
    "    P5 = DarknetConv2D_BN_Leaky(512, (1, 1))(P5)\n",
    "    # 使用了SPP结构，即不同尺度的最大池化后堆叠。\n",
    "    maxpool1 = MaxPooling2D(pool_size=(13, 13), strides=(1, 1), padding='same')(P5)\n",
    "    maxpool2 = MaxPooling2D(pool_size=(9, 9), strides=(1, 1), padding='same')(P5)\n",
    "    maxpool3 = MaxPooling2D(pool_size=(5, 5), strides=(1, 1), padding='same')(P5)\n",
    "    P5 = Concatenate()([maxpool1, maxpool2, maxpool3, P5])\n",
    "    P5 = DarknetConv2D_BN_Leaky(512, (1, 1))(P5)\n",
    "    P5 = DarknetConv2D_BN_Leaky(1024, (3, 3))(P5)\n",
    "    P5 = DarknetConv2D_BN_Leaky(512, (1, 1))(P5)\n",
    "    \n",
    "    # 3.2、p5 上采样与p4 实现 pan 结构\n",
    "    P5_upsample = compose(DarknetConv2D_BN_Leaky(256, (1,1)), UpSampling2D(2))(P5)\n",
    "    P4 = Concatenate()([P4, P5_upsample])\n",
    "    \n",
    "    # 4.1、p4 neck 部分处理\n",
    "    P4 = make_five_convs(P4,256)\n",
    "    \n",
    "    # 4.2、p4 上采样与p3 实现 pan 结构\n",
    "    P4_upsample = compose(DarknetConv2D_BN_Leaky(128, (1,1)), UpSampling2D(2))(P4)\n",
    "    P3 = Concatenate()([P3, P4_upsample])\n",
    "    \n",
    "    # 5、p3 输出\n",
    "    P3 = make_five_convs(P3,128)\n",
    "    P3_output = DarknetConv2D_BN_Leaky(256, (3,3))(P3)\n",
    "    P3_output = DarknetConv2D(num_anchors*(num_classes+5), (1,1), kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01))(P3_output)\n",
    "\n",
    "    # 6、p4 输出\n",
    "    P3_downsample = ZeroPadding2D(((1,0),(1,0)))(P3)\n",
    "    P3_downsample = DarknetConv2D_BN_Leaky(256, (3,3), strides=(2,2))(P3_downsample)\n",
    "    P4 = Concatenate()([P3_downsample, P4])\n",
    "    P4 = make_five_convs(P4,256)\n",
    "    P4_output = DarknetConv2D_BN_Leaky(512, (3,3))(P4)\n",
    "    P4_output = DarknetConv2D(num_anchors*(num_classes+5), (1,1), kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01))(P4_output)\n",
    "    \n",
    "    # 7、p5输出\n",
    "    P4_downsample = ZeroPadding2D(((1,0),(1,0)))(P4)\n",
    "    P4_downsample = DarknetConv2D_BN_Leaky(512, (3,3), strides=(2,2))(P4_downsample)\n",
    "    P5 = Concatenate()([P4_downsample, P5])\n",
    "    P5 = make_five_convs(P5,512)\n",
    "    P5_output = DarknetConv2D_BN_Leaky(1024, (3,3))(P5)\n",
    "    P5_output = DarknetConv2D(num_anchors*(num_classes+5), (1,1), kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.01))(P5_output)\n",
    "\n",
    "    return Model(inputs, [P5_output, P4_output, P3_output])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
