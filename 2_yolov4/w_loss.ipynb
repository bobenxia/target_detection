{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6a8c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c_yolov4_head_keras.ipynb\n",
      "importing Jupyter notebook from a_csp_darknet53.ipynb\n",
      "importing Jupyter notebook from z_layers.ipynb\n",
      "importing Jupyter notebook from b_yolov4_neck_and_body.ipynb\n"
     ]
    }
   ],
   "source": [
    "import Ipynb_importer\n",
    "from c_yolov4_head_keras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "643333f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _smooth_labels(y_true, label_smoothing):\n",
    "    label_smoothing = K.constant(label_smoothing, dtype=K.floatx)\n",
    "    return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bdf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_focal_loss(y_true, y_pred, gamma=2.0 alpha=0.25):\n",
    "    \"\"\"\n",
    "    Compute sigmoid focal loss.\n",
    "    Reference Paper:\n",
    "        \"Focal Loss for Dense Object Detection\"\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "        \n",
    "    \"\"\"\n",
    "    sigmoid_loss = K.binary_crossentropy(y_true, y_pred, from_logits=True)\n",
    "    \n",
    "    pred_prob = tf.sigmoid(y_pred)\n",
    "    p_t = ((y_true * pred_prob) + ((1 - y_true) * (1 - pred_prob)))\n",
    "    modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "    alpha_weight_factor = (y_true * alpha + (1 - y_true) * (1 - alpha))\n",
    "\n",
    "    sigmoid_focal_loss = modulating_factor * alpha_weight_factor * sigmoid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ed43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(b1, b2):\n",
    "    \"\"\"\n",
    "    Return iou tensor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b1: tensor, shape=(i1,...,iN, 4), xywh\n",
    "    b2: tensor, shape=(j, 4), xywh\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    iou: tensor, shape=(i1,...,iN, j)\n",
    "    \"\"\"\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b1 = K.expand_dims(b1, -2)\n",
    "    b1_xy = b1[..., :2]\n",
    "    b1_wh = b1[..., 2:4]\n",
    "    b1_wh_half = b1_wh/2.\n",
    "    b1_mins = b1_xy - b1_wh_half\n",
    "    b1_maxs = b1_xy + b1_wh_half\n",
    "    \n",
    "    # Expand dim to apply broadcasting.\n",
    "    b2 = K.expand_dims(b2, 0)\n",
    "    b2_xy = b2[..., :2]\n",
    "    b2_wh = b2[..., 2:4]\n",
    "    b2_wh_half = b2_wh/2.\n",
    "    b2_mins = b2_xy - b2_wh_half\n",
    "    b2_maxes = b2_xy + b2_wh_half\n",
    "    \n",
    "    intersect_mins = K.maximun(b1_mins, b2_mins)\n",
    "    intersect_maxs =K.minimum(b1_maxs, b2_maxes)\n",
    "    intersect_wh = K.maximum(intersect_maxs - intersect_mins, 0.)\n",
    "    intersect_area = intersect_wh[0] * intersect_wh[1]\n",
    "    \n",
    "    b1_area = b1_wh[..., 0] * b1_wh[..., 1]\n",
    "    b2_area = b2_wh[..., 0] * b2_wh[..., 1]\n",
    "    \n",
    "    iou = intersect_area / (b1_area + b2_area - intersect_area)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b5ca3d",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "> https://zhuanlan.zhihu.com/p/42081893\n",
    "\n",
    "在yolo_loss 方法中\n",
    "- args是Lambda层的输入，即model_body.output和y_true的组合；\n",
    "- anchors是二维数组，结构是(9, 2)，即9个anchor box；\n",
    "- num_classes是类别数；\n",
    "- ignore_thresh是过滤阈值；\n",
    "- label_smoothing 是标签平滑\n",
    "\n",
    "在损失方法yolo_loss中，设置若干参数：\n",
    "- num_layers：层的数量，是anchors数量的3分之1；\n",
    "- yolo_outputs和y_true：分离args，前3个是yolo_outputs预测值，后3个是y_true真值；\n",
    "- anchor_mask：anchor box的索引数组，3个1组倒序排序，678对应13x13，345对应26x26，123对应52x52；即[[6, 7, 8], [3, 4, 5], [0, 1, 2]]；\n",
    "- input_shape：K.shape(yolo_outputs[0])[1:3]，第1个预测矩阵yolo_outputs[0]的结构（shape）的第1~2位，即(?, 13, 13, 18)中的(13, 13)。再x32，就是YOLO网络的输入尺寸，即(416, 416)，因为在网络中，含有5个步长为(2, 2)的卷积操作，降维32=5^2倍；\n",
    "- grid_shapes：与input_shape类似，K.shape(yolo_outputs[l])[1:3]，以列表的形式，选择3个尺寸的预测图维度，即[(13, 13), (26, 26), (52, 52)]；\n",
    "- batch_size：第1个预测图的结构的第1位，即K.shape(yolo_outputs[0])[0]，输入模型的图片总量，即批次数；\n",
    "- batch_size_f：m的float类型，即K.cast(m, K.dtype(yolo_outputs[0]))\n",
    "- loss：损失值为0；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbbbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(args, anchors，num_classes, ignore_thresh=.5, label_smoothing=0, elim_grid_sense=False, use_focal_loss=False, use_focal_obj_loss=False, use_softmax_loss=False, use_giou_loss=False, use_diou_loss=True):\n",
    "    '''\n",
    "    YOLOv3 loss function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body\n",
    "    y_true: list of array, the output of preprocess_true_boxes\n",
    "    anchors: array, shape=(N, 2), wh\n",
    "    num_classes: integer\n",
    "    ignore_thresh: float, the iou threshold whether to ignore object confidence loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss: tensor, shape=(1,)\n",
    "\n",
    "    '''\n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "    #   将预测结果和实际ground truth分开，args是[*model_body.output, *y_true]\n",
    "    #   y_true是一个列表，包含三个特征层，shape分别为(m,13,13,3,85),(m,26,26,3,85),(m,52,52,3,85)。\n",
    "    #   yolo_outputs是一个列表，包含三个特征层，shape分别为(m,13,13,3,85),(m,26,26,3,85),(m,52,52,3,85)。\n",
    "    #---------------------------------------------------------------------------------------------------#\n",
    "    num_layers = len(anchors)//3\n",
    "    y_true = args[num_layers:]\n",
    "    yolo_outputs = args[:num_layers]\n",
    "    #-----------------------------------------------------------#\n",
    "    #   13x13的特征层对应的anchor是[142, 110], [192, 243], [459, 401]\n",
    "    #   26x26的特征层对应的anchor是[36, 75], [76, 55], [72, 146]\n",
    "    #   52x52的特征层对应的anchor是[12, 16], [19, 36], [40, 28]\n",
    "    #-----------------------------------------------------------#   \n",
    "    if num_layers == 3:\n",
    "        anchor_mask = [[6,7,8], [3,4,5], [0,1,2]]\n",
    "        scale_x_y = [1.05, 1.1, 1.2] if elim_grid_sense else [None, None, None]\n",
    "    else:\n",
    "        anchor_mask = [[3,4,5], [0,1,2]]\n",
    "        scale_x_y = [1.05, 1.05] if elim_grid_sense else [None, None]\n",
    "        \n",
    "    # 得到input_shpae为416,416 \n",
    "    input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0]))\n",
    "    grid_shapes = [K.cast(K.shape(yolo_outputs[i])[1:3], K.dtype(y_true[0])) for i in range(num_layers)]  # grid_shape是指特征图shape\n",
    "    loss = 0\n",
    "    num_pos = 0\n",
    "    total_location_loss = 0\n",
    "    total_confidence_loss = 0\n",
    "    total_class_loss = 0\n",
    "    batch_size = K.shape(yolo_outputs[0])[0]  # batch size\n",
    "    batch_size_f = K.cast(batch_size, K.dtype(yolo_outputs[0]))\n",
    "    \n",
    "    # 依次计算特征图的损失值\n",
    "    for i in range(num_layers):\n",
    "        # 物体置信度和类别置信度\n",
    "        object_mask = y_true[i][..., 4:5]\n",
    "        true_class_probs = y_true[i][..., 5:]\n",
    "        # 是否使用标签平滑\n",
    "        if label_smoothing:\n",
    "            true_class_probs = _smooth_labels(true_class_probs, label_smoothing)\n",
    "            true_objectness_probs = _smooth_labels(object_mask, label_smoothing)\n",
    "        else:\n",
    "            true_objectness_probs = object_mask\n",
    "        \n",
    "        # 使用 yolo_decode 解码预测图，输出：(以 13*13 特征图举例)\n",
    "        # - 网格 gird, 结构是(13, 13, 1, 2)，数值为0~12的全遍历二元组；\n",
    "        # - 预测值 raw_pred:\n",
    "        # - pred_xy和pred_wh都是归一化后的起始点xy和宽高wh，xy的结构是(?, 13, 13, 3, 2)，wh的结构是(?, 13, 13, 3, 2)；\n",
    "        grid, raw_pred, pred_xy, pred_wh = yolo_decode(yolo_outputs[i], anchors[anchor_mask[i]],\n",
    "                                                      num_classes, input_shape, scale_x_y=scale_x_y[i], calc_loss=True)\n",
    "        pred_box = K.concatenate([pred_xy, pred_wh])\n",
    "                  \n",
    "        # Darknet raw box to calculate loss.\n",
    "        # - y_true的第0和1位是中心点xy的相对位置，范围是0~1；y_true的第2和3位是宽高wh的相对input_shape的位置，范围是0~1；\n",
    "        # - raw_true_xy: 在网络中的中心点 xy, 偏移数据，值的范围是 0~1；\n",
    "        # - raw_true_wh：在网络中的 wh 针对于 anchors 的比例，再转换为log形式，范围是有正有负；\n",
    "        # - box_loss_scale：计算 wh 权重，取值范围（1~2）；\n",
    "        raw_true_xy = y_true[i][..., :2]*grid_shapes[i][::-1] - grid\n",
    "        raw_true_wh = K.log(y_true[i][..., 2:4]/anchors[anchor_mask[i]*input_shape]* input_shape[::-1])\n",
    "        raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh)) # avoid log(0)=-inf\n",
    "        box_loss_scale = 2 - y_true[i][...,2:3]*y_true[i][...,3:4]\n",
    "        \n",
    "        # 根据ignore_thresh 生成，ignore_mask，将预测框pred_box和真值框true_box计算IoU，\n",
    "        # 抑制不需要的anchor框的值，即IoU小于最大阈值的anchor框。\n",
    "        # ignore_mask的shape是(?, ?, ?, 3, 1)，第0位是批次数，第1~2位是特征图尺寸。\n",
    "        ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True)\n",
    "        object_mask_bool = K.cast(object_mask, 'bool')\n",
    "        def loop_body(b, ignore_mask):\n",
    "            true_box = tf.boolean_mask(y_true[i][b,...,0:4], object_mask_bool[b,...,0])\n",
    "            iou = box_iou(pred_box[b], true_box)\n",
    "            best_iou = K.max(iou, axis=-1)\n",
    "            ignore_mask = ignore_mask.write(b, K.cast(best_iou<ignore_thresh, K.dtype(true_box)))\n",
    "            return b+1, ignore_mask\n",
    "        _, ignore_mask = tf.while_loop(lambda b,*args: b<batch_size, loop_body, [0, ignore_mask])\n",
    "        ignore_mask = ignore_mask.stack()\n",
    "        ignore_mask = K.expand_dims(ignore_mask, -1)\n",
    "        \n",
    "        # 第一部分损失：置信度的损失值\n",
    "        # condidence_loss\n",
    "        # 两部分组成，第一部分是存在物体的损失值，第 2 部分是不存物体的损失值，其中乘以掩码 ignore_mask,\n",
    "        # 忽略预测框中 IOU大于阈值的框\n",
    "        if use_focal_obj_loss:\n",
    "            # Focal loss for objectness confidence\n",
    "            # TODO: sigmoid_focal_loss\n",
    "            confidence_loss = sigmoid_focal_loss(true_objectness_probs, raw_pred[...,4:5])\n",
    "        else:\n",
    "            confidence_loss = object_mask * K.binary_crossentropy(true_objectness_probs, raw_pred[...,4:5], from_logits=True)+ \\\n",
    "                (1-object_mask) * K.binary_crossentropy(object_mask, raw_pred[...,4:5], from_logits=True) * ignore_mask\n",
    "            \n",
    "        # 第二部分损失：类别损失\n",
    "        # class_loss\n",
    "        if use_focal_loss:\n",
    "            # Focal loss for classification score\n",
    "            if use_softmax_loss:\n",
    "                # TODO: softmax_focal_loss\n",
    "                class_loss = softmax_focal_loss(true_class_probs, raw_pred[..., 5:])\n",
    "            else:\n",
    "                class_loss = sigmoid_focal_loss(true_class_probs, raw_pred[...,5:])\n",
    "        else:\n",
    "            if use_softmax_loss:\n",
    "                # use softmax style classification output\n",
    "                class_loss = object_mask * K.expand_dims(K.categorical_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True), axis=-1)\n",
    "            else:\n",
    "                # use sigmoid style classification output\n",
    "                class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[...,5:], from_logits=True)\n",
    "\n",
    "        # 第三部分损失：定位损失\n",
    "        # location_loss\n",
    "        if use_giou_loss:\n",
    "            # Calculate GIoU loss as location loss\n",
    "            raw_true_box = y_true[i][...,0:4]\n",
    "            # TODO: box_giou\n",
    "            giou = box_giou(raw_true_box, pred_box)\n",
    "            giou_loss = object_mask * box_loss_scale * (1 - giou)\n",
    "            giou_loss = K.sum(giou_loss) / batch_size_f\n",
    "            location_loss = giou_loss\n",
    "        elif use_diou_loss:\n",
    "            # Calculate DIoU loss as location loss\n",
    "            raw_true_box = y_true[i][...,0:4]\n",
    "            # TODO: box_diou\n",
    "            diou = box_diou(raw_true_box, pred_box)\n",
    "            diou_loss = object_mask * box_loss_scale * (1 - diou)\n",
    "            diou_loss = K.sum(diou_loss) / batch_size_f\n",
    "            location_loss = diou_loss\n",
    "        else:\n",
    "            # Standard YOLOv3 location loss\n",
    "            # K.binary_crossentropy is helpful to avoid exp overflow.\n",
    "            xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[...,0:2], from_logits=True)\n",
    "            wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh-raw_pred[...,2:4])\n",
    "            xy_loss = K.sum(xy_loss) / batch_size_f\n",
    "            wh_loss = K.sum(wh_loss) / batch_size_f\n",
    "            location_loss = xy_loss + wh_loss\n",
    "        \n",
    "        confidence_loss = K.sum(confidence_loss) / batch_size_f\n",
    "        class_loss = K.sum(class_loss) / batch_size_f\n",
    "        loss += location_loss + confidence_loss + class_loss\n",
    "        total_location_loss += location_loss\n",
    "        total_confidence_loss += confidence_loss\n",
    "        total_class_loss += class_loss\n",
    "\n",
    "    # Fit for tf 2.0.0 loss shape\n",
    "    loss = K.expand_dims(loss, axis=-1)\n",
    "\n",
    "    return loss, total_location_loss, total_confidence_loss, total_class_loss\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47652c",
   "metadata": {},
   "source": [
    "### 1、DIOU_Loss（Distance_IOU_Loss）\n",
    "\n",
    "<img src=\"https://pic1.zhimg.com/80/v2-029f094658e87f441bf30c80cb8d07d0_1440w.jpg\" alt=\"img\" style=\"zoom:45%;\" />\n",
    "\n",
    "### 2、CIOU_loss\n",
    "CIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。\n",
    "\n",
    "![img](https://pic2.zhimg.com/80/v2-a24dd2e0d0acef20f6ead6a13b5c33d1_1440w.jpg)\n",
    "\n",
    "其中v是衡量长宽比一致性的参数，我们也可以定义为：\n",
    "\n",
    "![img](https://pic2.zhimg.com/80/v2-5abd8f82d7e30bdf21d2fd5851cb53a1_1440w.jpg)\n",
    "\n",
    "这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f800c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IouLoss(object):\n",
    "    def __init__(self,\n",
    "                loss_weight=2.5\n",
    "                max_height=608\n",
    "                max_width=608\n",
    "                ciou_term=False\n",
    "                loss_squre=True):\n",
    "        self._loss_weight = loss_weight\n",
    "        self._MAX_H = max_height\n",
    "        self._MAX_W = max_width\n",
    "        self.ciou_term = ciou_term\n",
    "        self.loss_squre = loss_squre\n",
    "        \n",
    "    def __call__(self, x, y, w, h\n",
    "                tx, ty, tw, th,\n",
    "                anchors,\n",
    "                downsample_ratio,\n",
    "                batch_size,\n",
    "                scale_x_y=1.,\n",
    "                ioup=None,\n",
    "                eps=1.e-10):\n",
    "        '''\n",
    "        Args:\n",
    "            x  | y | w | h  ([Variables]): the output of yolov for encoded x|y|w|h\n",
    "            tx |ty |tw |th  ([Variables]): the target of yolov for encoded x|y|w|h\n",
    "            anchors ([float]): list of anchors for current output layer\n",
    "            downsample_ratio (float): the downsample ratio for current output layer\n",
    "            batch_size (int): training batch size\n",
    "            eps (float): the decimal to prevent the denominator eqaul zero\n",
    "        '''\n",
    "        pred = self._bbox_transform(x, y, w, h, anchors, downsample_ratio,\n",
    "                                   batch_size, False, scale_x_y, eps)\n",
    "        gt = self._bbox_transform(tx, ty, tw, th, anchors, downsample_ratio,\n",
    "                                    batch_size, True, scale_x_y, eps)\n",
    "        iouk = self._iou(pred, gt, ioup, eps)\n",
    "        if self.loss_square:\n",
    "            loss_iou = 1. - iouk * iouk\n",
    "        else:\n",
    "            loss_iou = 1. - iouk\n",
    "            \n",
    "        return loss_iou\n",
    "        \n",
    "    \n",
    "    def _iou(self, pred, gt, ioup=None, eps=1.e-10):\n",
    "        x1, y1, x2, y2 = pred\n",
    "        x1g, y1g, x2g, y2g = gt\n",
    "        \n",
    "        xkis1 = tf.maximum(x1, x1g)\n",
    "        ykis1 = tf.maximum(y1, y1g)\n",
    "        xkis2 = tf.minimum(x2, x2g)\n",
    "        ykis2 = tf.minimum(y2, y2g)\n",
    "        \n",
    "        inter_w = tf.maximum((xkis2 - xkis1), 0.0)\n",
    "        inter_h = tf.maximum((ykis2 - ykis1), 0.0)\n",
    "        # 计算交集部分\n",
    "        intsctk = inter_w * inter_h\n",
    "        \n",
    "        # 计算并集部分\n",
    "        unionk = (x2 - x1) * (y2 - y1) + (x2g - x1g) * (y2g - y1g) - intsctk + eps\n",
    "        \n",
    "        # 计算交并比\n",
    "        iouk = intsctk / unionk\n",
    "        \n",
    "        # 如果使用 ciou\n",
    "        if self.ciou_term:\n",
    "            ciou = self.get_ciou_term(pred, gt, iouk, eps)\n",
    "            iouk = iouk - ciou\n",
    "        \n",
    "        return iouk\n",
    "    \n",
    "    def get_ciou_term(self, pred, gt, iouk, eps):\n",
    "        x1, y1, x2, y2 = pred\n",
    "        x1g, y1g, x2g, y2g = gt\n",
    "        \n",
    "        # 计算中心位置和宽高\n",
    "        cx = (x1 + x2) / 2\n",
    "        cy = (y1 + y2) / 2\n",
    "        w = (x2 - x1) + 1e-9\n",
    "        h = (y2 - y1) + 1e-9\n",
    "\n",
    "        cxg = (x1g + x2g) / 2\n",
    "        cyg = (y1g + y2g) / 2\n",
    "        wg = x2g - x1g\n",
    "        hg = y2g - y1g\n",
    "        \n",
    "        # 最小外接框坐标计算\n",
    "        xc1 = tf.minimum(x1, x1g)\n",
    "        yc1 = tf.minimum(y1, y1g)\n",
    "        xc2 = tf.maximum(x2, x2g)\n",
    "        yc2 = tf.maximum(y2, y2g)\n",
    "        # 计算对角线距离\n",
    "        dist_union = (xc2 - xc1) ** 2 + (yc2 - yc1) ** 2\n",
    "        # 计算中心点距离\n",
    "        dist_intersection = (cx - cxg) ** 2 + (cy - cyg) ** 2\n",
    "        # DIOU term\n",
    "        diou_term = (dist_intersection + eps) / (dist_union + eps)\n",
    "        \n",
    "        arctan = tf.atan(wg / hg) - tf.atan(w / h)\n",
    "        v = 4. / (np.pi ** 2) * (arctan ** 2)\n",
    "        # CIOU term，公式见上面\n",
    "        ciou_term = v**2 / (1 - iouk + v + eps)\n",
    "        \n",
    "        return diou_term + ciou_term\n",
    "        \n",
    "        \n",
    "    def _bbox_transform(self, dcx, dcy, dw, dh, anchors, downsample_ratio,\n",
    "                        batch_size, is_gt, scale_x_y, eps):\n",
    "        '''用来解析预测框和真实框坐标，暂时没看\n",
    "        '''\n",
    "        shape_fmp = tf.shape(dcx)\n",
    "        # batch_size = shape_fmp[0]\n",
    "        anchor_per_scale = shape_fmp[1]\n",
    "        output_size = shape_fmp[2]\n",
    "        output_size_f = tf.cast(output_size, tf.float32)\n",
    "        rows = tf.range(output_size_f, dtype=tf.float32)\n",
    "        cols = tf.range(output_size_f, dtype=tf.float32)\n",
    "        rows = tf.tile(rows[tf.newaxis, tf.newaxis, tf.newaxis, :], [batch_size, anchor_per_scale, output_size, 1])\n",
    "        cols = tf.tile(cols[tf.newaxis, tf.newaxis, :, tf.newaxis], [batch_size, anchor_per_scale, 1, output_size])\n",
    "\n",
    "        if is_gt:\n",
    "            cx = (dcx + rows) / output_size_f\n",
    "            cy = (dcy + cols) / output_size_f\n",
    "        else:\n",
    "            dcx_sig = tf.sigmoid(dcx)\n",
    "            dcy_sig = tf.sigmoid(dcy)\n",
    "            if (abs(scale_x_y - 1.0) > eps):\n",
    "                dcx_sig = scale_x_y * dcx_sig - 0.5 * (scale_x_y - 1)\n",
    "                dcy_sig = scale_x_y * dcy_sig - 0.5 * (scale_x_y - 1)\n",
    "            cx = (dcx_sig + rows) / output_size_f\n",
    "            cy = (dcy_sig + cols) / output_size_f\n",
    "\n",
    "        anchor_w_ = [anchors[i] for i in range(0, len(anchors)) if i % 2 == 0]\n",
    "        anchor_w_np = np.array(anchor_w_)\n",
    "        anchor_w_ = tf.ones(anchor_w_np.shape, dtype=tf.float32) * anchor_w_np\n",
    "        anchor_w = tf.tile(anchor_w_[tf.newaxis, :, tf.newaxis, tf.newaxis], [batch_size, 1, output_size, output_size])\n",
    "\n",
    "        anchor_h_ = [anchors[i] for i in range(0, len(anchors)) if i % 2 == 1]\n",
    "        anchor_h_np = np.array(anchor_h_)\n",
    "        anchor_h_ = tf.ones(anchor_h_np.shape, dtype=tf.float32) * anchor_h_np\n",
    "        anchor_h = tf.tile(anchor_h_[tf.newaxis, :, tf.newaxis, tf.newaxis], [batch_size, 1, output_size, output_size])\n",
    "\n",
    "        # e^tw e^th\n",
    "        exp_dw = tf.exp(dw)\n",
    "        exp_dh = tf.exp(dh)\n",
    "        pw = (exp_dw * anchor_w) / (output_size_f * downsample_ratio)\n",
    "        ph = (exp_dh * anchor_h) / (output_size_f * downsample_ratio)\n",
    "\n",
    "        x1 = cx - 0.5 * pw\n",
    "        y1 = cy - 0.5 * ph\n",
    "        x2 = cx + 0.5 * pw\n",
    "        y2 = cy + 0.5 * ph\n",
    "\n",
    "        return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_obj_loss(output, obj, tobj, gt_box, batch_size, anchors,\n",
    "                 num_classes, downsample, ignore_thresh, scale_x_y):\n",
    "    # A prediction bbox overlap any gt_bbox over ignore_thresh,\n",
    "        # objectness loss will be ignored, process as follows:\n",
    "    _anchors = np.array(anchors)\n",
    "    _anchors = np.reshape(_anchors, (-1, 2).astype(np.float32))\n",
    "    \n",
    "    image_size = tf.ones((batch_size, 2), dtype=tf.float32)\n",
    "    bbox,  prob = paddle_yolo_box(output, _anchors, downsample,\n",
    "                                     num_classes, scale_x_y, im_size, clip_bbox=False,\n",
    "                                     conf_thresh=0.0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bcd22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
