# 目标检测-YOLO系列

# 0、Yolo算法

> https://www.jianshu.com/p/86b8208f634f

在Yolo算法发表之前，大部分表现比较好的对象检测（Object Detection）算法都是以 R-CNN 为代表两阶段算法，慢。

Yolo算法提出一步完成预测，在一个CNN网络模型中完成图片中所有位置对象的box和类别预测，推理速度加快。

其创新点：提出将输入图片进行 N*N 的栅格化（每个小单元叫 grid cell），然后将图片中某个对象的位置的预测任务交给该对象中心位置所在的 grid cell 的bounding box。在训练的过程，我们通过 grid cell 的方式告诉模型，图中对象 A 应该是由中心落在特定 grid cell 的某个范围内的某些像素组成，模型接收到这些信息就会在 grid cell 周围以一定大小范围取寻找所有满足对象 A 特征的像素，经过很多次带惩罚的尝试训练后，它就能找到这个准确的方位。

![img](https://upload-images.jianshu.io/upload_images/18299912-39fa866f475846df.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1192/format/webp)

# 1、Yolo v1

> https://zhuanlan.zhihu.com/p/70387154
>
> https://zhuanlan.zhihu.com/p/115759795

## 1.1 网络结构

<img src="https://pic2.zhimg.com/80/v2-d9f14f0d6f0371912e1ce588f88e39cd_1440w.jpg" alt="img" style="zoom:67%;" />

输入图像大小为448，经过若干个卷积层与池化层，变为7\*7\*1024张量（图一中倒数第三个立方体），最后经过两层全连接层，输出张量维度为7\*7\*30，这就是Yolo v1的整个神经网络结构，和一般的卷积物体分类网络没有太多区别.

最大的不同就是：分类网络最后的全连接层，一般连接于一个一维向量，向量的不同位代表不同类别，而这里的输出向量是一个三维的张量（7\*7\*30）。上图中Yolo的backbone网络结构，受启发于GoogLeNet，也是v2、v3中Darknet的先锋。本质上来说没有什么特别，没有使用BN层，用了一层Dropout。除了最后一层的输出使用了线性激活函数，其他层全部使用Leaky Relu激活函数。网络结构没有特别的东西，不再赘述。

## 1.2 网络输出张量维度

这里的输出维度非常重要

**(1) 7\*7 **

7\*7是指图片被分成了7\*7个格子，如下所示：

<img src="https://pic4.zhimg.com/80/v2-2516e6896899723daa5f7194739c0497_1440w.jpg" alt="img" style="zoom:33%;" />

在Yolo中，如果一个物体的**中心点**，落在了某个格子中，那么这个格子将负责预测这个物体。

用上图举例，设左下角格子假设坐标为 ![[公式]](https://www.zhihu.com/equation?tex=%281%2C1%29)，小狗所在的最小包围矩形框的中心，落在了 ![[公式]](https://www.zhihu.com/equation?tex=%282%2C3%29) 这个格子中。那么7\*7个格子中，![[公式]](https://www.zhihu.com/equation?tex=%282%2C3%29) 这个格子负责预测小狗，而那些没有物体中心点落进来的格子，则不负责预测任何物体。

这个设定就好比该网络在一开始，就将整个图片上的预测任务进行了分工，一共设定7\*7个按照方阵列队的检测人员，每个人员负责检测一个物体，大家的分工界线，就是看被检测物体的中心点落在谁的格子里。当然，是7\*7还是9\*9，是上图中的参数S，可以自己修改，精度和性能会随之有些变化。

**(2) 30的含义**

刚才设定了49个检测人员，那么每个人员负责检测的内容，就是这里的30（注意，30是张量最后一维的长度）。

在Yolo v1论文中，30是由 ![[公式]](https://www.zhihu.com/equation?tex=%284%2B1%29%2A2+%2B+20) 得到的。其中4+1是矩形框的中心点坐标 ![[公式]](https://www.zhihu.com/equation?tex=x%2C+y) ，长宽 ![[公式]](https://www.zhihu.com/equation?tex=w%2C+h) 以及是否属于被检测物体的置信度 ![[公式]](https://www.zhihu.com/equation?tex=c+) ；2是一个格子共回归两个矩形框，每个矩形框分别产生5个预测值（ ![[公式]](https://www.zhihu.com/equation?tex=x%2Cy%2Cw%2Ch%2Cc) )；20代表预测20个类别。

这里有几点需要注意：1. 每个方格（grid） 产生2个预测框，2也是参数，可以调，但是一旦设定为2以后，那么每个方格只产生两个矩形框，最后选定置信度更大的矩形框作为输出，**也就是最终**

1. **每个方格只输出一个预测矩形框**
2. **每个方格只能预测一个物体。**

虽然可以通过调整参数，产生不同的矩形框，但这只能提高矩形框的精度。**所以当有很多个物体的中心点落在了同一个格子里，该格子只能预测一个物体**。也就是格子数为7*7时，该网络最多预测49个物体。

> 在强行施加了格点限制以后，每个格点只能输出一个预测结果，所以该算法最大的不足，就是对一些邻近小物体的识别效果不是太好，例如成群结队的小鸟。

## 1.3 Loss 函数

当选取7\*7的grid和2个bounding box之后，输出为7\*7\*30的tensor。其中每个30d向量包括：5d长度的bbox1预测+5d长度的bbox2预测+该grid属于20个class的概率。

loss函数如下：

<img src="https://pic1.zhimg.com/80/v2-aad10d0978fe7bc62704a767eabd0b54_1440w.jpg" alt="img" style="zoom:50%;" />

1. **预测框的中心点**![[公式]](https://www.zhihu.com/equation?tex=%28x%2C+y%29) 。造成的损失是图五中的第一行。其中 ![[公式]](https://www.zhihu.com/equation?tex=1%5E%7Bobj%7D_%7Bij%7D) 为控制函数，在标签中包含物体的那些格点处，该值为 1 ；若格点不含有物体，该值为 0。也就是只对那些有真实物体所属的格点进行损失计算，若该格点不包含物体，那么预测数值不对损失函数造成影响。![[公式]](https://www.zhihu.com/equation?tex=%28x%2C+y%29) 数值与标签用简单的平方和误差。
2. **预测框的宽高**![[公式]](https://www.zhihu.com/equation?tex=%28w%2C+h%29) 。造成的损失是图五的第二行。![[公式]](https://www.zhihu.com/equation?tex=1%5E%7Bobj%7D_%7Bij%7D)的含义一样，也是使得只有真实物体所属的格点才会造成损失。这里对 ![[公式]](https://www.zhihu.com/equation?tex=%28w%2C+h%29) 在损失函数中的处理分别取了**根号**，原因在于，如果不取根号，损失函数往往更倾向于调整尺寸比较大的预测框。例如，20个像素点的偏差，对于800*600的预测框几乎没有影响，此时的IOU数值还是很大，但是对于30*40的预测框影响就很大。取根号是为了尽可能的消除大尺寸框与小尺寸框之间的差异。
3. 第三行与第四行，都是**预测框的置信度**C。当该格点不含有物体时，该置信度的标签为0；若含有物体时，该置信度的标签为预测框与真实物体框的IOU数值（IOU计算公式为：两个框交集的面积除以并集的面积）。
4. 第五行为**物体类别概率**P，对应的类别位置，该标签数值为1，其余位置为0，与分类网络相同。

此时再来看 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_%7Bcoord%7D+) 与 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_%7Bnoobj%7D+) ，Yolo面临的物体检测问题，是一个典型的类别数目不均衡的问题。其中49个格点，含有物体的格点往往只有3、4个，其余全是不含有物体的格点。此时如果不采取点措施，那么物体检测的mAP不会太高，因为模型更倾向于不含有物体的格点。![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_%7Bcoord%7D+) 与 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_%7Bnoobj%7D+)的作用，就是让含有物体的格点，在损失函数中的权重更大，**让模型更加“重视”含有物体的格点所造成的损失**。在论文中， ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_%7Bcoord%7D+) 与 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_%7Bnoobj%7D+) 的取值分别为5与0.5。

## 1.4 一些技巧

1. 回归offset代替直接回归坐标

   ![[公式]](https://www.zhihu.com/equation?tex=%28x%2C+y%29) **不直接回归中心点坐标数值，而是回归相对于格点左上角坐标的位移值**。例如，第一个格点中物体坐标为 ![[公式]](https://www.zhihu.com/equation?tex=%282.3%2C+3.6%29) ，另一个格点中的物体坐标为![[公式]](https://www.zhihu.com/equation?tex=%285.4%2C+6.3%29)，这四个数值让神经网络暴力回归，有一定难度。所以这里的offset是指，既然格点已知，那么物体中心点的坐标一定在格点正方形里，相对于格点左上角的位移值一定在区间\[0, 1)中。让神经网络去预测 ![[公式]](https://www.zhihu.com/equation?tex=%280.3%2C+0.6%29) 与 ![[公式]](https://www.zhihu.com/equation?tex=%280.4%2C+0.3%29) 会更加容易，在使用时，加上格点左上角坐标![[公式]](https://www.zhihu.com/equation?tex=%282%2C+3%29)、![[公式]](https://www.zhihu.com/equation?tex=%285%2C+6%29)即可。

2. 同一格点的不同预测框有不同作用

   前文中提到，每个格点预测两个或多个矩形框。此时假设每个格点预测两个矩形框。那么在训练时，见到一个真实物体，我们是希望两个框都去逼近这个物体的真实矩形框，还是只用一个去逼近？或许通常来想，让两个人一起去做同一件事，比一个人做一件事成功率要高，所以可能会让两个框都去逼近这个真实物体。但是作者没有这样做，**在损失函数计算中，只对和真实物体最接近的框计算损失，其余框不进行修正**。这样操作之后作者发现，一个格点的两个框在尺寸、长宽比、或者某些类别上逐渐有所分工，总体的召回率有所提升。

3. 使用非极大抑制生成预测框

   在预测的时候，格点与格点并不会冲突，但是在预测一些大物体或者邻近物体时，会有多个格点预测了同一个物体。此时采用非极大抑制技巧，过滤掉一些重叠的矩形框。但是mAP提升并没有显著提升。

4. 推理时将 ![[公式]](https://www.zhihu.com/equation?tex=p%2Ac) 作为输出置信度

   > ![[公式]](https://www.zhihu.com/equation?tex=Pr%28Class_i%7CObject%29+%E2%88%97+Pr%28Object%29+%E2%88%97+IOU%5E%7Btruth%7D_%7Bpred%7D+%3D+Pr%28Class_i%29+%E2%88%97+IOU%5E%7Btruth%7D_%7Bpred%7D+)

   在推理时，**使用物体的类别预测最大值 ![[公式]](https://www.zhihu.com/equation?tex=p) 乘以 预测框的最大值 ![[公式]](https://www.zhihu.com/equation?tex=c) ，作为输出预测物体的置信度**。这样也可以过滤掉一些大部分重叠的矩形框。输出检测物体的置信度，同时考虑了矩形框与类别，满足阈值的输出更加可信。

## 1.5 与 Fast R-CNN 的错误分析对比



错误分析：

<img src="https://pic3.zhimg.com/80/v2-3bd6cea2ccb96831cc357b801122e0e6_1440w.jpg" alt="img" style="zoom:30%;" />

首先给出图中各个单词的含义：

> • Correct: correct class and IOU > .5
> • Localization: correct class, .1 < IOU < .5
> • Similar: class is similar, IOU > .1
> • Other: class is wrong, IOU > .1
> • Background: IOU < .1 for any object

其中，Yolo的Localization错误率更高，直接对位置进行回归，确实不如滑窗式的检测方式准确率高。但是Yolo对于背景的误检率更低，由于Yolo在推理时，可以“看到”整张图片，所以能够更好的区分背景与待测物体。作者提到Yolo对于小物体检测效果欠佳，不过在v2与v3中都做了不少改进。

# 2、Yolo v2

> https://zhuanlan.zhihu.com/p/35325884
>
> https://zhuanlan.zhihu.com/p/74540100

## 2.1 Yolo v2 的改进策略

YOLOv1虽然检测速度很快，但是在检测精度上却不如R-CNN系检测方法，YOLOv1在物体定位方面（localization）不够准确，并且召回率（recall）较低。YOLOv2共提出了几种改进策略来提升YOLO模型的定位准确度和召回率，从而提高mAP，YOLOv2在改进中遵循一个原则：保持检测速度，这也是YOLO模型的一大优势。YOLOv2的改进策略如图所示，可以看出，大部分的改进方法都可以比较显著提升模型的mAP。下面详细介绍各个改进策略。

<img src="https://pic2.zhimg.com/80/v2-8f48ba80f887fd5b26bd4211cf740c39_1440w.jpg" alt="img" style="zoom:47%;" />

**Batch Normalization**

Batch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。

High Resolution Classifier（分类网络高分辨率预训练）

在Yolov1中，网络的backbone部分会在ImageNet数据集上进行预训练，训练时网络输入图像的分辨率为224\*224，分辨率相对较低，不利于检测模型。在v2中，将分类网络在输入图片分辨率为448\*448的ImageNet数据集上训练10个epoch，再使用检测数据集（例如coco）进行微调。高分辨率预训练使mAP提高了大约4%。

Convolutional With Anchor Boxes（Anchor Box替换全连接层）

第一篇解读v1时提到，每个格点预测两个矩形框，在计算loss时，只让与ground truth最接近的框产生loss数值，而另一个框不做修正。这样规定之后，作者发现两个框在物体的大小、长宽比、类别上逐渐有了分工。

> YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差。YOLOv2借鉴了Faster R-CNN中RPN网络的先验框（anchor boxes，prior boxes，SSD也采用了先验框）策略。RPN对CNN特征提取器得到的特征图（feature map）进行卷积来预测每个位置的边界框以及置信度（是否含有物体），并且各个位置设置不同尺度和比例的先验框，所以RPN预测的是边界框相对于先验框的offsets值（其实是transform值，详细见[Faster R_CNN论文](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.01497)），采用先验框使得模型更容易学习。

在v2中，神经网络不对预测矩形框的宽高的绝对值进行预测，而是预测与Anchor框的偏差（offset），每个格点指定n个Anchor框。

在训练时，最接近ground truth的框产生loss，其余框不产生loss。在引入Anchor Box操作后，mAP由69.5下降至69.2，原因在于，每个格点预测的物体变多之后，召回率大幅上升，准确率有所下降，总体mAP略有下降。

> 所以YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。为了使检测所用的特征图分辨率更高，移除其中的一个pool层。在检测模型中，YOLOv2不是采用 ![[公式]](https://www.zhihu.com/equation?tex=448%5Ctimes448) 图片作为输入，而是采用 ![[公式]](https://www.zhihu.com/equation?tex=416%5Ctimes416) 大小。因为YOLOv2模型下采样的总步长为 ![[公式]](https://www.zhihu.com/equation?tex=32) ，对于 ![[公式]](https://www.zhihu.com/equation?tex=416%5Ctimes416) 大小的图片，最终得到的特征图大小为 ![[公式]](https://www.zhihu.com/equation?tex=13%5Ctimes13) ，维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。所以在YOLOv2设计中要保证最终的特征图有奇数个位置。对于YOLOv1，每个cell都预测2个boxes，每个boxes包含5个值： ![[公式]](https://www.zhihu.com/equation?tex=%28x%2C+y%2C+w%2C+h%2C+c%29) ，前4个值是边界框位置与大小，最后一个值是置信度（confidence scores，包含两部分：含有物体的概率以及预测框与ground truth的IOU）。但是每个cell只预测一套分类概率值（class predictions，其实是置信度下的条件概率值）,供2个boxes共享。YOLOv2使用了anchor boxes之后，每个位置的各个anchor box都单独预测一套分类概率值，这和SSD比较类似（但SSD没有预测置信度，而是把background作为一个类别来处理）。

**New Network: Darknet-19**

YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如图4所示。Darknet-19与VGG16模型设计原则是一致的，主要采用 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3) 卷积，采用 ![[公式]](https://www.zhihu.com/equation?tex=2%5Ctimes2) 的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。

与NIN([Network in Network](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1312.4400))类似，Darknet-19最终采用global avgpooling做预测，并且在 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3) 卷积之间使用 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。

**Dimension Clusters（Anchor Box的宽高由聚类产生）**

这里算是作者的一个创新点。Faster R-CNN中的九个Anchor Box的宽高是事先设定好的比例大小，一共设定三个面积大小的矩形框，每个矩形框有三个宽高比：1:1，2:1，1:2，总共九个框。

而在v2中，Anchor Box的宽高不经过人为获得，而是将训练数据集中的矩形框全部拿出来，用kmeans聚类得到先验框的宽和高。例如使用5个Anchor Box，那么kmeans聚类的类别中心个数设置为5。加入了聚类操作之后，引入Anchor Box之后，mAP上升。

需要强调的是，聚类必须要定义聚类点（矩形框 ![[公式]](https://www.zhihu.com/equation?tex=%28w%2Ch%29)）之间的距离函数，文中使用如下函数：

<img src="https://pic3.zhimg.com/80/v2-418f012af6cc4f94db0486cdca8968aa_1440w.png" alt="img" style="zoom:33%;" />

使用（1-IOU）数值作为两个矩形框的的距离函数，这里的运用也是非常的巧妙。

**Direct location prediction（绝对位置预测）**

YOLOv2借鉴RPN网络使用anchor boxes来预测边界框相对先验框的offsets。边界框的实际中心位置 ![[公式]](https://www.zhihu.com/equation?tex=%28x%2Cy%29) ，需要根据预测的坐标偏移值 ![[公式]](https://www.zhihu.com/equation?tex=%28t_x%2C+t_y%29) ，先验框的尺度 ![[公式]](https://www.zhihu.com/equation?tex=%28w_a%2C+h_a%29) 以及中心坐标 ![[公式]](https://www.zhihu.com/equation?tex=%28x_a%2C+y_a%29) （特征图每个位置的中心点）来计算：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5Cx+%3D+%28t_x%5Ctimes+w_a%29-x_a)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5Cy%3D%28t_y%5Ctimes+h_a%29+-+y_a)

但是上面的公式是无约束的，预测的边界框很容易向任何方向偏移，如当 ![[公式]](https://www.zhihu.com/equation?tex=t_x%3D1) 时边界框将向右偏移先验框的一个宽度大小，而当 ![[公式]](https://www.zhihu.com/equation?tex=t_x%3D-1) 时边界框将向左偏移先验框的一个宽度大小，因此每个位置预测的边界框可以落在图片任何位置，这导致模型的不稳定性，在训练时需要很长时间来预测出正确的offsets。

所以，YOLOv2弃用了这种预测方式，而是沿用YOLOv1的方法，就是预测边界框中心点相对于对应cell左上角位置的相对偏移值，为了将边界框中心点约束在当前cell中，使用sigmoid函数处理偏移值，这样预测的偏移值在(0,1)范围内（每个cell的尺度看做1）。总结来看，根据边界框预测的4个offsets ![[公式]](https://www.zhihu.com/equation?tex=t_x%2C+t_y%2C+t_w%2C+t_h) ，可以按如下公式计算出边界框实际位置和大小：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5Cb_x+%3D+%5Csigma+%28t_x%29%2Bc_x)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5Cb_y+%3D+%5Csigma+%28t_y%29+%2B+c_y)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5Cb_w+%3D+p_we%5E%7Bt_w%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5Cb_h+%3D+p_he%5E%7Bt_h%7D)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%28c_x%2C+x_y%29) 为cell的左上角坐标，如图5所示，在计算时每个cell的尺度为1，所以当前cell的左上角坐标为 ![[公式]](https://www.zhihu.com/equation?tex=%281%2C1%29) 。由于sigmoid函数的处理，边界框的中心位置会约束在当前cell内部，防止偏移过多。而 ![[公式]](https://www.zhihu.com/equation?tex=p_w) 和 ![[公式]](https://www.zhihu.com/equation?tex=p_h) 是先验框的宽度与长度，前面说过它们的值也是相对于特征图大小的，在特征图中每个cell的长和宽均为1。这里记特征图的大小为 ![[公式]](https://www.zhihu.com/equation?tex=%28W%2C+H%29) （在文中是 ![[公式]](https://www.zhihu.com/equation?tex=%2813%2C+13%29) )，这样我们可以将边界框相对于整张图片的位置和大小计算出来（4个值均在0和1之间）：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5Cb_x+%3D+%28%5Csigma+%28t_x%29%2Bc_x%29%2FW)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C+b_y+%3D+%28%5Csigma+%28t_y%29+%2B+c_y%29%2FH)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5Cb_w+%3D+p_we%5E%7Bt_w%7D%2FW)

![[公式]](https://www.zhihu.com/equation?tex=+%5C%5Cb_h+%3D+p_he%5E%7Bt_h%7D%2FH)

如果再将上面的4个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的最终位置和大小了。这就是YOLOv2边界框的整个解码过程。约束了边界框的位置预测值使得模型更容易稳定训练，结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。

<img src="https://pic3.zhimg.com/80/v2-7fee941c2e347efc2a3b19702a4acd8e_1440w.jpg" alt="img" style="zoom:70%;" />

**Fine-Grained Features（细粒度特征）**

YOLOv2的输入图片大小为 ![[公式]](https://www.zhihu.com/equation?tex=416%5Ctimes416) ，经过5次maxpooling之后得到 ![[公式]](https://www.zhihu.com/equation?tex=13%5Ctimes13) 大小的特征图，并以此特征图采用卷积做预测。 ![[公式]](https://www.zhihu.com/equation?tex=13%5Ctimes13) 大小的特征图对检测大物体是足够了，但是对于小物体还需要更精细的特征图（Fine-Grained Features）。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。

<img src="https://pic1.zhimg.com/80/v2-293f609a711615905b0bf7faa83583f8_1440w.jpg" alt="img" style="zoom:50%;" />

YOLOv2所利用的Fine-Grained Features是 ![[公式]](https://www.zhihu.com/equation?tex=26%5Ctimes26) 大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为 ![[公式]](https://www.zhihu.com/equation?tex=26%5Ctimes26%5Ctimes512) 的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。

前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个 ![[公式]](https://www.zhihu.com/equation?tex=2%5Ctimes2) 的局部区域，然后将其转化为channel维度，对于 ![[公式]](https://www.zhihu.com/equation?tex=26%5Ctimes26%5Ctimes512) 的特征图，经passthrough层处理之后就变成了 ![[公式]](https://www.zhihu.com/equation?tex=13%5Ctimes13%5Ctimes2048) 的新特征图（特征图大小降低4倍，而channles增加4倍，图6为一个实例），这样就可以与后面的 ![[公式]](https://www.zhihu.com/equation?tex=13%5Ctimes13%5Ctimes1024) 特征图连接在一起形成 ![[公式]](https://www.zhihu.com/equation?tex=13%5Ctimes13%5Ctimes3072) 大小的特征图，然后在此特征图基础上卷积做预测。

另外，作者在后期的实现中借鉴了ResNet网络，不是直接对高分辨特征图处理，而是增加了一个中间卷积层，先采用64个 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积核进行卷积，然后再进行passthrough处理，这样 ![[公式]](https://www.zhihu.com/equation?tex=26%5Ctimes26%5Ctimes512) 的特征图得到 ![[公式]](https://www.zhihu.com/equation?tex=13%5Ctimes13%5Ctimes256) 的特征图。这算是实现上的一个小细节。使用Fine-Grained Features之后YOLOv2的性能有1%的提升。

**Multi-Scale Training**

由于YOLOv2模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于 ![[公式]](https://www.zhihu.com/equation?tex=416%5Ctimes416) 大小的图片。为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值： ![[公式]](https://www.zhihu.com/equation?tex=%5C%7B320%2C+352%2C...%2C+608%5C%7D) ，输入图片最小为 ![[公式]](https://www.zhihu.com/equation?tex=320%5Ctimes320) ，此时对应的特征图大小为 ![[公式]](https://www.zhihu.com/equation?tex=10%5Ctimes10) （不是奇数了，确实有点尴尬），而输入图片最大为 ![[公式]](https://www.zhihu.com/equation?tex=608%5Ctimes608) ，对应的特征图大小为 ![[公式]](https://www.zhihu.com/equation?tex=19%5Ctimes19) 。在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。

<img src="https://pic2.zhimg.com/80/v2-3c72c10c00c9268091d8c93f908d90ed_1440w.jpg" alt="img" style="zoom:30%;" />

采用Multi-Scale Training策略，YOLOv2可以适应不同大小的图片，并且预测出很好的结果。在测试时，YOLOv2可以采用不同大小的图片作为输入，在VOC 2007数据集上的效果如下图所示。可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于 ![[公式]](https://www.zhihu.com/equation?tex=544%5Ctimes544) ，mAP高达78.6%。注意，这只是测试时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。

<img src="https://pic3.zhimg.com/80/v2-71f001caa2968d5fb9196250d80f7b82_1440w.jpg" alt="img" style="zoom:30%;" />

总结来看，虽然YOLOv2做了很多改进，但是大部分都是借鉴其它论文的一些技巧，如Faster R-CNN的anchor boxes，YOLOv2采用anchor boxes和卷积做预测，这基本上与SSD模型（单尺度特征图的SSD）非常类似了，而且SSD也是借鉴了Faster R-CNN的RPN网络。

从某种意义上来说，YOLOv2和SSD这两个one-stage模型与RPN网络本质上无异，只不过RPN不做类别的预测，只是简单地区分物体与背景。在two-stage方法中，RPN起到的作用是给出region proposals，其实就是作出粗糙的检测，所以另外增加了一个stage，即采用R-CNN网络来进一步提升检测的准确度（包括给出类别预测）。而对于one-stage方法，它们想要一步到位，直接采用“RPN”网络作出精确的预测，要因此要在网络设计上做很多的tricks。YOLOv2的一大创新是采用Multi-Scale Training策略，这样同一个模型其实就可以适应多种大小的图片了。

## 2.2 Yolo v2 训练

YOLOv2的训练主要包括三个阶段。

第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为 ![[公式]](https://www.zhihu.com/equation?tex=224%5Ctimes224) ，共训练160个epochs。

然后第二阶段将网络的输入调整为 ![[公式]](https://www.zhihu.com/equation?tex=448%5Ctimes448) ，继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。

第三个阶段就是修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。网络修改包括（[网路结构可视化](https://link.zhihu.com/?target=http%3A//ethereon.github.io/netscope/%23/gist/d08a41711e48cf111e330827b1279c31)）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3%5Ctimes2014)卷积层，同时增加了一个passthrough层，最后使用 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积层输出预测结果，输出的channels数为： ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Bnum_anchors%7D%5Ctimes%285%2B%5Ctext%7Bnum_classes%7D%29) ，和训练采用的数据集有关系。由于anchors数为5，对于VOC数据集输出的channels数就是125，而对于COCO数据集则为425。这里以VOC数据集为例，最终的预测矩阵为 ![[公式]](https://www.zhihu.com/equation?tex=T) （shape为 ![[公式]](https://www.zhihu.com/equation?tex=%28%5Ctext%7Bbatch_size%7D%2C+13%2C+13%2C+125%29) ），可以先将其reshape为 ![[公式]](https://www.zhihu.com/equation?tex=%28%5Ctext%7Bbatch_size%7D%2C+13%2C+13%2C+5%2C+25%29) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=T%5B%3A%2C+%3A%2C+%3A%2C+%3A%2C+0%3A4%5D) 为边界框的位置和大小 ![[公式]](https://www.zhihu.com/equation?tex=%28t_x%2C+t_y%2C+t_w%2C+t_h%29) ， ![[公式]](https://www.zhihu.com/equation?tex=T%5B%3A%2C+%3A%2C+%3A%2C+%3A%2C+4%5D) 为边界框的置信度，而 ![[公式]](https://www.zhihu.com/equation?tex=T%5B%3A%2C+%3A%2C+%3A%2C+%3A%2C+5%3A%5D) 为类别预测值。

<img src="https://pic2.zhimg.com/80/v2-a2167f0ab8d7b65a849ce4f38e53e6b5_1440w.jpg" alt="img" style="zoom:56%;" />

![img](https://pic4.zhimg.com/80/v2-b23fdd08f65266f7af640c1d3d00c05f_1440w.jpg)

# 3、Yolo v3

> https://zhuanlan.zhihu.com/p/143747206
>
> https://zhuanlan.zhihu.com/p/76802514

## 3.1 网络结构图

<img src="https://pic2.zhimg.com/80/v2-af7f12ef17655870f1c65b17878525f1_1440w.jpg" alt="img" style="zoom:50%;" />

**上图三个蓝色方框内表示Yolov3的三个基本组件**：

1. **CBL：**Yolov3网络结构中的最小组件，由**Conv+Bn+Leaky_relu**激活函数三者组成。
2. **Res unit：**借鉴**Resnet**网络中的残差结构，让网络可以构建的更深。
3. **ResX：**由一个**CBL**和**X**个残差组件构成，是Yolov3中的大组件。每个Res模块前面的CBL都起到下采样的作用，因此经过5次Res模块后，得到的特征图是**608->304->152->76->38->19大小**。

**其他基础操作：**

1. **Concat：**张量拼接，会扩充两个张量的维度，例如26\*26\*256和26\*26\*512两个张量拼接，结果是26\*26\*768。Concat和cfg文件中的route功能一样。
2. **add：**张量相加，张量直接相加，不会扩充维度，例如104\*104\*128和104\*104\*128相加，结果还是104\*104\*128。add和cfg文件中的shortcut功能一样。

**Backbone中卷积层的数量：**

每个ResX中包含1+2*X个卷积层，因此整个主干网络Backbone中一共包含**1+（1+2\*1）+（1+2\*2）+（1+2\*8）+（1+2\*8）+（1+2\*4）=52**，再加上一个FC全连接层，即可以组成一个**Darknet53分类网络**。不过在目标检测Yolov3中，去掉FC层，不过为了方便称呼，仍然把**Yolov3**的主干网络叫做**Darknet53结构**。

## 3.2 数据流图

![img](https://pic3.zhimg.com/80/v2-d2596ea39974bcde176d1cf4dc99705e_1440w.jpg)

网络结构解析：

1. Yolov3中，只有卷积层，通过**调节卷积步长控制输出特征图的尺寸**。所以对于输入图片尺寸没有特别限制。流程图中，输入图片以256*256作为样例。
2. Yolov3借鉴了**金字塔特征图**思想，**小尺寸特征图用于检测大尺寸物体**，而**大尺寸特征图检测小尺寸物体**。特征图的输出维度为 ![[公式]](https://www.zhihu.com/equation?tex=N%5Ctimes+N+%5Ctimes+%5B3+%5Ctimes+%284+%2B+1+%2B+80%29%5D) ， ![[公式]](https://www.zhihu.com/equation?tex=N%5Ctimes+N) 为输出特征图格点数，一共3个Anchor框，每个框有4维预测框数值 ![[公式]](https://www.zhihu.com/equation?tex=t_x+%2Ct_y+%2Ct_w%2C+t_h) ，1维预测框置信度，80维物体类别数。所以第一层特征图的输出维度为 ![[公式]](https://www.zhihu.com/equation?tex=8+%5Ctimes+8+%5Ctimes+255) 。
3. Yolov3总共输出3个特征图，第一个特征图下采样32倍，第二个特征图下采样16倍，第三个下采样8倍。输入图像经过Darknet-53（无全连接层），再经过Yoloblock生成的特征图被当作两用：
   - 第一用为经过3\*3卷积层、1\*1卷积之后生成特征图一，
   - 第二用为经过1\*1卷积层加上采样层，与Darnet-53网络的中间层输出结果进行拼接，产生特征图二。同样的循环之后产生特征图三。
4. concat操作与加和操作的区别：加和操作来源于ResNet思想，将输入的特征图，与输出特征图对应维度进行相加，即 ![[公式]](https://www.zhihu.com/equation?tex=y+%3D+f%28x%29%2Bx) ；而concat操作源于DenseNet网络的设计思路，将特征图按照通道维度直接进行拼接，例如8\*8\*16的特征图与8\*8\*16的特征图拼接后生成8\*8\*32的特征图。
5. 上采样层(upsample)：作用是将小尺寸特征图通过插值等方法，生成大尺寸图像。例如使用最近邻插值算法，将8\*8的图像变换为16\*16。上采样层不改变特征图的通道数。

Yolo的整个网络，吸取了Resnet、Densenet、FPN的精髓，可以说是融合了目标检测当前业界最有效的全部技巧。

## 3.3 Yolo输出特征图（前向过程）

根据不同的输入尺寸，会得到不同大小的输出特征图，以图二中输入图片256 × 256 × 3为例，输出的特征图为8 × 8 × 255、16 × 16 × 255、32 × 32 × 255。

在Yolov3的设计中，每个特征图的每个格子中，都配置3个不同的先验框，所以最后三个特征图，这里暂且reshape为8 × 8 × 3 × 85、16 × 16 × 3 × 85、32 × 32 × 3 × 85

三张特征图就是整个Yolo输出的检测结果，检测框位置（4维）、检测置信度（1维）、类别（80维）都在其中，加起来正好是85维。特征图最后的维度85，代表的就是这些信息，而特征图其他维度N × N × 3，N × N代表了检测框的参考位置信息，3是3个不同尺度的先验框。下面详细描述怎么将检测信息解码出来：

**先验框**

在Yolov1中，网络直接回归检测框的宽、高，这样效果有限。所以在Yolov2中，改为了回归基于先验框的变化值，这样网络的学习难度降低，整体精度提升不小。Yolov3沿用了Yolov2中关于先验框的技巧，并且**使用k-means对数据集中的标签框进行聚类**，得到类别中心点的9个框，作为先验框。

在COCO数据集中（原始图片全部resize为416 × 416），九个框分别是 (10×13)，(16×30)，(33×23)，(30×61)，(62×45)，(59× 119)， (116 × 90)， (156 × 198)，(373 × 326) ，顺序为w × h。

> 注：先验框只与检测框的w、h有关，与x、y无关。

1. **检测框解码**

有了先验框与输出特征图，就可以解码检测框 x，y，w，h。

![[公式]](https://www.zhihu.com/equation?tex=b_x%3D%5Csigma+%28t_x%29+%2B+c_x+%5C%5C+b_y%3D%5Csigma+%28t_y%29+%2B+c_y+%5C%5C+b_w%3Dp_we%5E%7Bt_w%7D++%5C%5C+b_h%3Dp_he%5E%7Bt_h%7D+%5C%5C)

如下图所示， ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%28t_x%29%2C+%5Csigma%28t_y%29) 是基于矩形框中心点左上角格点坐标的偏移量， ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 是**激活函数**，论文中作者使用**sigmoid**。 ![[公式]](https://www.zhihu.com/equation?tex=p_w%2C+p_h) 是先验框的宽、高，通过上述公式，计算出实际预测框的宽高 ![[公式]](https://www.zhihu.com/equation?tex=%28b_w%2C+b_h%29) 。

<img src="https://pic2.zhimg.com/80/v2-758b1df9132a9f4b4e0c7def735e9a11_1440w.jpg" alt="img" style="zoom:60%;" />

举个具体的例子，假设对于第二个特征图16 × 16 × 3 × 85中的第[5，4，2]维，上图中的 ![[公式]](https://www.zhihu.com/equation?tex=c_y) 为5， ![[公式]](https://www.zhihu.com/equation?tex=+c_x) 为4，第二个特征图对应的先验框为(30×61)，(62×45)，(59× 119)，prior_box的index为2，那么取最后一个59，119作为先验w、先验h。这样计算之后的 ![[公式]](https://www.zhihu.com/equation?tex=b_x%2Cb_y) 还需要乘以特征图二的采样率16，得到真实的检测框x，y。

2. **检测置信度解码**

物体的检测置信度，在Yolo设计中非常重要，关系到算法的检测正确率与召回率。

置信度在输出85维中占固定一位，由sigmoid函数解码即可，解码之后数值区间在[0，1]中。

3. **类别解码**

   > https://zhuanlan.zhihu.com/p/42865896
   >
   > 物体之间的相互覆盖都是不能避免的。因此一个锚点的感受野肯定会包含两个甚至更多个不同物体的可能。如果使用softmax作为激活函数，意味着在一个锚点中的检测是互斥的，只有一个或者说少数点的置信度可以大于阈值。使用sigmoid分类器，最终各类别之间的互斥被取消。

COCO数据集有80个类别，所以类别数在85维输出中占了80维，每一维独立代表一个类别的置信度。使用sigmoid激活函数替代了Yolov2中的softmax，**取消了类别之间的互斥，可以使网络更加灵活。**

三个特征图一共可以解码出 8 × 8 × 3 + 16 × 16 × 3 + 32 × 32 × 3 = 4032 个box以及相应的类别、置信度。这4032个box，在训练和推理时，使用方法不一样：

1. 训练时4032个box全部送入打标签函数，进行后一步的标签以及损失函数的计算。
2. 推理时，选取一个置信度阈值，过滤掉低阈值box，再经过nms（非极大值抑制），就可以输出整个网络的预测结果了。

## 3.4 训练策略与损失函数（反向过程）

**训练策略总结如下：**

1. 预测框一共分为三种情况：正例（positive）、负例（negative）、忽略样例（ignore）。
2. 正例：任取一个ground truth，与4032个框全部计算IOU，IOU最大的预测框，即为正例。并且一个预测框，只能分配给一个ground truth。例如第一个ground truth已经匹配了一个正例检测框，那么下一个ground truth，就在余下的4031个检测框中，寻找IOU最大的检测框作为正例。ground truth的先后顺序可忽略。
   1. 正例产生置信度loss、检测框loss、类别loss。
   2. 预测框为对应的ground truth box标签（需要反向编码，使用真实的x、y、w、h计算出 ![[公式]](https://www.zhihu.com/equation?tex=t_x+%2Ct_y+%2Ct_w%2C+t_h) ）
   3. 类别标签对应类别为1，其余为0；置信度标签为1。
3. 忽略样例：正例除外，与任意一个ground truth的IOU大于阈值（论文中使用0.5），则为忽略样例。忽略样例不产生任何loss。
4. 负例：正例除外（与ground truth计算后IOU最大的检测框，但是IOU小于阈值，仍为正例），与全部ground truth的IOU都小于阈值（0.5），则为负例。负例只有置信度产生loss，置信度标签为0。

**Loss 函数**：

![[公式]](https://www.zhihu.com/equation?tex=loss_%7BN_1%7D+%3D+%5Clambda_%7Bbox%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7D%5B%28t_x+-+t_x%27%29%5E2+%2B+%28t_y+-+t_y%27%29%5E2%5D%7D+%5C%5C+%2B%5Clambda_%7Bbox%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7D%5B%28t_w+-+t_w%27%29%5E2+%2B+%28t_h+-+t_h%27%29%5E2%5D%7D++%5C%5C+-+%5Clambda_%7Bobj%7D%5Csum_%7Bi%3D0%7D%5E%7BN%5Ctimes+N%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7Dlog%28c_%7Bij%7D%29%7D+%5C%5C+-%5Clambda_%7Bnoobj%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bnoobj%7Dlog%281-c_%7Bij%7D%29%7D+%5C%5C+-%7B%5Clambda%7D_%7Bclass%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5C%5C%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7D+%5Csum_%7Bc+%5Cin+classes+%7D%5Bp_%7Bij%7D%27%28c%29log%28p_%7Bij%7D%28c%29%29%2B%281-p_%7Bij%7D%27%28c%29%29log%281-p_%7Bij%7D%28c%29%29%5D+%7D+)

Yolov3 Loss为三个特征图Loss之和：

![[公式]](https://www.zhihu.com/equation?tex=Loss%3Dloss_%7BN_1%7D++%2Bloss_%7BN_2%7D+%2Bloss_%7BN_3%7D++%5C%5C)

1. ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 为权重常数，控制检测框Loss、obj置信度Loss、noobj置信度Loss之间的比例，通常负例的个数是正例的几十倍以上，可以通过权重超参控制检测效果。
2. ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bobj%7D) 若是正例则输出1，否则为0； ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bnoobj%7D) 若是负例则输出1，否则为0；忽略样例都输出0。
3. x、y、w、h使用MSE作为损失函数，也可以使用smooth L1 loss（出自Faster R-CNN）作为损失函数。smooth L1可以使训练更加平滑。置信度、类别标签由于是0，1二分类，所以使用交叉熵作为损失函数。

**训练策略解释：**

1. ground truth为什么不按照中心点分配对应的预测box？

（1）在Yolov3的训练策略中，不再像Yolov1那样，每个cell负责中心落在该cell中的ground truth。原因是Yolov3一共产生3个特征图，3个特征图上的cell，中心是有重合的。训练时，可能最契合的是特征图1的第3个box，但是推理的时候特征图2的第1个box置信度最高。所以Yolov3的训练，不再按照ground truth中心点，严格分配指定cell，而是根据预测值寻找IOU最大的预测框作为正例。

（2）笔者实验结果：第一种，ground truth先从9个先验框中确定最接近的先验框，这样可以确定ground truth所属第几个特征图以及第几个box位置，之后根据中心点进一步分配。第二种，全部4032个输出框直接和ground truth计算IOU，取IOU最高的cell分配ground truth。第二种计算方式的IOU数值，往往都比第一种要高，这样wh与xy的loss较小，网络可以更加关注类别和置信度的学习；其次，在推理时，是按照置信度排序，再进行nms筛选，第二种训练方式，每次给ground truth分配的box都是最契合的box，给这样的box置信度打1的标签，更加合理，最接近的box，在推理时更容易被发现。

2. Yolov1中的置信度标签，就是预测框与真实框的IOU，Yolov3为什么是1？

（1）置信度意味着该预测框是或者不是一个真实物体，是一个二分类，所以标签是1、0更加合理。

（2）笔者实验结果：第一种：置信度标签取预测框与真实框的IOU；第二种：置信度标签取1。第一种的结果是，在训练时，有些预测框与真实框的IOU极限值就是0.7左右，置信度以0.7作为标签，置信度学习有一些偏差，最后学到的数值是0.5，0.6，那么假设推理时的激活阈值为0.7，这个检测框就被过滤掉了。但是IOU为0.7的预测框，其实已经是比较好的学习样例了。尤其是coco中的小像素物体，几个像素就可能很大程度影响IOU，所以第一种训练方法中，置信度的标签始终很小，无法有效学习，导致检测召回率不高。而检测框趋于收敛，IOU收敛至1，置信度就可以学习到1，这样的设想太过理想化。而使用第二种方法，召回率明显提升了很高。

3. 为什么有忽略样例？

（1）忽略样例是Yolov3中的点睛之笔。由于Yolov3使用了多尺度特征图，不同尺度的特征图之间会有重合检测部分。比如有一个真实物体，在训练时被分配到的检测框是特征图1的第三个box，IOU达0.98，此时恰好特征图2的第一个box与该ground truth的IOU达0.95，也检测到了该ground truth，如果此时给其置信度强行打0的标签，网络学习效果会不理想。

（2）笔者实验结果：如果给全部的忽略样例置信度标签打0，那么最终的loss函数会变成 ![[公式]](https://www.zhihu.com/equation?tex=Loss_%7Bobj%7D) 与 ![[公式]](https://www.zhihu.com/equation?tex=Loss_%7Bnoobj%7D) 的拉扯，不管两个loss数值的权重怎么调整，或者网络预测趋向于大多数预测为负例，或者趋向于大多数预测为正例。而加入了忽略样例之后，网络才可以学习区分正负例。

**优化器**：

作者在文中没有提及优化器，Adam，SGD等都可以用，github上Yolov3项目中，大多使用Adam优化器。

# 4、Yolo v4

> https://zhuanlan.zhihu.com/p/143747206

## 4.1 网络结构图

![img](https://pic2.zhimg.com/80/v2-ccc16892e80035886e36c0100dbd444d_1440w.jpg)

**先整理下Yolov4的五个基本组件**：

1. **CBM：**Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。
2. **CBL：**由Conv+Bn+Leaky_relu激活函数三者组成。
3. **Res unit：**借鉴Resnet网络中的残差结构，让网络可以构建的更深。
4. **CSPX：**借鉴CSPNet网络结构，由卷积层和X个Res unint模块Concate组成。
5. **SPP：**采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。

**其他基础操作：**

1. **Concat：**张量拼接，维度会扩充，和Yolov3中的解释一样，对应于cfg文件中的route操作。
2. **add：**张量相加，不会扩充维度，对应于cfg文件中的shortcut操作。

**Backbone中卷积层的数量：**

和Yolov3一样，再来数一下Backbone里面的卷积层数量。

每个CSPX中包含5+2*X个卷积层，因此整个主干网络Backbone中一共包含1+（5+2、*1）+（5+2*2）+（5+2*8）+（5+2*8）+（5+2*4）=72。

## 4.2 创新

为了便于分析，将Yolov4的整体结构拆分成四大板块：

![img](https://pic4.zhimg.com/80/v2-88544afd1a5b01b17f53623a0fda01db_1440w.jpg)

从以上4个部分对YoloV4的创新之处进行讲解，让大家一目了然。

1. **输入端：**这里指的创新主要是训练时对输入端的改进，主要包括**Mosaic数据增强、cmBN、SAT自对抗训练**
2. **BackBone主干网络：**将各种新的方式结合起来，包括：**CSPDarknet53、Mish激活函数、Dropblock**
3. **Neck：**目标检测网络在BackBone和最后的输出层之间往往会插入一些层，比如Yolov4中的**SPP模块**、**FPN+PAN结构**
4. **Prediction：**输出层的锚框机制和Yolov3相同，主要改进的是训练时的损失函数**CIOU_Loss**，以及预测框筛选的nms变为**DIOU_nms**

### 4.2.1 输入端创新

**Yolov4**对训练时的输入端进行改进，使得训练在单张GPU上也能有不错的成绩。比如**数据增强Mosaic、cmBN、SAT自对抗训练。**

**（1）Mosaic数据增强**

**Yolov4**中使用的**Mosaic**是参考2019年底提出的**CutMix数据增强**的方式，但**CutMix**只使用了两张图片进行拼接，而**Mosaic数据增强**则采用了4张图片，**随机缩放、随机裁剪、随机排布**的方式进行拼接。

<img src="https://pic4.zhimg.com/80/v2-dddc368bc1c8ec6239d152c609774673_1440w.jpg" alt="img" style="zoom:33%;" />

这里首先要了解为什么要进行**Mosaic数据增强**呢？

在平时项目训练时，**小目标的AP**一般比中目标和大目标低很多。而Coco数据集中也包含大量的小目标，但比较麻烦的是小目标的分布**并不均匀**。Coco数据集中小目标占比达到**41.4%**，数量比中目标和大目标都要多。但在所有的训练集图片中，只有**52.3%**的图片有小目标，而中目标和大目标的分布相对来说更加均匀一些。

针对这种状况，Yolov4的作者采用了**Mosaic数据增强**的方式。

主要有几个优点：

1. **丰富数据集：**随机使用**4张图片**，随机缩放，再随机分布进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。
2. **减少GPU：**可能会有人说，随机缩放，普通的数据增强也可以做，但作者考虑到很多人可能只有一个GPU，因此Mosaic增强训练时，可以直接计算4张图片的数据，使得Mini-batch大小并不需要很大，一个GPU就可以达到比较好的效果。

**(2) Self-Adversarial Training(自对抗训练)**

这是在一张图上，让神经网络反向更新图像，对图像做改变扰动，然后在这个图像上训练。这个方法，是图像风格化的主要方法，让网络反向更新图像来风格化图像（对风格化感兴趣，可以看看我写的一篇介绍[谷歌的一个实时任意风格化的文章](https://zhuanlan.zhihu.com/p/105550915)）；

>  https://www.zhihu.com/question/390191723/answer/1177584901

### 4.2.2 BackBone 创新

**（1）CSPDarknet53**

**CSPDarknet53**是在Yolov3主干网络**Darknet53**的基础上，借鉴**2019年CSPNet**的经验，产生的**Backbone**结构，其中包含了**5个CSP**模块。

<img src="https://pic3.zhimg.com/80/v2-139a50003c09efe54b2db906710f6252_1440w.jpg" alt="img" style="zoom:50%;" />

这里因为**CSP模块**比较长，不放到本处，大家也可以点击Yolov4的[netron网络结构图](https://link.zhihu.com/?target=https%3A//blog.csdn.net/nan355655600/article/details/106246422)，对比查看，一目了然。

每个CSP模块前面的卷积核的大小都是3*3，stride=2，因此可以起到下采样的作用。

因为Backbone有5个**CSP模块**，输入图像是**608\*608**，所以特征图变化的规律是：**608->304->152->76->38->19**

经过5次CSP模块后得到19*19大小的特征图。

而且作者只在Backbone中采用了**Mish激活函数**，网络后面仍然采用**Leaky_relu激活函**



我们再看看下作者为啥要参考2019年的**CSPNet**，采用CSP模块？

CSPNet论文地址：[https://arxiv.org/pdf/1911.11929.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1911.11929.pdf)

CSPNet全称是Cross Stage Paritial Network，主要从网络结构设计的角度解决推理中从计算量很大的问题。

CSPNet的作者认为推理计算过高的问题是由于网络优化中的**梯度信息重复**导致的。

因此采用CSP模块先将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并，在减少了计算量的同时可以保证准确率。

因此Yolov4在主干网络Backbone采用CSPDarknet53网络结构，主要有三个方面的优点：

**优点一：**增强CNN的学习能力，使得在轻量化的同时保持准确性。

**优点二：**降低计算瓶颈

**优点三：**降低内存成本

**（2）Mish激活函数**

Mish激活函数是**2019年下半年**提出的激活函数，和**Leaky_relu激活函数**的图形对比如下：

<img src="https://pic3.zhimg.com/80/v2-896f1e7c34a9d35ab3baf75777a244c6_1440w.jpg" alt="img" style="zoom:80%;" />

Yolov4的**Backbone**中都使用了**Mish激活函数**，而后面的网络则还是使用leaky_relu函数。实验测试得到的。

**（3）Dropblock**

Yolov4中使用的**Dropblock**，其实和常见网络中的Dropout功能类似，也是缓解过拟合的一种正则化方式。

传统的Dropout很简单，一句话就可以说的清：**随机删除减少神经元的数量，使网络变得更简单。**

<img src="https://pic1.zhimg.com/80/v2-fc1787220d4a48d285ade7d55ea90854_1440w.jpg" alt="img" style="zoom:40%;" />

而Dropblock和Dropout相似，比如下图：

<img src="https://pic2.zhimg.com/80/v2-8b9a2710b100dccd1ebc1fe500d5a7a1_1440w.jpg" alt="img" style="zoom:47%;" />

中间Dropout的方式会随机的删减丢弃一些信息，但**Dropblock的研究者**认为，卷积层对于这种随机丢弃并不敏感，因为卷积层通常是三层连用：**卷积+激活+池化层**，池化层本身就是对相邻单元起作用。而且即使随机丢弃，卷积层仍然可以从相邻的激活单元学习到**相同的信息**。

因此，在全连接层上效果很好的Dropout在卷积层上**效果并不好**。

所以**右图Dropblock的研究者**则干脆整个局部区域进行删减丢弃。

这种方式其实是借鉴**2017年的cutout数据增强**的方式，cutout是将输入图像的部分区域清零，而Dropblock则是将Cutout应用到每一个特征图。而且并不是用固定的归零比率，而是在训练时以一个小的比率开始，随着训练过程**线性的增加这个比率**。

<img src="https://pic4.zhimg.com/80/v2-2fa7921731f3448abbbf65f478c5db47_1440w.jpg" alt="img" style="zoom:70%;" />

**Dropblock**的研究者与**Cutout**进行对比验证时，发现有几个特点：

**优点一：**Dropblock的效果优于Cutout

**优点二：**Cutout只能作用于输入层，而Dropblock则是将Cutout应用到网络中的每一个特征图上

**优点三：**Dropblock可以定制各种组合，在训练的不同阶段可以修改删减的概率，从空间层面和时间层面，和Cutout相比都有更精细的改进。

**Yolov4**中直接采用了更优的**Dropblock**，对网络的正则化过程进行了全面的升级改进。

### 4.2.3 Neck创新

在目标检测领域，为了更好的提取融合特征，**通常在Backbone和输出层之间，插入一些层**，这个部分称为Neck。相当于目标检测网络的颈部，也是非常关键的。

Yolov4的Neck结构主要采用了**SPP模块**、**FPN+PAN**的方式。

**（1）SPP模块**

SPP模块，其实在Yolov3中已经存在了，在**Yolov4**的C++代码文件夹**中**有一个**Yolov3_spp版本**，但有的同学估计从来没有使用过，在Yolov4中，SPP模块仍然是在Backbone主干网络之后：

<img src="https://pic1.zhimg.com/80/v2-60f3d4a7fb071766ac3c3bf70bb5a6f8_1440w.jpg" alt="img" style="zoom:50%;" />

作者在SPP模块中，使用k={1*1,5*5,9*9,13*13}的最大池化的方式，再将不同尺度的特征图进行Concat操作。

**注意：**这里最大池化采用**padding操作**，移动的步长为1，比如13×13的输入特征图，使用5×5大小的池化核池化，**padding=2**，因此池化后的特征图仍然是13×13大小。

![img](https://pic2.zhimg.com/80/v2-f0a07a504f94f3cfb25f8ce7420faa39_1440w.jpg)

**（2）FPN+PAN**

**PAN结构**比较有意思，看了网上Yolov4关于这个部分的讲解，大多都是讲的比较笼统的，而PAN是借鉴[图像分割领域PANet](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.01534)的创新点，有些同学可能不是很清楚。

下面将这个部分拆解开来，看下Yolov4中是如何设计的。

**Yolov3结构：**

我们先来看下Yolov3中Neck的FPN结构

<img src="https://pic1.zhimg.com/80/v2-41fcbf90757e76578eaf1e6994cb159c_1440w.jpg" alt="img" style="zoom:47%;" />

可以看到经过几次下采样，三个紫色箭头指向的地方，输出分别是**76\*76、38\*38、19\*19**

以及最后的**Prediction**中用于预测的三个特征图**①19\*19\*255、②38\*38\*255、③76\*76\*255。[注：255表示80类别(1+4+80)×3=255]**

我们将Neck部分用立体图画出来，更直观的看下两部分之间是如何通过**FPN结构**融合的。

<img src="https://pic2.zhimg.com/80/v2-48085568c7e30a0a1c6d07f1f418a7a9_1440w.jpg" alt="img" style="zoom:80%;" />

如图所示，FPN是自顶向下的，将高层的特征信息通过**上采样**的方式进行传递融合，得到进行预测的特征图。

**Yolov4结构：**

<img src="https://pic3.zhimg.com/80/v2-5251e9c0784871a37c693d53f7d57f92_1440w.jpg" alt="img" style="zoom:67%;" />

我们也看下**Neck**部分的立体图像，看下两部分是如何通过**FPN+PAN结构**进行融合的。

<img src="https://pic1.zhimg.com/80/v2-a204a672779d1c2bc26777437771cda4_1440w.jpg" alt="img" style="zoom:80%;" />

和Yolov3的FPN层不同，Yolov4在FPN层的后面还添加了一个**自底向上的特征金字塔。**

其中包含两个**PAN结构。**

这样结合操作，FPN层自顶向下传达**强语义特征**，而特征金字塔则自底向上传达**强定位特征**，两两联手，从不同的主干层对不同的检测层进行参数聚合,这样的操作确实很皮。

**FPN+PAN**借鉴的是18年CVPR的**PANet**，当时主要应用于**图像分割领域**，但Alexey将其拆分应用到Yolov4中，进一步提高特征提取的能力。

**注意一：**

Yolov3的FPN层输出的三个大小不一的特征图①②③直接进行预测，但Yolov4的FPN层，只使用最后的一个76*76特征图①，而经过两次PAN结构，输出预测的特征图②和③。

> 这里的不同也体现在cfg文件中，这一点有很多同学之前不太明白，
>
> 比如Yolov3.cfg最后的三个Yolo层，
>
> 第一个Yolo层是最小的特征图**19\*19**，mask=**6,7,8**，对应**最大的anchor box。**
>
> 第二个Yolo层是中等的特征图**38\*38**，mask=**3,4,5**，对应**中等的anchor box。**
>
> 第三个Yolo层是最大的特征图**76\*76**，mask=**0,1,2**，对应**最小的anchor box。**
>
> 而Yolov4.cfg则**恰恰相反**
>
> 第一个Yolo层是最大的特征图**76\*76**，mask=**0,1,2**，对应**最小的anchor box。**
>
> 第二个Yolo层是中等的特征图**38\*38**，mask=**3,4,5**，对应**中等的anchor box。**
>
> 第三个Yolo层是最小的特征图**19\*19**，mask=**6,7,8**，对应**最大的anchor box。**

**注意点二：**

原本的PANet网络的**PAN结构**中，两个特征图结合是采用**shortcut**操作，而Yolov4中则采用**concat（route）**操作，特征图融合后的尺寸发生了变化。

<img src="https://pic2.zhimg.com/80/v2-c2f9cb3d71bc3011f6f18adc00db3319_1440w.jpg" alt="img" style="zoom:40%;" />

### 4.2.4 Prediction创新

**（1）CIOU_loss**

目标检测任务的损失函数一般由**Classificition Loss（分类损失函数）**和**Bounding Box Regeression Loss（回归损失函数）**两部分构成。

Bounding Box Regeression的Loss近些年的发展过程是：**Smooth L1 Loss-> IoU Loss（2016）-> GIoU Loss（2019）-> DIoU Loss（2020）->CIoU Loss（2020）**

我们从最常用的**IOU_Loss**开始，进行对比拆解分析，看下Yolov4为啥要选择**CIOU_Loss。**

**a.IOU_Loss**

<img src="https://pic3.zhimg.com/80/v2-c812620791de642ccb7edcde9e1bd742_1440w.jpg" alt="img" style="zoom:50%;" />

可以看到IOU的loss其实很简单，主要是**交集/并集**，但其实也存在两个问题。

<img src="https://pic4.zhimg.com/80/v2-e3d9a882dec6bb5847be80899bb98ea3_1440w.jpg" alt="img" style="zoom:50%;" />

**问题1：**即状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。

**问题2：**即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。

因此**2019**年出现了GIOU_Loss来进行改进。

**b.GIOU_Loss**

<img src="https://pic4.zhimg.com/80/v2-443123f1aa540f7dfdc84b233edcdc67_1440w.jpg" alt="img" style="zoom:45%;" />

可以看到右图GIOU_Loss中，增加了相交尺度的衡量方式，缓解了单纯IOU_Loss时的尴尬。

但为什么仅仅说缓解呢？

因为还存在一种**不足**：

<img src="https://pic3.zhimg.com/80/v2-49024c2ded9faafe7639c5207e575ed6_1440w.jpg" alt="img" style="zoom:44%;" />

**问题**：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的**GIOU值**也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。
基于这个问题，**2020年**的AAAI又提出了**DIOU_Loss**。

**c.DIOU_Loss**

好的目标框回归函数应该考虑三个重要几何因素：**重叠面积、中心点距离，长宽比。**

针对IOU和GIOU存在的问题，作者从两个方面进行考虑

**一：如何最小化预测框和目标框之间的归一化距离？**

**二：如何在预测框和目标框重叠时，回归的更准确？**

针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）

<img src="https://pic1.zhimg.com/80/v2-029f094658e87f441bf30c80cb8d07d0_1440w.jpg" alt="img" style="zoom:45%;" />

DIOU_Loss考虑了**重叠面积**和**中心点距离**，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。

但就像前面好的目标框回归函数所说的，没有考虑到长宽比。

<img src="https://pic4.zhimg.com/80/v2-22bf2e9c8a2fbbbb877e0f1ede69009f_1440w.jpg" alt="img" style="zoom:50%;" />

比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。

但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。

针对这个问题，又提出了CIOU_Loss，不得不说，科学总是在解决问题中，不断进步！！

**d.CIOU_Loss**

CIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。

![img](https://pic2.zhimg.com/80/v2-a24dd2e0d0acef20f6ead6a13b5c33d1_1440w.jpg)

其中v是衡量长宽比一致性的参数，我们也可以定义为：

![img](https://pic2.zhimg.com/80/v2-5abd8f82d7e30bdf21d2fd5851cb53a1_1440w.jpg)

这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。



再来综合的看下各个Loss函数的不同点：

- **IOU_Loss：**主要考虑检测框和目标框重叠面积。
- **GIOU_Loss：**在IOU的基础上，解决边界框不重合时的问题。
- **DIOU_Loss：**在IOU和GIOU的基础上，考虑边界框中心点距离的信息。
- **CIOU_Loss：**在DIOU的基础上，考虑边界框宽高比的尺度信息。

Yolov4中采用了**CIOU_Loss**的回归方式，使得预测框回归的**速度和精度**更高一些。

**（2）DIOU_nms**

Nms主要用于预测框的筛选，常用的目标检测算法中，一般采用普通的nms的方式，Yolov4则借鉴上面D/CIOU loss的论文：[https://arxiv.org/pdf/1911.08287.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1911.08287.pdf)

将其中计算IOU的部分替换成DIOU的方式：

再来看下实际的案例

<img src="https://pic3.zhimg.com/80/v2-ddb336d26adb2a2e37415b6266c88ec6_1440w.jpg" alt="img" style="zoom:50%;" />

在上图重叠的摩托车检测中，中间的摩托车因为考虑边界框中心点的位置信息，也可以回归出来。

因此在重叠目标的检测中，**DIOU_nms**的效果优于**传统的nms**。

**注意：有读者会有疑问，这里为什么不用CIOU_nms，而用DIOU_nms?**

**答：**因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。

但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。

# 5、Yolo v5

> https://zhuanlan.zhihu.com/p/172121380

## 5.1 网络结构图

![img](https://pic1.zhimg.com/80/v2-770a51ddf78b084affff948bb522b6c0_1440w.jpg)

## 5.2 创新

### 5.2.1 输入端创新

**（1）Mosaic数据增强**

和 Yolo v4一样

**（2） 自适应锚框计算**

在Yolo算法中，针对不同的数据集，都会有**初始设定长宽的锚框**。

在网络训练中，网络在初始锚框的基础上输出预测框，进而和**真实框groundtruth**进行比对，计算两者差距，再反向更新，**迭代网络参数**。

在Yolov3、Yolov4中，训练不同的数据集时，计算初始锚框的值是通过单独的程序运行的。

但Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。

当然，如果觉得计算的锚框效果不是很好，也可以在代码中将自动计算锚框功能**关闭**。

![img](https://pic3.zhimg.com/80/v2-807f03432ab4deb4a959d2dddd95923e_1440w.png)

控制的代码即**train.py**中上面一行代码，设置成**False**，每次训练时，不会自动计算。

**（3）自适应图片缩放**

在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。

比如Yolo算法中常用**416\*416，608\*608**等尺寸，比如对下面**800\*600**的图像进行缩放。

<img src="https://pic1.zhimg.com/80/v2-7cfa86448e0a543d613f2f8e64c63ce4_1440w.jpg" alt="img" style="zoom:50%;" />

但**Yolov5代码**中对此进行了改进，也是**Yolov5推理速度**能够很快的一个不错的trick。

作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。

因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像**自适应的添加最少的黑边**。

- **第一步：计算缩放比例**

  <img src="https://pic1.zhimg.com/v2-e15a41b2ba677b8546769d30b0654c1c_r.jpg" alt="preview" style="zoom:30%;" />

  选择小的缩放系数。

- **第二步：计算缩放后的尺寸**

  <img src="https://pic3.zhimg.com/80/v2-2c9a77f1b484d49ce3beba9c70a2effe_1440w.jpg" alt="img" style="zoom:33%;" />

  原始图片的长宽都乘以最小的缩放系数0.52，宽变成了416，而高变成了312。

- **第三步：计算黑边填充数值**

  <img src="https://pic3.zhimg.com/80/v2-799a7cb6bb7e5994472f0162abc4cf02_1440w.jpg" alt="img" style="zoom:33%;" />

  将416-312=104，得到原本需要填充的高度。再采用numpy中np.mod取余数的方式，得到8个像素，再除以2，即得到图片高度两端需要填充的数值。

此外，需要注意的是：

- a.这里填充的是黑色，即**（0，0，0）**，而Yolov5中填充的是灰色，即**（114,114,114）**，都是一样的效果。
- b.训练时没有采用缩减黑边的方式，还是采用传统填充的方式，即缩放到416*416大小。只是在测试，使用模型推理时，才采用缩减黑边的方式，提高目标检测，推理的速度。
- c.为什么np.mod函数的后面用**32**？因为Yolov5的网络经过5次下采样，而2的5次方，等于**32**。所以至少要去掉32的倍数，再进行取余。

### 5.2.2 Backbone

**（1）Focus结构**

![img](https://pic3.zhimg.com/80/v2-5c6d24c95f743a31d90845a4de2e5a36_1440w.jpg)

Focus结构，在Yolov3&Yolov4中并没有这个结构，其中比较关键是切片操作。

比如右图的切片示意图，4*4*3的图像切片后变成2*2*12的特征图。

以Yolov5s的结构为例，原始608*608*3的图像输入Focus结构，采用切片操作，先变成304*304*12的特征图，再经过一次32个卷积核的卷积操作，最终变成304*304*32的特征图。

**需要注意的是**：Yolov5s的Focus结构最后使用了32个卷积核，而其他三种结构，使用的数量有所增加，先注意下，后面会讲解到四种结构的不同点。

**（2）CSP结构**

Yolov4网络结构中，借鉴了CSPNet的设计思路，在主干网络中设计了CSP结构。

<img src="https://pic4.zhimg.com/80/v2-1da2f1a93c888dfe7c9002de56cff02b_1440w.jpg" alt="img" style="zoom:50%;" />

Yolov5与Yolov4不同点在于，Yolov4中只有主干网络使用了CSP结构。

而Yolov5中设计了两种CSP结构，以**Yolov5s网络**为例，**CSP1_X结构**应用于**Backbone主干网络**，另一种**CSP2_X**结构则应用于**Neck**中。

<img src="https://pic1.zhimg.com/80/v2-76933f7cd2400be642a0ad48e8c401e4_1440w.jpg" alt="img" style="zoom:67%;" />

### 5.2.3 Neck

Yolov4的Neck结构中，采用的都是普通的卷积操作。而Yolov5的Neck结构中，采用借鉴CSPnet设计的CSP2结构，加强网络特征融合的能力。

<img src="https://pic4.zhimg.com/80/v2-d8d0ff4768a92c9a10adbe08241c0507_1440w.jpg" alt="img" style="zoom:67%;" />

### 5.2.4 输出端

**（1）Bounding box损失函数**

Yolov5中采用其中的GIOU_Loss做Bounding box的损失函数。

而Yolov4中采用CIOU_Loss作为目标Bounding box的损失。

**（2）nms非极大值抑制**

Yolov4在DIOU_Loss的基础上采用DIOU_nms的方式，而Yolov5中采用加权nms的方式。

# 6、PP-Yolo

> https://mp.weixin.qq.com/s/pHOFqFihkkRVTYbkSTlG4w
>
> 

# 7、scale-Yolo v4

> code:https://github.com/wangermeng2021/Scaled-YOLOv4-tensorflow2



# 8、附录

## 8.1 mish激活函数

> 常见激活函数：https://zhuanlan.zhihu.com/p/139696588

<img src="https://pic3.zhimg.com/80/v2-896f1e7c34a9d35ab3baf75777a244c6_1440w.jpg" alt="img" style="zoom:80%;" />

```python
# tf
from tensorflow.keras import backend as K

class Mish(layer):
    def __init__(self, **kwargs):
        super(Mish, self).__init__(**kwargs)
        
    def call(self, inputs):
        return inputs * K.tanh(K.softplus(inputs))

    
# torch
import torch.nn as nn

class Mish(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self,x):
        x = x * (torch.tanh(nn.Softplus(x)))
        return x
```

## 8.2 CSPNet

> https://zhuanlan.zhihu.com/p/116611721

<img src="https://pic4.zhimg.com/80/v2-8bdfc298ccc95672bbeb17dd1c710b03_720w.jpg" alt="img" style="zoom:50%;" />

```python
def CSP_Block(x, num_filters, num_blocks, all_narrow=True):
    # 填充x的边界为0，由(?, 416, 416, 32)转换为(?, 417, 417, 32)。
    # 因为下一步卷积操作的步长为2，所以图的边长需要是奇数。
    # 对高和宽进行压缩
    x = ZeroPadding2D(((1, 0), (1, 0)))(x)
    x = DarknetConv2D_BN_Mish(num_filters, (3,3), strides=(2,2))(x)
    
    # 残差
    x_1 = DarknetConv2D_BN_Mish(num_filters//2 if all_narrow else num_filters, (1,1))(x)
    
    # 主干
    x_2 = DarknetConv2D_BN_Mish(num_filters//2 if all_narrow else num_filters, (1,1))(x)
    for i in range(num_blocks):
        x_blocks = compose(
                DarknetConv2D_BN_Mish(num_filters//2, (1,1)),
                DarknetConv2D_BN_Mish(num_filters//2 if all_narrow else num_filters, (3,3)))(x_2)
        x_2 = Add()([x_2, x_blocks])
    x_2 = DarknetConv2D_BN_Mish(num_filters//2 if all_narrow else num_filters, (1,1))(x_2)
    
    # 主干、残差汇合
    x = Concatenate()([x_2, x_1])
    
    return DarknetConv2D_BN_Mish(num_filters,(1,1))(x)
```



## 8.3 理解 Yolov3 的 anchor、置信度和类别概率

> https://www.jianshu.com/p/86b8208f634f

在网络最后的输出中，对于每个grid cell产生3个bounding box。每个bounding box的输出有三类参数：

- 一个是对象的box参数，一共是四个值，即**box的中心点坐标（x,y）和box的宽和高（w,h）**;
- 一个是**置信度**，这是个区间在[0,1]之间的值；
- 最后一个是**一组条件类别概率**，都是区间在[0,1]之间的值，代表概率。

### 8.3.1 anchor box

anchor box其实就是从训练集的所有ground truth box中统计(使用k-means)出来的在训练集中最经常出现的几个box形状和尺寸。

- **有助于模型快速收敛**。
- anchor box其实就是对预测的对象范围进行约束，并加入了尺寸先验经验，从而实现多尺度学习的目的。

最终负责预测grid cell中对象的box的最小单元是bounding box。按照v3的设计，每个grid cell会输出 3 个参数相同的 bounding box。之前的 v2 设计中，每个grid cell只负责预测一个对象，而**v3中grid cell中的三个 bounding box 都可以预测对象**，当然他们应该对应不同的ground truth。

**利用 anchor box 计算 bounding box**

> 其中，p\_{w}和p\_{h}为anchor box的宽和高，
> t\_{w}和t\_{h}为bounding box直接预测出的宽和高
> b\_{w}和b\_{h}为转换后预测的实际宽和高，
>
> 
>
> t\_{x}和t\_{y}是预测出的坐标
>
> c\_{x}和c\_{y}的坐标是(0,0) (0,1),(0,2),(0,3)…(0,13)
> (1,0),(1,1),(1,2),(1,3)…(1,13)等等
>
> b\_{x}和b\_{y}为转换后预测的实际坐标



![[公式]](https://www.zhihu.com/equation?tex=b_x%3D%5Csigma+%28t_x%29+%2B+c_x+%5C%5C+b_y%3D%5Csigma+%28t_y%29+%2B+c_y+%5C%5C+b_w%3Dp_we%5E%7Bt_w%7D++%5C%5C+b_h%3Dp_he%5E%7Bt_h%7D+%5C%5C)

> 这里σ ( t x )为sigmoid函数，c\_{x}和c\_{y}分别为grid cell方格左上角点相对整张图片的坐标。
>
> 这里的sigmoid函数，是为了将t\_{x}和t\_{y}压缩到[0,1]之间。

### 8.3.2 置信度（confidence）

置信度是每个bounding box 输出的其中一个重要参数，作用定义有两重：一重是，代表当前 box 是否有对象![P_{r}(Object)](https://math.jianshu.com/math?formula=P_%7Br%7D(Object))，注意，这里**判断的是否是背景还是不是背景**；另一重，当当前box有对象，它预测box与物体真实的box可能的 ![IOU_{pred}^{truth}](https://math.jianshu.com/math?formula=IOU_%7Bpred%7D%5E%7Btruth%7D)的值，注意，这里**所说的物体真实的box实际是不存在的**，这只是模型表达自己框出了物体的**自信程度**。

用数学形式表示置信度：

![\begin{aligned} C_{i}^{j} & = P_{r}(Object) * IOU_{pred}^{truth} \end{aligned}](https://math.jianshu.com/math?formula=%5Cbegin%7Baligned%7D%20C_%7Bi%7D%5E%7Bj%7D%20%26%20%3D%20P_%7Br%7D(Object)%20*%20IOU_%7Bpred%7D%5E%7Btruth%7D%20%5Cend%7Baligned%7D)

其中，![C_{i}^{j}](https://math.jianshu.com/math?formula=C_%7Bi%7D%5E%7Bj%7D)表示第 i 个grid cell的第 j 个 bounding box 的置信度

那么怎么训练![C_{i}^{j}](https://math.jianshu.com/math?formula=C_%7Bi%7D%5E%7Bj%7D)

> 计算先验的 anchor box 与该对象的ground truth box 的 IOU最大，那它就负责预测这个对象。此时 C_i^j = 1，其他情况下 C_i^j = 0

### 8.3.3 类别概率

对象条件类别概率是一组概率的数组，数组的长度为当前模型检测的类别种类数量，**它的意义是当bounding box认为当前box中有对象时，要检测的所有类别中每种类别的概率**.

其实这个和分类模型最后使用softmax函数输出的一组类别概率是类似的，只是二者存在两点不同：

1. YOLO的对象类别概率中没有background一项，也不需要，因为对background的预测已经交给置信度了，所以它的输出是有条件的，那就是在置信度表示当前box有对象的前提下，所以条件概率的数学形式为 P_r(class_i|Object)
2. 分类模型中最后输出之前使用softmax求出每个类别的概率，也就是说各个类别之间是互斥的，而YOLOv3算法的每个类别概率是单独用逻辑回归函数(sigmoid函数)计算得出了，所以每个类别不必是互斥的，也就是说一个对象可以被预测出多个类别。

##  8.4 Eliminate grid sensitivity 消除网格敏感度

> https://zhuanlan.zhihu.com/p/139724869

YOLOv3中，预测框中心点坐标的计算方法是 ![[公式]](https://www.zhihu.com/equation?tex=b_%7Bx%7D%3D%5Csigma%28t_%7Bx%7D%29%2Bc_%7Bx%7D) 。鉴于 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%28%29) 函数的特点， ![[公式]](https://www.zhihu.com/equation?tex=b_%7Bx%7D) 很难取到![[公式]](https://www.zhihu.com/equation?tex=c_%7Bx%7D) 或者 ![[公式]](https://www.zhihu.com/equation?tex=c_%7Bx%7D%2B1)。 其现象就是预测框的中心位置不会落在grid boundary。YOLOv4的解决办法是

![[公式]](https://www.zhihu.com/equation?tex=b_%7Bx%7D%3D%5Calpha%5Csigma%28t_%7Bx%7D%29-%28%5Calpha-1%29%2F2%2Bc_%7Bx%7D)

就是cfg文件中的scale_x_y。实现代码见yolo_layer.c::forward_yolo_layer()，通过scal_add_cpu实现。

更详细的讨论请参考[#3293](https://link.zhihu.com/?target=https%3A//github.com/AlexeyAB/darknet/issues/3293)

## 8.5 Label smoothing 平滑

> https://www.codenong.com/cs106919907/
>
> https://zhuanlan.zhihu.com/p/116466239
>
> https://www.jianshu.com/p/6a5ea4ddbf32
>
> https://blog.csdn.net/sinat_36618660/article/details/100166957



## 8.6 正负样本分配

> https://jishuin.proginn.com/p/763bfbd3314f

该部分做的是确定正负样本，是在 anchor维度上。也就是确定所有的 anchor 哪些是正样本，哪些是负样本。

划分为正样本的 anchor 意味着负责 gt box 的预测，训练的时候就会计算 gt box 的loss。而负样本表明该 anchor 没有负责任何物体，当然也需要计算 loss，但是只计算 confidence loss，因为没有目标，所以无法计算 box loss 和类别 loss。

Yolo 还有一个设置就是忽略样本，也就是 被划分为负样本anchor 和 gt box有较大的iou，但是不负责预测它，忽略掉，不计算任何 loss。

总结一下：

- **正样本**：负责计算 gt box 的 anchor。loss 计算 box loss（包括中心点+宽高）+ confidence loss + 类别 loss；
- **负样本**：不负责计算 gt box 的anchor。loss 只计算 confidence loss；
- **忽略样例**：和 gt box 的iou大于一定阈值，但又不负责 gt box的 anchor，一般是中心点 grid cell 附件的其他 grid cell 里的 anchor。不计算任何loss

## 8.7 tf中python 的一些使用

## **补充**

### 1. “...”操作符

在Python中，“...”(ellipsis)操作符，表示其他维度不变，只操作最前或最后1维；

```python3
import numpy as np

x = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
"""
[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]]
"""
print(x.shape)  # (3, 4)
y = x[1:2, ...]
"""
[[5 6 7 8]]
"""
print(y)
```

### 2. 遍历数值组合

在YOLO v3中，当计算网格值时，需要由相对位置，转换为绝对位置，就是相对值，加上网格的左上角的值，如相对值(0.2, 0.3)在第(1, 1)网格中的绝对值是(1.2, 1.3)。当转换坐标值时，根据坐标点的位置，添加相应的初始值即可。这样，就需要遍历两两的数值组合，如生成0至12的网格矩阵。

通过arange -> reshape -> tile -> concatenate的组合，即可快速完成。

源码：

```python3
from keras import backend as K

grid_y = K.tile(K.reshape(K.arange(0, stop=3), [-1, 1, 1]), [1, 3, 1])
grid_x = K.tile(K.reshape(K.arange(0, stop=3), [1, -1, 1]), [3, 1, 1])

sess = K.get_session()
print(grid_x.shape)  # (3, 3, 1)
print(grid_y.shape)  # (3, 3, 1)
z = K.concatenate([grid_x, grid_y])
print(z.shape)  # (3, 3, 2)
print(sess.run(z))
"""
创建3x3的二维矩阵，遍历全部数组0~2
"""
```

### 3. ::-1

“::-1”是颠倒数组的值，例如：

```python3
import numpy as np

a = np.array([1, 2, 3, 4, 5])
print a[::-1]
"""
[5 4 3 2 1]
"""
```

### 4. Session

在Keras中，使用Session测试验证数据，实现：

```python3
from keras import backend as K

sess = K.get_session()
a = K.constant([2, 4])
b = K.constant([3, 2])
c = K.square(a - b)
print(sess.run(c))
```

## 8.8 Focal Loss 

> https://zhuanlan.zhihu.com/p/103623160

回顾一下常用的 `BinayCrossEntropyLoss` 公式如下：

<img src="https://pic3.zhimg.com/80/v2-350e60278495fc29f8d25ddda1157122_720w.jpg" alt="img" style="zoom:60%;" />

不难看出，CE是个“笨学生”。

考前复习的时候，**他不会划重点，对所有知识点 “一视同仁”**。
如果教科书上有100道例题，包括: 90道加减乘除 + 10道 三角函数。CE同学就会吭哧吭哧的“平均用力”反复练习这100道例题，结果可想而知——他会精通那90道个位数加减乘除题目，然后其他题目基本靠蒙。那10道他不会的题，往往还是分值高的压轴题。

嗯，大概就是这么个症状。

**解决方案**

-- 指个方向， 别再“平均用力”

**方法一：分科复习**

> 每个【科目】的难度是不同的； 你要花 30%的精力在四则运算，70%的精力在三角函数。 --- 老师告诉CE同学 第一个技巧

对应到公式中，就是针对每个类别赋予不同的权重，即下述 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_t)

<img src="https://pic1.zhimg.com/80/v2-675ea305b015cd3ce51c953cde4bfb28_720w.jpg" alt="img" style="zoom:70%;" />

### 方法二、刷题战术

> 每道【题目】的难度是不同的； 你要根据以往刷类似题时候的正确率来合理分配精力。
> --- 老师告诉CE同学 第二个技巧

观察CE中的 ![[公式]](https://www.zhihu.com/equation?tex=p_t) ，它反映了模型对这个样本的识别能力（即 “这个知识点掌握得有多好”）；显然，对于 ![[公式]](https://www.zhihu.com/equation?tex=p_t) 越大的样本，我们越要打压它对loss的贡献。

这里有个超参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) ; 直观来看， ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 越大 打压越重。如下图所示:

<img src="https://pic1.zhimg.com/80/v2-b06026daefe6e6e67dc680e8468ab02c_720w.jpg" alt="img" style="zoom:67%;" />

- 横轴是 ![[公式]](https://www.zhihu.com/equation?tex=p_t) , 纵轴是 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BFL%7D%28p_t%29)
- 总体来说，所有曲线都是单调下降的，即 “掌握越好的知识点越省力”
- 当 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%3D0) 时，FL退化成CE，即蓝色线条
- 当 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma) 很大时，线条逐步压低到绿色位置，即各样本对于总loss的贡献受到打压；中间靠右区段承压尤其明显

### 方法三、综合上述两者

<img src="https://pic4.zhimg.com/80/v2-cbb231731ce5755825b9d9abd6673e8f_720w.jpg" alt="img" style="zoom:67%;" />