# 目标检测-区域卷积神经网络（R-CNN）系列、

> https://zhuanlan.zhihu.com/p/43624561
>
> https://zh.d2l.ai/chapter_computer-vision/rcnn.html#R-CNN

# 1、RCNN

回顾一下RCNN的算法框架

- selective search选取建议框
- 构建AlexNet卷积神经网络进行特征提取
- 构建SVM分类器进行分类（非极大抑制选取合适的框）
- 构建回归器进行调整

# 2_0、前置知识

> [RoIPooling、RoIAlign笔记](https://www.cnblogs.com/wangyong/p/8523814.html)

# 2、Fast RCNN

## 2.1 改动

首先，Fast RCNN 相比于 RCNN 主要在以下方面进行了改进

- Fast RCNN 仍然使用selective search选取2000个建议框，但是这里不是将这么多建议框都输入卷积网络中。
  - 而是将原始图片输入卷积网络中得到特征图，再使用建议框对特征图提取特征框。
  - 这样做的好处是，原来建议框重合部分非常多，卷积重复计算严重，而这里每个位置都只计算了一次卷积，大大减少了计算量
- 由于建议框大小不一，得到的特征框需要转化为相同大小，这一步是通过ROI池化层来实现的（ROI表示region of interest即目标）
- Fast RCNN里没有SVM分类器和回归器了，分类和预测框的位置大小都是通过卷积神经网络输出的
- 为了提高计算速度，网络最后使用SVD代替全连接层

## 2.2 检测流程

- 拿到一张图片，使用selective search选取建议框

- 将原始图片输入卷积神经网络之中，获取特征图（最后一次池化前的卷积计算结果）

- 对每个建议框，从特征图中找到对应位置（按照比例寻找即可），截取出特征框（深度保持不变）

  - 

- 将每个特征框划分为 ![[公式]](https://www.zhihu.com/equation?tex=H%5Ctimes+W) 个网格（论文中是 ![[公式]](https://www.zhihu.com/equation?tex=7+%5Ctimes+7) ），在每个网格内进行池化（即每个网格内取最大值），这就是ROI池化。这样每个特征框就被转化为了 ![[公式]](https://www.zhihu.com/equation?tex=7%5Ctimes+7%5Ctimes+C) 的矩阵(其中C为深度)

  - 

- 对每个矩阵拉长为一个向量，分别作为之后的全连接层的输入

- 全连接层的输出有两个，计算分类得分和bounding box回归（bounding box表示预测时要画的框）。前者是sotfmax的21类分类器（假设有20个类别+背景类），输出属于每一类的概率（所有建议框的输出构成得分矩阵）；后者是输出一个 ![[公式]](https://www.zhihu.com/equation?tex=20%5Ctimes+4) 的矩阵，4表示(x, y, w, h)，20表示20个类，这里是对20个类分别计算了框的位置和大小

- 对输出的得分矩阵使用非极大抑制方法选出少数框，对每一个框选择概率最大的类作为标注的类，根据网络结构的第二个输出，选择对应类下的位置和大小对图像进行标注

  ![Fast R-CNN模型](https://zh.d2l.ai/_images/fast-rcnn.svg)

## 2.3 网络结构

论文中使用了多种网络结构进行训练，这里以VGG-16（AlexNet之后的又一经典网络）为例。

最开始仍然是在ImageNet数据集上训练一个1000类的分类网络。

然后对模型进行“三个变动”

- 将最后一个最大池化层换成 ROI 池化层
- 将最后一个全连接层和后面的 softmax 1000 分类器换成两个并行层，一个是全连接层 1+21 分类器，另一个是全连接层 2+表示每个预测框位置的输出。
- 输入的不再只是图片，还有提取到的建议框位置信息

变动后的模型结构如下：

![img](https://pic3.zhimg.com/80/v2-deca5ef9583a530eb7249b3b5bbfc04a_720w.jpg)

## 2.4 训练过程

使用变动后的模型，在标注过的图像数据上继续训练，训练时要输入图像、标注（这里将人为标注的框称为ground truth）和建议框信息。这里为了提高训练速度，采取了小批量梯度下降的方式，每次使用2张图片的128张建议框（每张图片取64个建议框）更新参数。

每次更新参数的训练步骤如下：

- 2张图像直接经过前面的卷积层获得特征图
- 根据ground truth标注所有建议框的类别。具体步骤为，对每一个类别的ground truth，与它的iou大于0.5的建议框标记为groud truth的类别，对于与ground truth的iou介于0.1到0.5之间的建议框，标注为背景类别
- 每张图片随机选取64个建议框（要控制背景类的建议框占75%），提取出特征框
- 特征框继续向下计算，进入两个并行层计算损失函数（损失函数具体计算见下面）
- 反向传播更新参数（关于ROI池化的反向传播细节可以参考[这篇博客](https://link.zhihu.com/?target=https%3A//blog.csdn.net/WoPawn/article/details/52463853)）

## 2.5 损失函数

损失函数分为两个部分，分别对应两个并行层

# 3、Faster RCNN

> https://zhuanlan.zhihu.com/p/32702387
>
> https://www.cnblogs.com/wangyong/p/8513563.html
>
> https://zhuanlan.zhihu.com/p/31426458

<img src="https://images2018.cnblogs.com/blog/75922/201803/75922-20180306111851933-70273855.png" alt="img" style="zoom:80%;" />

![Faster R-CNN模型](https://zh.d2l.ai/_images/faster-rcnn.svg)



## 3.1 改动

- 将传统的 Selective Search 提取目标的方法替换成网络训练来实现，使得全流程的检测、分类速度大幅提升。
- 与Fast R-CNN相比，只有生成提议区域的方法从选择性搜索变成了**RPN(Region Proposal Networks)**，而其他部分均保持不变。

## 3.2 RPN解读（一）

![img](https://pic1.zhimg.com/80/v2-74336cc69ba587ca1ebc9bf159579a14_720w.jpg)

粉色框内就是RPN，它做两件事：

> 1. 把feature map分割成多个小区域，识别出哪些小区域是前景，哪些是背景，简称RPN Classification，对应粉色框中上半分支；
> 2. 获取前景区域的大致坐标，简称RPN bounding box regression，对应下半分支；

**3.2.1 RPN Classification**

PRN classification 的过程就是个二分类的过程。先在 feature map 上均匀的划分出 K * H *  M 个区域（称为 anchor，K=9，H是feature map的高度，W是宽度），通过比较这些anchor和ground truth间的重叠情况来决定哪些anchor是前景，哪些是背景，也就是给每一个anchor都打上前景或背景的label。有了labels，你就可以对RPN进行训练使它对任意输入都具备识别前景、背景的能力。

> - 在上半分支中可以看出 rpn_cls_score_reshape 模块输出的结构是[1,9*H,W,2]，就是9xHxW个anchor二分类为前景、背景的概率；
> - anchor_target_layer模块输出的是每一个anchor标注的label，拿它和二分类概率一比较就能得出分类的loss。

一个feature map有9xHxW个anchor，就是说每个点对应有9个anchor，这9个anchor有1:1、1:2、2:1三种长宽比，每种长宽比都有三种尺寸（见下图）。

> - 一般来说原始输入图片都要缩放到固定的尺寸才能作为网络的输入，这个尺寸在作者源码里限制成800x600，9种anchor还原到原始图片上基本能覆盖800x600图片上各种尺寸的坐标。

<img src="https://pic3.zhimg.com/80/v2-004bb64a7c612d8fa33a268247132f26_720w.jpg" alt="img" style="zoom:63%;" />

**3.2.2 RPN bounding box regression**

RPN bounding box regression用于得出前景的大致位置，要注意这个位置并不精确。

前面的RPN classification给所有的anchor打上label后，我们需用一个表达式来建立anchor与ground truth的关系。

假设anchor中心位置坐标是[Ax, Ay]，长高为Aw和Ah，对应ground truth的4个值为[Gx,Gy,Gw,Gh]，他们间的关系可以用公式１来表示。[dx(A), dy(A), dw(A), dh(A)]就是anchor与ground truth之间的偏移量，由公式１可以推导出公式２，这里用对数来表示长宽的差别，是为了在差别大时能快速收敛，差别小时能较慢收敛来保证精度

<img src="https://pic2.zhimg.com/80/v2-8ffe9f9a9e929b7707dae010831f8ded_720w.jpg" alt="img" style="zoom:80%;" />

<img src="https://pic1.zhimg.com/80/v2-901d45fa03e1c4b673d9ac9b486403ac_720w.jpg" alt="img" style="zoom:80%;" />

有了这４个偏移量，你就可以拿他们去训练图2 RPN中下面一个分支的输出。完成训练后RPN就具备识别每一个anchor到与之对应的最优proposal偏移量的能力（[d'x(A), d'y(A), d'w(A), d'h(A)]），换个角度看就是得到了所有proposal的位置和尺寸。(要注意这里是相对于于proposal的，而不是相对于ground truth的)

有了偏移量再根据公式1就能算出proposal的大致位置。在这个过程中HxWx9个anchor能算出HxWx9个proposal，大多数都是聚集在ground truth周围的候选框，这么多相近的proposal完全没必要反而增加了计算量，这时就要用一些方法来精选出最接近ground truth的proposal，Ross Girshick给了三个步骤：

1. 先选出前景概率最高的N个proposal;
2. 做非极大值抑制(NMS)
3. NMS后再次选择前景概率最高的M个proposal;

经历这三个步骤后能够得到proposal的大致位置，但这还不够，为了得到更精确的坐标，你还要利用公式2再反推出这个大致的proposal和真实的ground truth间还有多少偏移量，对这个新的偏移量再来一次回归才是完成了精确的定位。

![img](https://pic1.zhimg.com/80/v2-15c312d3487bd32de453ddb131345ae8_720w.jpg)

proposal精确位置回归时计算loss的公式和公式3中RPN bounding box regression的loss计算方法完全相同，也用smooth L1方法。

**3.2.3 RPN 的 loss 计算**

RPN训练时要把RPN classification和RPN bounding box regression的loss加到一起来实现联合训练。公式3中*Ncls*是一个batch的大小256，*Lcls(pi, pi\*)*是前景和背景的对数损失，*pi*是anchor预测为目标的概率，就是前面rpn_cls_score_reshape输出的前景部分score值，*pi*\*是前景的label值，就是1，将一个batch所有loss求平均就是RPN classification的损失；公式3中*Nreg*是anchor的总数，λ是两种 loss的平衡比例，*ti*是图2中rpn_bbox_pred模块输出的[d'x(A), d'y(A), d'w(A), d'h(A)]，*t\*i*是训练时每一个anchor与ground truth间的偏移量，*t\*i*与*ti*用smooth L1方法来计算loss就是RPN bounding box regression的损失：

<img src="https://pic1.zhimg.com/80/v2-9ea28a5c8ff3a52b0bf0ee980b17c85c_720w.jpg" alt="img" style="zoom:80%;" />

<img src="https://pic2.zhimg.com/80/v2-296babc5c2f7906a7f6861acbaa45075_720w.jpg" alt="img" style="zoom:80%;" />

关于Smooth L1的原理，请参考：

[http://pages.cs.wisc.edu/~gfung/GeneralL1/L1_approx_bounds.pdf](https://link.zhihu.com/?target=https%3A//link.jianshu.com/%3Ft%3Dhttp%3A%2F%2Fpages.cs.wisc.edu%2F%7Egfung%2FGeneralL1%2FL1_approx_bounds.pdf)

## 3.2 RPN解读（二）

<img src="https://images2018.cnblogs.com/blog/75922/201806/75922-20180606060221885-1218551944.png" alt="img" style="zoom:80%;" />

其中‘rpn_loss_cls’、‘rpn_loss_bbox’是分别对应softmax，smooth L1计算损失函数，‘rpn_cls_prob’计算概率值(可用于下一层的nms非最大值抑制操作)

## 3.3 RPN的实际使用

在实际应用时并不是把全部HxWx9个anchor都拿来做label标注，这里面有些规则来去除效果不好的anchor，具体的规则如下：

1. 覆盖到feature map边界线上的anchor不参与训练；
2. 前景和背景交界地带的anchor不参与训练。这些交界地带即不作为前景也不作为背景，以防出现错误的分类。在作者原文里把IOU>0.7作为标注成前景的门限，把IOU<0.3作为标注成背景的门限，之间的值就不参与训练，IOU是anchor与ground truth的重叠区域占两者总覆盖区域的比例，见示意图4；
3. 训练时一个batch的样本数是256，对应同一张图片的256个anchor，前景的个数不能超过一半，如果超出，就随机取128个做为前景，背景也有类似的筛选规则；

<img src="https://pic3.zhimg.com/80/v2-bf7f316cb24be6cc91a10c2bd960838e_720w.jpg" alt="img" style="zoom:80%;" />

## 3.4 RoI Pooling 部分

ROI Pooling做了两件事：1、从feature maps中“抠出”proposals（大小、位置由RPN生成）区域；2、把“抠出”的区域pooling成固定长度的输出。

图6是pooling过程的示意图，feature map中有两个不同尺寸的proposals，但pooling后都是7x7=49个输出，这样就能为后面的全连接层提供固定长度的输入。这种pooling方式有别于传统的pooling，没有任何tensorflow自带的函数能实现这种功能，你可以自己用python写个ROI Pooling的过程，但这样就调用不了GPU的并行计算能力，所以作者的源码里用C++来实现整个ROI Pooling。

<img src="https://pic4.zhimg.com/80/v2-d3aaed1c9c47ee0765078af3d858d1ff_720w.jpg" alt="img" style="zoom:80%;" />

为什么要pooling成固定长度的输出呢？这个其实来自于更早提出的SPP Net，RPN网络提取出的proposal大小是会变化的，而**分类用的全连接层输入必须固定长度**，所以必须有个从可变尺寸变换成固定尺寸输入的过程。在较早的R-CNN和Fast R-CNN结构中都通过对proposal进行拉升（warp）或裁减（crop）到固定尺寸来实现，拉升、裁减的副作用就是原始的输入发生变形或信息量丢失（图7），以致分类不准确。而ROI Pooling就完全规避掉了这个问题，proposal能完整的pooling成全连接的输入，而且没有变形，长度也固定

<img src="https://pic3.zhimg.com/80/v2-7c089073d1707d8a77a2d50405756376_720w.jpg" alt="img" style="zoom:67%;" />



## 3.5 训练过程

为了便于说明，我们把RPN中的rpn classification和rpn bounding box regression统称为RPN训练；把proposal layer中对proposal精确位置的训练和最终的准确分类训练统称为R-CNN训练。Ross Girshick在论文中介绍了3种训练方法：

- **Alternating training**：RPN训练和R-CNN训练交替进行，共交替两次。训练时先用ImageNet预训练的结果来初始化网络，训练RPN，用得到的proposal再训练R-CNN，之后用R-CNN训练出的参数来初始网络，再训练一次RPN，最后用RPN训练出的参数来初始化网络，最后训练次R-CNN，就完成了全部的训练过程。
- **Approximate joint training**：这里与前一种方法不同，不再是串行训练RPN和R-CNN，而是尝试把二者融入到一个网络内一起训练。这里Approximate 的意思是指把RPN bounding box regression部分反向计算得到的梯度完全舍弃，不用做更新网络参数的权重。Approximate joint training相对于Alternating traing减少了25-50%的训练时间。
- **Non-approximate training**：该方法和Approximate joint training基本一致，只是不再舍弃RPN bounding box regression部分得到的梯度。

# 4、FPN(偏论文解读)

> https://zhuanlan.zhihu.com/p/60340636

FPN 用来解决Faster RCNN物体检测算法在处理多尺度变化问题时的不足。Faster RCNN 无论是 RPN 网络 还是 Fast RCNN 网络，都是基于高层特征（conv4）。

这种做法一个明显的缺陷是对小物体不友好。为了处理小物体，经典的方法是采用图像金字塔的方法在训练或测试阶段对图片进行多尺度变换增强。但是会带来极大的计算量。

构造一种独特的特征金字塔来避免图像金字塔的计算量过高的问题，同时能较好的处理物体检测中的多尺度变化问题。

## 4.1 优缺点

虽然特征金字塔网络对传统方法和CNN方法都有一定程度的帮助，它有一个重大的缺陷无法忽视：带来了极大的计算量和内存需求。因此，现在的检测算法（例如Fast/Faster RCNN）一般在**训练时采用单尺度的方式**来加速训练，同时**测试时用多尺度的方式**来提升最终的性能。

除了图像金字塔可以用来处理多尺度的物体，深度网络本身具有的多层次结构也可以用来提取多尺度的特征（图1(c)）。例如**SSD方法[6]中就利用了多个特征层分别做预测**。然而，由于**底层的语义特征比较弱**，在处理小物体（特征一般只出现在较低的特征层）时效果表现得不够好。

提出一种合理利用深度卷积网络各个层次特征的方法。

## 4.2 相关工作

- OverFeat方法采用滑窗的方式从图像中选取各个尺度的图像块，然后用卷积神经网络来做识别。
- R-CNN用区域选择算法来选取一定数量的候选框（一般是2000个），然后将这些框统一缩放到深度卷积网络的输入像素大小，并用深度卷积网络来做识别。
- SPPNet[5]提出可以将单一尺度的图像输入到网络，然后在特征层用不同尺度的下采样来得到特征金字塔。这种处理方法比传统的图像金字塔更加高效。
- Fast RCNN和Faster RCNN则只采用了单一的尺度特征来平衡速度和精度。

## 4.3 FPN 结构

第一部分是自底向上的过程，第二部分是自顶向下和侧向连接的融合过程。

**自底向上的过程**：自底向上的过程和普通的CNN没有区别。

- 现代的CNN网络一般都是按照特征图大小划分为不同的stage，每个stage之间特征图的尺度比例相差为2。
- 在FPN中，每个stage对应了一个特征金字塔的级别（level），并且每个stage的最后一层特征被选为对应FPN中相应级别的特征。
  - 以ResNet为例，选取conv2、conv3、conv4、conv5层的最后一个残差block层特征作为FPN的特征，记为{C2、C3、C4、C5}。这几个特征层相对于原图的步长分别为4、8、16、32。

**自顶向下过程以及侧向连接**：自顶向下的过程通过上采样（up-sampling）的方式将顶层的小特征图（例如20）放大到上一个stage的特征图一样的大小（例如40）。上采样的方法可以用最近邻差值实现。

- 这样的好处是既利用了顶层较强的语义特征（利于分类），又利用了底层的高分辨率信息（利于定位）。

- 为了将高层语义特征和底层的精确定位能力结合，作者提出类似于残差网络的侧向连接结构。侧向连接将上一层经过上采样后和当前层分辨率一致的特征，通过相加的方法进行融合。（这里为了修正通道数量，将当前层先经过1x1卷积操作。）

  <img src="https://pic4.zhimg.com/v2-8e3aa638e06cbe3c6f86789c797972b3_r.jpg" alt="preview" style="zoom:70%;" />

具体的，C5层先经过1x1卷积，得到M5特征。M5通过上采样，再加上C4经过1x1卷积后的特征，得到M4。这个过程再做两次，分别得到M3和M2。M层特征再经过3x3卷积，得到最终的P2、P3、P4、P5层特征。另外，和传统的图像金字塔方式一样，所有M层的通道数都设计成一样的，本文都用d=256。细节图如下所示（以ResNet为例）：

<img src="https://pic2.zhimg.com/80/v2-38ecc58507df271897fdae605868d6e1_1440w.png" alt="img" style="zoom:70%;" />

FPN本身不是检测算法，只是一个特征提取器。它需要和其他检测算法结合才能使用。

[从代码细节理解 FPN （Feature Pyramid Networks for Object Detection）](https://zhuanlan.zhihu.com/p/35854548)



# 5、Mask RCNN

> https://zhuanlan.zhihu.com/p/37998710
>
> https://zhuanlan.zhihu.com/p/60340636

## 5.1 FPN+Fast RCNN

Faster RCNN中的RPN是通过最后一层的特征来做的。最后一层的特征经过3x3卷积，得到256个channel的卷积层，再分别经过两个1x1卷积得到类别得分和边框回归结果。

这里将特征层之后的RPN子网络称之为网络头部（network head）。

对于特征层上的每一个点，作者用anchor的方式预设了9个框。这些框本身包含不同的尺度和不同的长款比例。

FPN针对RPN的改进是将网络头部应用到每一个P层。由于每个P层相对于原始图片具有不同的尺度信息，因此作者将原始RPN中的尺度信息分离，让每个P层只处理单一的尺度信息。具体的，对{32^2、64^2、128^2、256^2、512^2}这五种尺度的anchor，分别对应到{P2、P3、P4、P5、P6}这五个特征层上。每个特征层都处理1:1、1:2、2:1三种长宽比例的候选框。P6是专门为了RPN网络而设计的，用来处理512大小的候选框。它由P5经过下采样得到。

另外，上述5个网络头部的参数是共享的。作者通过实验发现，网络头部参数共享和不共享两种设置得到的结果几乎没有差别。

<img src="https://pic1.zhimg.com/80/v2-18b0db72ed142c8208c0644c8b5a8090_1440w.jpg" alt="img" style="zoom:80%;" />

**FPN应用于Fast RCNN**

作者将FPN的各个特征层类比为图像金字塔的各个level的特征，从而将不同尺度的RoI映射到对应的特征层上。以224大小的图片输入为例，宽高为w和h的RoI将被映射到的特征级别为k，它的计算公式如下：

![[公式]](https://www.zhihu.com/equation?tex=k%3D%5Cleft+%5Clfloor+k_%7B0%7D%2B%5Clog_%7B2%7D%28%5Csqrt%7Bwh%7D%2F224%29+%5Cright+%5Crfloor)

在ResNet中，k0的值为4，对应了长宽为224的框所在的层次。如果框的长宽相对于224分别除以2，那么k的值将减1，以此类推。

在Faster RCNN中，ResNet的conv4层被用来提取RoI，经过RoI Pooling后映射到14x14的大小。经过RoI Pooling后的特征再进入原来的conv5层，进而得到最终的分类和边框回归结果。在FPN中，conv5层已经被用来作为特征提取器得到P5层；因此，这里单独设计两个1024维的全连接层作为检测网络的网络头部。新的网络头部是随机初始化的，它相比于原来的conv5层更加轻量级。

> 没看完