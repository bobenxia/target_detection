# Yolo v3/v4 的继续理解

# Yolo v3

这篇的理解入口从获得三张特征图开始。

> https://zhuanlan.zhihu.com/p/138857662
>
> https://zhuanlan.zhihu.com/p/95598987

## 1、v3 的 anchor

锚，anchor，理解成基础框/先验框。后面需要以这些框为基础进行 bounding box 微调。

在YOLO V2中，设置了5个宽高比例的anchor（通过聚类获得），每个cell（特征图的一个单元格）负责的anchor的数量为5，而在YOLO V3中，共设置9个宽高不同的anchor（同样是通过聚类获取得到），每个cell的anchor的数量为9/3=3个，因为YOLO V3**有3个feature_map**，不同feature_map的**size和感受野**是不一样的，**较小**size的feature_map具有**较大的感受野**，所以负责检测**较大**的物体，同理，**较大**size的feature_map**感受野较小**，负责检测**较小**的物体。

所以在训练之前，我们需要根据自己的数据集通过聚类生成适合自己数据集训练的基础框 anchor，这样可以加速自己的训练过程。

## 2、v3的前向传播

Yolov3 接收 **416x416x3** 的图片，返回三个**不同size**的**特征图y1（13x13x255）,y2（26x26x255）,y3（52x52x255）**,这里的255是在**COCO数据集**下的结果。

因为COCO数据集有80类物体，每个特征图的cell负责三个先验框anchor，则有

```text
255 = 3 x [80(类别估计概率分布)+1（置信度）+4（预测框）]
```

## 3、v3 的损失函数

v3 的损失函数看似有五个部分，实际是三个部分，即

```
预测框的位置损失 + 预测框预测的类别损失 + 预测框置信度损失
```

损失函数如下：

![img](https://pic3.zhimg.com/80/v2-2ae5b55a1092621ff84f16dd182c87ba_720w.png)

```python
 def loss_layer(self, feature_map_i, y_true, anchors):
        '''
        '''
        ###########
        # get mask
        ###########

            """
            对每张图片进行分析，循环batch size下
            """
            """
            每次循环对一张图片中所有预测框和真实框求IOU
            并且剔除一些质量不好的框
            """

        ############
        # loss_part
        ############
        
        #下面求框的损失

        #下面求置信度损失

        #分类损失
        

        return xy_loss, wh_loss, conf_loss, class_loss
```

代码基本框架如上。

开始一步步分析

损失函数的输入为**某一特征图**（即YOLO V3的输出张量），**y_true**（图片标注的信息），**聚类获得的anchor**(这里不同特征图对应的anchor应当也是不同的)。

这里我们详细的说一下y_true的结构，这里我们以特征图宽高为 13 x 13的为例：

```text
y_ture 的shape为 [N, 13, 13, 3, 4 + 1 + num_classes + 1]
```

这里的

- N为batch_size
- 13为特征图宽高
- 3为anchor数量（每个特征图对应固定3个anchor）
- 4为坐标信息（框中心横坐标、框中心纵坐标、宽、高）
- 1为置信度
- num_classes是一个分类标签，所属分类对应位置为1，其他为0
- 最后一个1指的是mix_w，默认值为1

### 3.1 

```python
# size in [h, w] format! don't get messed up!
grid_size = tf.shape(feature_map_i)[1:3] #输出feature map的宽、高
# the downscale ratio in height and weight
ratio = tf.cast(self.img_size / grid_size, tf.float32) #返回的该特征图相对于原图的scale
# N: batch_size
N = tf.cast(tf.shape(feature_map_i)[0], tf.float32)

# 将特征图上预测的框的位置，大小通过anchor先验框映射到原图大小，这里的x_y_offset, pred_boxes都是原图尺寸的
x_y_offset, pred_boxes, pred_conf_logits, pred_prob_logits = self.reorg_layer(feature_map_i, anchors)
```

- 首先，从特征图中获得 feature map 的尺寸（宽、高）
- 然后，计算特征图相对于原图的 scale，利用原图尺寸/特征图尺寸
- 再，获得 batch 大小

通过 feature 获得一系列参数后，现在将特征图上预测的框（位置、大小）信息，通过函数，映射到原图大小，方便后面和真实框进行 IOU 计算

> reorg_layer 就是这里的函数，不记录相关代码了，源代码在上面链接中的github链接中（狗头）

### 3.2 

接着，我们解析损失函数中的两个值， ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bobj%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bnoobj%7D)

他们均有下面代码生成，这里的**object_mask**指的是 ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bobj%7D) ，**ignore_mask**指的是 ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bnoobj%7D)

```python
###########
# get mask
###########

# shape: take 416x416 input image and 13*13 feature_map for example:
# [N, 13, 13, 3, 1]
object_mask = y_true[..., 4:5] #哪些cell负责检测物体，则该cell为1（三个位置都为1），否则为0

# the calculation of ignore mask if referred from
#循环操作，循环的是batch_size，意思就是对每张图片进行分析
ignore_mask = tf.TensorArray(tf.float32, size=0, dynamic_size=True)
def loop_cond(idx, ignore_mask): #如果idx小于N
    """
    对每张图片进行分析，循环batch size下
    """
    return tf.less(idx, tf.cast(N, tf.int32))
def loop_body(idx, ignore_mask):
    """
    每次循环对一张图片中所有预测框和真实框求IOU
    并且剔除一些质量不好的框
    """
    # shape: [13, 13, 3, 4] & [13, 13, 3]  ==>  [V, 4]
    # V: num of true gt box of each image in a batch
    valid_true_boxes = tf.boolean_mask(y_true[idx, ..., 0:4], tf.cast(object_mask[idx, ..., 0], 'bool'))
    # shape: [13, 13, 3, 4] & [V, 4] ==> [13, 13, 3, V]
    iou = self.box_iou(pred_boxes[idx], valid_true_boxes)
    # shape: [13, 13, 3]
    best_iou = tf.reduce_max(iou, axis=-1)
    # shape: [13, 13, 3]
    ignore_mask_tmp = tf.cast(best_iou < 0.5, tf.float32)
    # finally will be shape: [N, 13, 13, 3]
    ignore_mask = ignore_mask.write(idx, ignore_mask_tmp)
    return idx + 1, ignore_mask
_, ignore_mask = tf.while_loop(cond=loop_cond, body=loop_body, loop_vars=[0, ignore_mask])
# [N, 13, 13, 3]
ignore_mask = ignore_mask.stack()
# shape: [N, 13, 13, 3, 1]
ignore_mask = tf.expand_dims(ignore_mask, -1)
```

我们可以看出，特征图的哪个单元格**负责对应真实框的检测，**则该单元格中的3个anchor对应的置信度均为1，否则为0，这就是 ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bobj%7D) 。

接着作者做的事，并不是将 ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bobj%7D) 的补集设为 ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bnoobj%7D) ，，而是选择一些质量比较差（与真实框IOU<0.5)的设定为 ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bnoobj%7D) ，这在上面代码中是得到体现的。

### 3.3 

接着，作者对预测框和真实框的**位置信息**做了两个转换：

```python
# shape: [N, 13, 13, 3, 2]
# 以下都是相对于原图的
pred_box_xy = pred_boxes[..., 0:2] #预测的中心坐标
pred_box_wh = pred_boxes[..., 2:4] #预测的宽高

# get xy coordinates in one cell from the feature_map
# numerical range: 0 ~ 1 （数值范围）（转为归一化）
# shape: [N, 13, 13, 3, 2]
true_xy = y_true[..., 0:2] / ratio[::-1] - x_y_offset  #将真实的框的中心坐标归一化（映射到特征图）
pred_xy = pred_box_xy / ratio[::-1] - x_y_offset #将预测的框的中心坐标归一化（映射到特征图）

# get_tw_th
# numerical range: 0 ~ 1
# shape: [N, 13, 13, 3, 2]
true_tw_th = y_true[..., 2:4] / anchors # 真实的框的宽高相对于anchor的比例
pred_tw_th = pred_box_wh / anchors #预测的框的宽高相对于anchor的比例

# for numerical stability 为了数值稳定
# where会先判断第一项是否为true,如果为true则返回 x；否则返回 y;
true_tw_th = tf.where(condition=tf.equal(true_tw_th, 0),
                        x=tf.ones_like(true_tw_th), y=true_tw_th) #不允许出现宽高为0的情况，出现了就返回一个宽和高都为1的情况
pred_tw_th = tf.where(condition=tf.equal(pred_tw_th, 0),
                        x=tf.ones_like(pred_tw_th), y=pred_tw_th) 
# 下面true_tw_th进行指数运算和anchor运行就还原成原来的样子
# 这样子做也是为了后面求损失函数方便，毕竟损失函数不能太大了
true_tw_th = tf.log(tf.clip_by_value(true_tw_th, 1e-9, 1e9)) #真实框通过上述转换，转换为特征图尺寸上的宽高（其实还要进行log）
pred_tw_th = tf.log(tf.clip_by_value(pred_tw_th, 1e-9, 1e9))
```

可以看出，作者做这两个变换是非常灵活的，这样做的目的当然也是**便于损失函数的计算**啦！

上述的true_xy 和pred_xy是将预测框和真实框的**中心坐标**（原图比例）转换为下图中的 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%28t_x%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%28t_y%29+) 。

<img src="https://pic4.zhimg.com/80/v2-59ad3bd2926bf7c60bdafe2196644eab_720w.jpg" alt="img" style="zoom:70%;" />

上述的true_tw_th和pred_tw_th是将预测框和真实框的**宽高信息**（原图比例）转换为上图中的 ![[公式]](https://www.zhihu.com/equation?tex=t_h) 和 ![[公式]](https://www.zhihu.com/equation?tex=t_w++) 。

### 3.4 

接着，代码出现了一行

```python
# box size punishment: 框大小的惩罚，就是坐标损失前的权重，大的框权重设置小一点，小的设置大一点
# box with smaller area has bigger weight. This is taken from the yolo darknet C source code.
# shape: [N, 13, 13, 3, 1]
# 后面一串为真实框面积相对于原图的
box_loss_scale = 2. - (y_true[..., 2:3] / tf.cast(self.img_size[1], tf.float32)) * (y_true[..., 3:4] / tf.cast(self.img_size[0], tf.float32))
```

这个box_loss_scale的作用，原论文中也提及，对于**大**的检测框，其**位置损失**要**远大于，小**的框的**位置损失**。所以设置这个的意思很明显，对**大**的框的位置损失**降低敏感度，**对**小**的框的位置损失**增加敏感度。**

### 3.5 计算损失

1、**位置损失**

![img](https://pic4.zhimg.com/80/v2-2a282c1e22b0c6926a528d9c0482e5eb_720w.jpeg)

```python
#下面求框的损失
# [N, 13, 13, 3, 1]
mix_w = y_true[..., -1:] 
# shape: [N, 13, 13, 3, 1]
xy_loss = tf.reduce_sum(tf.square(true_xy - pred_xy) * object_mask * box_loss_scale * mix_w) / N #均方误差
wh_loss = tf.reduce_sum(tf.square(true_tw_th - pred_tw_th) * object_mask * box_loss_scale * mix_w) / N #均方误差
```

这和公式是一一对应，非常直观的。

2、**置信度损失**

![img](https://pic4.zhimg.com/80/v2-61f409a5cb18d34b93c4704307d55033_720w.jpg)

```python
#下面求置信度损失
# shape: [N, 13, 13, 3, 1]
conf_pos_mask = object_mask #那些cell负责物体检测，这个cell的三个格子就是1，否则为0
conf_neg_mask = (1 - object_mask) * ignore_mask #并不是选择所有的非负责cell中的框作为消极样本，而是选择IOU<0.5的为负样本，其余不考虑
conf_loss_pos = conf_pos_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf_logits)
conf_loss_neg = conf_neg_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf_logits)
# TODO: may need to balance the pos-neg by multiplying some weights
conf_loss = conf_loss_pos + conf_loss_neg

if self.use_focal_loss:
    alpha = 1.0
    gamma = 2.0
    # TODO: alpha should be a mask array if needed
    focal_mask = alpha * tf.pow(tf.abs(object_mask - tf.sigmoid(pred_conf_logits)), gamma)
    conf_loss *= focal_mask
conf_loss = tf.reduce_sum(conf_loss * mix_w) / N
```

代码中区分了positive和negative置信度，这里我感觉好像和上面的 ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bobj%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=1_%7Bij%7D%5E%7Bnoobj%7D) 没什么区别。

3、**分类损失**

![img](https://pic4.zhimg.com/80/v2-219537c123de3290c02e49e52cdbcc97_720w.jpg)

```python
# shape: [N, 13, 13, 3, 1]
# whether to use label smooth
if self.use_label_smooth:
    delta = 0.01
    label_target = (1 - delta) * y_true[..., 5:-1] + delta * 1. / self.class_num
else:
    label_target = y_true[..., 5:-1]

#分类损失
class_loss = object_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_target, logits=pred_prob_logits) * mix_w
class_loss = tf.reduce_sum(class_loss) / N
```

# Yolo v4

> https://zhuanlan.zhihu.com/p/150127712

![preview](https://pic3.zhimg.com/v2-4d60d4d8319e0213491bb52a179e152e_r.jpg)

CSPDarkNet、SSP、PAN这些看之前的，入口还是三张特征图

## 1、损失函数

YOLO V3的损失函数主要分为三部分，分别为：

- （1）bounding box regression损失
- （2）置信度损失
- （3）分类损失

YOLO V4相较于YOLO V3,只在bounding box regression做了创新，用CIOU代替了MSE，其他**两个部分没有做实质改变**。

### 1.1 bounding box regression 损失

```python
def loss_layer(conv, pred, label, bboxes, stride, num_class, iou_loss_thresh):
    conv_shape = tf.shape(conv)
    batch_size = conv_shape[0]
    output_size = conv_shape[1]
    input_size = stride * output_size
    conv = tf.reshape(conv, (batch_size, output_size, output_size,
                             3, 5 + num_class))
    conv_raw_prob = conv[:, :, :, :, 5:]

    pred_xywh = pred[:, :, :, :, 0:4]
    pred_conf = pred[:, :, :, :, 4:5]

    label_xywh = label[:, :, :, :, 0:4]
    respond_bbox = label[:, :, :, :, 4:5]
    label_prob = label[:, :, :, :, 5:]

    ciou = tf.expand_dims(bbox_ciou(pred_xywh, label_xywh), axis=-1)  # (8, 13, 13, 3, 1)
    input_size = tf.cast(input_size, tf.float32)

    # 每个预测框xxxiou_loss的权重 = 2 - (ground truth的面积/图片面积)
    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)
    ciou_loss = respond_bbox * bbox_loss_scale * (1 - ciou)  # 1. respond_bbox作为mask，有物体才计算xxxiou_loss
```

### 1.2 置信度损失

```python
# 2. respond_bbox作为mask，有物体才计算类别loss
prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)
```

### 1.3 分类损失

```python
    # 3. xxxiou_loss和类别loss比较简单。重要的是conf_loss，是一个focal_loss
    # 分两步：第一步是确定 grid_h * grid_w * 3 个预测框 哪些作为反例；第二步是计算focal_loss。
    expand_pred_xywh = pred_xywh[:, :, :, :, np.newaxis, :]  # 扩展为(?, grid_h, grid_w, 3,   1, 4)
    expand_bboxes = bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :]  # 扩展为(?,      1,      1, 1, 150, 4)
    iou = bbox_iou(expand_pred_xywh, expand_bboxes)  # 所有格子的3个预测框 分别 和  150个ground truth  计算iou。   (?, grid_h, grid_w, 3, 150)
    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)  # 与150个ground truth的iou中，保留最大那个iou。  (?, grid_h, grid_w, 3, 1)

    # respond_bgd代表  这个分支输出的 grid_h * grid_w * 3 个预测框是否是 反例（背景）
    # label有物体，respond_bgd是0。 没物体的话：如果和某个gt(共150个)的iou超过iou_loss_thresh，respond_bgd是0；如果和所有gt(最多150个)的iou都小于iou_loss_thresh，respond_bgd是1。
    # respond_bgd是0代表有物体，不是反例；  权重respond_bgd是1代表没有物体，是反例。
    # 有趣的是，模型训练时由于不断更新，对于同一张图片，两次预测的 grid_h * grid_w * 3 个预测框（对于这个分支输出）  是不同的。用的是这些预测框来与gt计算iou来确定哪些预测框是反例。
    # 而不是用固定大小（不固定位置）的先验框。
    respond_bgd = (1.0 - respond_bbox) * tf.cast(max_iou < iou_loss_thresh, tf.float32)

    # 二值交叉熵损失
    pos_loss = respond_bbox * (0 - K.log(pred_conf + K.epsilon()))
    neg_loss = respond_bgd  * (0 - K.log(1 - pred_conf + K.epsilon()))

    conf_loss = pos_loss + neg_loss
    # 回顾respond_bgd，某个预测框和某个gt的iou超过iou_loss_thresh，不被当作是反例。在参与“预测的置信位 和 真实置信位 的 二值交叉熵”时，这个框也可能不是正例(label里没标这个框是1的话)。这个框有可能不参与置信度loss的计算。
    # 这种框一般是gt框附近的框，或者是gt框所在格子的另外两个框。它既不是正例也不是反例不参与置信度loss的计算。（论文里称之为ignore）
```

# 3、Yolov3 边框预测分析

> https://zhuanlan.zhihu.com/p/49995236
>
> https://zhuanlan.zhihu.com/p/345073218
>
> https://www.jianshu.com/p/86b8208f634f

## 3.0 如何在训练中使用 先验框 anchor box

Yolo 的做法是不让 bounding box 直接预测实际 box 的宽和高（w, h），而是**利用乘积缩放**将预测的宽和高分别与anchor box绑定。因为如果使用差值作为衡量，那么尺寸大的anchor box相较之下会对偏移量有更大的容忍，而小尺寸会更敏感。不利于训练。

因此绑定关系为

![\begin{aligned} b_{w} & = a_{w}e^{t_{w}} \\ b_{h} & = a_{h}e^{t_{h}} \end{aligned}](https://math.jianshu.com/math?formula=%5Cbegin%7Baligned%7D%20b_%7Bw%7D%20%26%20%3D%20a_%7Bw%7De%5E%7Bt_%7Bw%7D%7D%20%5C%5C%20b_%7Bh%7D%20%26%20%3D%20a_%7Bh%7De%5E%7Bt_%7Bh%7D%7D%20%5Cend%7Baligned%7D)

其中，![a_{w}](https://math.jianshu.com/math?formula=a_%7Bw%7D)和![a_{h}](https://math.jianshu.com/math?formula=a_%7Bh%7D)为anchor box的宽和高，![t_{w}](https://math.jianshu.com/math?formula=t_%7Bw%7D)和![t_{h}](https://math.jianshu.com/math?formula=t_%7Bh%7D)为bounding box直接预测出的宽和高（其实算是比例系数），![b_{w}](https://math.jianshu.com/math?formula=b_%7Bw%7D)和![b_{h}](https://math.jianshu.com/math?formula=b_%7Bh%7D)为转换后预测的实际宽和高，这也就是最终预测中输出的宽和高。

> 所以为什么不直接相乘，用![\begin{aligned} b_{w}=a_{w}*t_{w}, b_{h}=a_{h}*t_{h}\end{aligned}](https://math.jianshu.com/math?formula=%5Cbegin%7Baligned%7D%20b_%7Bw%7D%3Da_%7Bw%7D*t_%7Bw%7D%2C%20b_%7Bh%7D%3Da_%7Bh%7D*t_%7Bh%7D%5Cend%7Baligned%7D) 这样的公式？

相应的，获得最终预测输出的 box 中心坐标：

![\begin{aligned} b_{x} & = \sigma(t_{x}) + c_{x} \\ b_{y} & = \sigma(t_{y}) + c_{y} \end{aligned}](https://math.jianshu.com/math?formula=%5Cbegin%7Baligned%7D%20b_%7Bx%7D%20%26%20%3D%20%5Csigma(t_%7Bx%7D)%20%2B%20c_%7Bx%7D%20%5C%5C%20b_%7By%7D%20%26%20%3D%20%5Csigma(t_%7By%7D)%20%2B%20c_%7By%7D%20%5Cend%7Baligned%7D)

其中，![\sigma(t_{x})](https://math.jianshu.com/math?formula=%5Csigma(t_%7Bx%7D))为sigmoid函数，![c_{x}](https://math.jianshu.com/math?formula=c_%7Bx%7D)和![c_{y}](https://math.jianshu.com/math?formula=c_%7By%7D)分别为grid cell方格左上角点相对整张图片的坐标。作者使用这样的转换公式主要是因为在训练时如果没有将![t_{x}](https://math.jianshu.com/math?formula=t_%7Bx%7D)和![t_{y}](https://math.jianshu.com/math?formula=t_%7By%7D)压缩到(0,1)区间内的话，模型在训练前期很难收敛。

最终可以得出实际输出的box参数公式如下，这个也是在推理时将输出转换为最终推理结果的公式：

![\begin{aligned} b_{x} & = \sigma(t_{x}) + c_{x} \\ b_{y} & = \sigma(t_{y}) + c_{y} \\ b_{w} & = a_{w}e^{t_{w}} \\ b_{h} & = a_{h}e^{t_{h}} \\ \end{aligned}](https://math.jianshu.com/math?formula=%5Cbegin%7Baligned%7D%20b_%7Bx%7D%20%26%20%3D%20%5Csigma(t_%7Bx%7D)%20%2B%20c_%7Bx%7D%20%5C%5C%20b_%7By%7D%20%26%20%3D%20%5Csigma(t_%7By%7D)%20%2B%20c_%7By%7D%20%5C%5C%20b_%7Bw%7D%20%26%20%3D%20a_%7Bw%7De%5E%7Bt_%7Bw%7D%7D%20%5C%5C%20b_%7Bh%7D%20%26%20%3D%20a_%7Bh%7De%5E%7Bt_%7Bh%7D%7D%20%5C%5C%20%5Cend%7Baligned%7D)

> 关于box参数的转换还有一点值得一提，作者在训练中并不是将![t_{x}](https://math.jianshu.com/math?formula=t_%7Bx%7D)、![t_{y}](https://math.jianshu.com/math?formula=t_%7By%7D)、![t_{w}](https://math.jianshu.com/math?formula=t_%7Bw%7D)和![t_{h}](https://math.jianshu.com/math?formula=t_%7Bh%7D)转换为![b_{x}](https://math.jianshu.com/math?formula=b_%7Bx%7D)、![b_{y}](https://math.jianshu.com/math?formula=b_%7By%7D)、![b_{w}](https://math.jianshu.com/math?formula=b_%7Bw%7D)和![b_{h}](https://math.jianshu.com/math?formula=b_%7Bh%7D)后与ground truth box的对应参数求误差，而是使用上述公式的逆运算将ground truth box的参数转换为与![t_{x}](https://math.jianshu.com/math?formula=t_%7Bx%7D)、![t_{y}](https://math.jianshu.com/math?formula=t_%7By%7D)、![t_{w}](https://math.jianshu.com/math?formula=t_%7Bw%7D)和![t_{h}](https://math.jianshu.com/math?formula=t_%7Bh%7D)对应的![g_{x}](https://math.jianshu.com/math?formula=g_%7Bx%7D)、![g_{y}](https://math.jianshu.com/math?formula=g_%7By%7D)、![g_{w}](https://math.jianshu.com/math?formula=g_%7Bw%7D)和![g_{h}](https://math.jianshu.com/math?formula=g_%7Bh%7D)，然后再计算误差，计算中由于sigmoid函数的反函数难计算，所以并没有计算sigmoid的反函数，而是计算输出对应的sigmoid函数值。
>
> ![\begin{aligned} \sigma(\hat t_{x}) =g_x - c_{x} \\ \sigma(\hat t_{y}) =g_y - c_{y} \\ \hat t_{w} = \log(g_{w} / a_{w}) \\ \hat t_{h} = \log(g_{h} / a_{h}) \\ \end{aligned}](https://math.jianshu.com/math?formula=%5Cbegin%7Baligned%7D%20%5Csigma(%5Chat%20t_%7Bx%7D)%20%3Dg_x%20-%20c_%7Bx%7D%20%5C%5C%20%5Csigma(%5Chat%20t_%7By%7D)%20%3Dg_y%20-%20c_%7By%7D%20%5C%5C%20%5Chat%20t_%7Bw%7D%20%3D%20%5Clog(g_%7Bw%7D%20%2F%20a_%7Bw%7D)%20%5C%5C%20%5Chat%20t_%7Bh%7D%20%3D%20%5Clog(g_%7Bh%7D%20%2F%20a_%7Bh%7D)%20%5C%5C%20%5Cend%7Baligned%7D)
>
> 

## 3.1 公式的解读

论文中边框预测公式如下：

![[公式]](https://www.zhihu.com/equation?tex=b_x%3D%5Csigma+%28t_x%29+%2B+c_x+%5C%5C+b_y%3D%5Csigma+%28t_y%29+%2B+c_y+%5C%5C+b_w%3Dp_we%5E%7Bt_w%7D++%5C%5C+b_h%3Dp_he%5E%7Bt_h%7D+%5C%5C)

其中，Cx, Cy 是 feature map 中 grid cell 的左上角坐标， 在 yolov3 中每个 grid cell 在 Feature map 中的宽和高均为1。

公式中的 Pw, Ph 是通过聚类获得的先验框 anchor box 映射到 feature map 中的宽和高。在coco数据集中，图片尺寸为416\*416，对于13\*13的特征图，其Stride = 416/13=32，因此该特征图对应的先验框  (116 × 90)， (156 × 198)，(373 × 326) ，需要都除以32映射到 feature map 坐标系中。

<img src="https://pic2.zhimg.com/80/v2-758b1df9132a9f4b4e0c7def735e9a11_1440w.jpg" alt="img" style="zoom:60%;" />

最终得到的边框坐标值是bx,by,bw,bh即边界框bbox相对于feature map的位置和大小，是我们需要的预测输出坐标。但**我们网络实际上的学习目标是tx,ty,tw,th这４个offsets**，其中**tx,ty**是预测的坐标偏移值，**tw,th**是尺度缩放，有了这４个offsets，自然可以根据之前的公式去求得真正需要的bx,by,bw,bh４个坐标。

## 3.2 偏移量offset的作用

↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓

**通过学习偏移量，就可以通过网络原始给定的先验框anchor box坐标经过线性回归微调（平移加尺度缩放）去逐渐靠近groundtruth。**

<img src="https://pic4.zhimg.com/80/v2-1229d5cfa3a57f06246eef447d5d32cf_720w.jpg" alt="img" style="zoom:50%;" />

## 3.3 图片尺寸适应输入尺寸

输入尺寸 input_size 为 416\*416，图片尺寸 image_size 为 需要按照纵横比例缩放到 416\*416， **取 min(w/img_w, h/img_h)这个比例来缩放，保证长的边缩放为需要的输入尺寸416，而短边按比例缩放不会扭曲**。

例如：mg_w,img_h是原图尺寸768,576, 缩放后的尺寸为new_w, new_h=416,312，需要的输入尺寸是w,h=416*416.如图2所示：

<img src="https://pic4.zhimg.com/80/v2-62f64986f2a5bf499045e274bcbc782b_720w.jpg" alt="img" style="zoom:47%;" />

## 3.4 推理过程中选择哪个 bounding box

存在一个问题：在训练过程中，我们挑选 bounding box 的准则是选择预测的 box 与 ground truth box 的 IOU最大的 bounding box 作为最优的 box，但是在预测中没有 grounding truth box，怎么才能挑选最优的 bounding box？

这里需要置信度 confidence

置信度是每个bounding box输出的其中一个重要参数，作者对他的作用定义有两重：一重是代表当前box是否有对象的概率![P_{r}(Object)](https://math.jianshu.com/math?formula=P_%7Br%7D(Object))，注意，是对象，不是某个类别的对象，也就是说它用来说明当前box内只是个背景（backgroud）还是有某个物体（对象）；另一重表示当前的box有对象时，它自己预测的box与物体真实的box可能的![IOU_{pred}^{truth}](https://math.jianshu.com/math?formula=IOU_%7Bpred%7D%5E%7Btruth%7D)的值，注意，这里所说的物体真实的box实际是不存在的，这只是模型表达自己框出了物体的自信程度。以上所述，也就不难理解作者为什么将其称之为置信度了，因为不管哪重含义，都表示一种自信程度：框出的box内确实有物体的自信程度和框出的box将整个物体的所有特征都包括进来的自信程度。经过以上的解释，其实我们也就可以用数学形式表示置信度的定义了：

![\begin{aligned} C_{i}^{j} & = P_{r}(Object) * IOU_{pred}^{truth} \end{aligned}](https://math.jianshu.com/math?formula=%5Cbegin%7Baligned%7D%20C_%7Bi%7D%5E%7Bj%7D%20%26%20%3D%20P_%7Br%7D(Object)%20*%20IOU_%7Bpred%7D%5E%7Btruth%7D%20%5Cend%7Baligned%7D)

其中，![C_{i}^{j}](https://math.jianshu.com/math?formula=C_%7Bi%7D%5E%7Bj%7D)表示第i个grid cell的第j个bounding box的置信度。对于如何训练![C_{i}^{j}](https://math.jianshu.com/math?formula=C_%7Bi%7D%5E%7Bj%7D)的方法，在损失函数小节中说明。

## 3.5 条件类别概率（conditional class probalilities）

条件类别概率是一组概率的数组，数组的长度为当前模型检测的类别种类数量，它的意义是当bounding box认为当前box中有对象时，要检测的所有类别中每种类别的概率，其实这个和分类模型最后使用softmax函数输出的一组类别概率是类似的，只是二者存在两点不同：

- YOLO的对象类别概率中没有background一项，也不需要，因为对background的预测已经交给置信度了，所以它的输出是有条件的，那就是在置信度表示当前box有对象的前提下，所以条件概率的数学形式为![P_{r}(class_{i}|Object)](https://math.jianshu.com/math?formula=P_%7Br%7D(class_%7Bi%7D%7CObject))
- 分类模型中最后输出之前使用softmax求出每个类别的概率，也就是说各个类别之间是互斥的，而YOLOv3算法的每个类别概率是单独用逻辑回归函数(sigmoid函数)计算得出了，所以每个类别不必是互斥的，也就是说一个对象可以被预测出多个类别。

https://zhuanlan.zhihu.com/p/345073218

https://zhuanlan.zhihu.com/p/50595699