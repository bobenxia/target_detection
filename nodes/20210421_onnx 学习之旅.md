# 20210421_onnx 学习之旅

## 1、ONNX opset_version

ONNX 原来有版本啊

正常情况下，支持 opset-9

## 2、tensorflow-onnx 仓库

> https://github.com/onnx/tensorflow-onnx

## 3、ONNX 相关资料

> https://zhuanlan.zhihu.com/p/346511883

### 3.1 什么是 ONNX？

开放神经网络交换 （Open Neural Network Exchange）简称 ONNX 是微软和 Facebook 提出用来表示深度学习模型的开放合适。

所谓开放就是 ONNX 定义了一组和环境平台无关的标准格式，来增强各种 AI 模型的可交互性。

换句话说，无论你使用何种训练框架训练模型（Tensoflow/Pytorch...），在训练完毕后你可以将这些框架的模型统一转换成 ONNX 这种统一的格式进程存储。注意，ONNX 文件中不仅仅存储了**神经网络模型的权重**，同时也存储了**模型的结构信息**以及**网络中每一层的输入输出**和一些其他的辅助信息。

> 可以使用 netron 查看网络结构

在获取到 ONNX 模型之后，模型部署人员可以将这个模型部署到兼容 ONNX 的运行环境中。这里一般还会设计到额外的模型转换工作。典型的比如在Android端利用NCNN部署ONNX格式模型，那么就需要将ONNX利用NCNN的转换工具转换到NCNN所支持的`bin`和`param`格式。

### 3.2 ProtoBuf 简介

在分析 ONNX 组织格式前我们需要了解 Protobuf。

ONNX 使用的是 Protobuf 这个序列化数据结构去存储神经网络的权重信息。Caffe 或者 Caffe2 的模型存储数据结构协议是 Protobuf。

Protobuf 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API（摘自官方介绍）。

Protobuf协议是一个以`*.proto`后缀文件为基础的，这个文件描述了用户自定义的数据结构。如果需要了解更多细节请参考0x7节的资料3，这里只是想表达ONNX是基于Protobuf来做数据存储和传输，那么自然`onnx.proto`就是ONNX格式文件了，接下来我们就分析一下ONNX格式。

### 3.3 ONNX 格式分析

这一节我们来分析一下ONNX的组织格式，上面提到ONNX中最核心的部分就是`onnx.proto`（`https://github.com/onnx/onnx/blob/master/onnx/onnx.proto`）这个文件了，它定义了ONNX这个数据协议的规则和一些其它信息。现在是2021年1月，这个文件有700多行，我们没有必要把这个文件里面的每一行都贴出来，我们只要搞清楚里面的核心部分即可。在这个文件里面以`message`关键字开头的对象是我们需要关心的。我们列一下最核心的几个对象并解释一下它们之间的关系。

- `ModelProto`
- `GraphProto`
- `NodeProto`
- `ValueInfProto`
- `TensorProto`
- `AttributeProto`

当我们加载一个 ONNX 之后，我们获得是一个 `ModelProto`，，它包含了一些版本信息，生产者信息和一个`GraphProto`。

在`GraphProto`里面又包含了四个`repeated`数组，它们分别是`node`(`NodeProto`类型)，`input`(`ValueInfoProto`类型)，`output`(`ValueInfoProto`类型)和**`initializer`(`TensorProto`类型)**。

其中，`node`中存放了模型中所有的计算节点，`input`存放了模型的输入节点，`output`存放了模型中所有的输出节点，**`initializer`存放了模型的所有权重参数**。

我们想完整的表达一个神经网络，不仅仅要知道网络的各个节点信息，还要知道它们的拓扑关系。这个拓扑关系在ONNX中是如何表示的呢？ONNX的每个计算节点都会有`input`和`output`两个数组，这两个数组是string类型，通过`input`和`output`的指向关系，我们就可以利用上述信息快速构建出一个深度学习模型的拓扑图。

> 这里要注意一下，`GraphProto`中的`input`数组不仅包含我们一般理解中的图片输入的那个节点，还包含了模型中所有的权重。例如，`Conv`层里面的`W`权重实体是保存在`initializer`中的，那么相应的会有一个同名的输入在`input`中，其背后的逻辑应该是把权重也看成模型的输入，并通过`initializer`中的权重实体来对这个输入做初始化，即一个赋值的过程。

最后，**每个计算节点中还包含了一个`AttributeProto`数组**，用来描述该节点的属性，比如`Conv`节点或者说卷积层的属性包含`group`，`pad`，`strides`等等，每一个计算节点的属性，输入输出信息都详细记录在`https://github.com/onnx/onnx/blob/master/docs/Operators.md`。

### 3.4 onnx.helper

现在我们知道 ONNX 是把一个网络的每一层或者说一个算子当作节点 `node`，使用这些 `node`去构建一个 `Graph`，即一个网络。最后将`Graph`和其它的生产者信息，版本信息等合并在一起生成一个`Model`，也即是最终的ONNX模型文件。

在构建ONNX模型的时候，`https://github.com/onnx/onnx/blob/master/onnx/helper.py`这个文件非常重要，我们可以利用它提供的`make_node`，`make_graph`，`make_tensor`等等接口完成一个ONNX模型的构建，一个示例如下：

```python
import onnx
from onnx import helper
from onnx import AttributeProto, TensorProto, GraphProto

# The protobuf definition can be found here:
# https://github.com/onnx/onnx/blob/master/onnx/onnx.proto

# Create one input (ValueInfoProto)
X = helper.make_tensor_value('X', TensorProto.FLOAT, [3,2])
pads = helper.make_tensor_value_info('pads', TensorProto.FLOAT, [1, 4])
value = helper.make_tensor_value_info('value', AttributeProto.FLOAT, [1])

# Create one output (ValueInfoProto)
Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [3, 4])

# Create a node (NodeProto) - This is based on Pad-11
node_def = helper.make_node(
	'Pad', # node name
    ['X', 'pads', 'value'],  # inputs
    ['Y'],  # outputs
    mode='constant',  # attributes
)

# Create the graph (GraphProto)
graph_def = helper.make_graph(
    [node_def],
    'test-model',
    [X, pads, value],
    [Y],
)

# Create the model (ModelProto)
model_def = helper.make_model(graph_def, producer_name='onnx-example')

print('The model is:\n{}'.format(model_def))
onnx.checker.check_model(model_def)
print('The model is checked!')
```

这个官方示例为我们演示了如何使用`onnx.helper`的`make_tensor`，`make_tensor_value_info`，`make_attribute`，`make_node`，`make_graph`，`make_node`等方法来完整构建了一个ONNX模型。需要注意的是在上面的例子中，输入数据是一个一维Tensor，初始维度为`[2]`，这也是为什么经过维度为`[1,4]`的Pad操作之后获得的输出Tensor维度为`[3,4]`。另外由于Pad操作是没有带任何权重信息的，所以当你打印ONNX模型时，`ModelProto`的`GraphProto`是没有`initializer`这个属性的。

### 3.5 onnx-simplifier

使用ONNX进行模型部署经常碰到一些因为版本兼容性，或者各种框架OP没有对齐等原因导致的各种BUG。这里以一个经典的Pytorch转ONNX的reshape问题为例子，来尝试讲解一下大老师的onnx-simplifier是怎么处理的.

当我们想把下面这段代码导出ONNX模型时：

```python
import torch 

class JustReshape(torch.nn.Module):
    def __init__(self):
        super(JustReshape, self).__init__()
        
    def forward(self, x):
        return x.view((x.shape[0], x.shape[1], x.shape[3], x.shape[2]))
    
net = JustReshape()
model_name = 'just_reshape.onnx'
dummy_input = torch.randn(2,3,4,5)
torch.onnx.export(net, dummy_input, model_name, input_names=['input'], output_names=['output'])
```

由于这个模型输入维度是固定的，所以我们期望模型是这样的：

![img](https://pic4.zhimg.com/80/v2-59b6de77babbecfaca6012c65cd69b93_720w.png)

我们期待的ONNX模型

但是，即使使用了ONNX的`polished`工具也只能获得下面的模型：

<img src="https://pic2.zhimg.com/80/v2-fc50aae30dd42d3400afc8abaaf2d53d_720w.jpg" alt="img" style="zoom:90%;" />

要解决这个问题，有两种方法

- 第一种是做一个强制类型转换，将 x.shape[0] 类似的变量强制转换成常量 int(x.shape[0])
- 第二种使用 onnx-simplifier 来解决这个问题

onnx-simplifier 的核心思想是：利用 onnxruntime 推理一遍 ONNX 的计算图，然后使用常量输出替代冗余的运算 OP。主题代码为：

```python
def simplify(model:Union[str, onnx.ModelProto], check_n:int=0, perform_optimization:bool=True, skip_fuse_bn:bool=False, input_shapes:Optional[TensorShapes]=None,skipped_optimizers: Optional[Sequence[str]] = None, skip_shape_inference=False) -> Tuple[onnx.ModelProto, bool]:
    if input_shapes is None:
        input_shapes = {}
    if type(model) == str:
        # 加载 ONNX 模型
        model = onnx.load(model)
    # 检查 ONNX 模型格式是否正确，图结构是否完整，节点是否正确
    onnx.checker.check_model(model)
    # 深拷贝一份原始 ONNX 模型
   	model_ori = copy.deepcopy(model)
    if not skip_shape_inference:
        # 获取 ONNX 模型中特征图的尺寸
        model = infer_shapes(model)
     
    # 检查输入是否有问题
    input_shapes = check_and_update_shapes(model, input_shapes)
    
    # 对原始的 ONNX 模型做一些图优化工作
    if perform_optimization:
        model = optimize(model, skip_fuse_bn, skipped_optimizers)
        
    const_nodes = get_constant_nodes(model)
    res = forward_for_node_outputs(model, const_nodes, input_shapes=input_shapes)
    const_nodes = clean_constant_nodes(const_nodes, res)
    model = eliminate_const_nodes(model, const_nodes, res)
    onnx.checker.check_model(model)
    
    if not skip_shape_inference:
        model = infer_shapes(model)
    if perform_optimization:
        model = optimize(model, skip_fuse_bn, skipped_optimizers)

    check_ok = check(model_ori, model, check_n, input_shapes=input_shapes)

    return model, check_ok
```

程序使用到了 `check_and_update_input_shapes` 接口，这个接口的代码如下，它可以用来判断输入的格式是否正确以及输入模型是否存在所有的指定输入节点

```python
def check_and_update_input_shapes(model: onnx.ModelProto, input_shapes: TensorShapes) -> TensorShapes:
    input_names = get_input_names(model)
    if None in input_shapes:
        if len(input_names) == 1:
            input_shapes[input_names[0]] = input_shapes[None]
            del input_shapes[None]
        else:
            raise RuntimeError(
                'The model has more than 1 inputs, please use the format "input_name:dim0,dim1,...,dimN" in --input-shape')
    for x in input_shapes:
        if x not in input_names:
            raise RuntimeError(
                'The model doesn\'t have input named "{}"'.format(x))
    return input_shapes
```

在确定了输入没有问题之后，程序会根据用户指定是否优化 ONNX 模型进入优化函数，函数定义如下：

```python
def optimize(model: onnx.ModelProto, skip_fuse_bn:bool, skipped_optimizers:Optional[Sequence[str]]) -> onnx.ModelProto:
   """
    :model参数: 待优化的ONXX模型.
    :return: 优化之后的ONNX模型.
    简化之前, 使用这个方法产生会在'forward_all'用到的ValueInfo
    简化之后，使用这个方法去折叠前一步产生的常量到initializer中并且消除没被使用的常量
    """
	onnx.checker.check_model(model)
    onnx.helper.strip_doc_string(model)
    optimizers_list = [
        'eliminate_deadend',
        'eliminate_nop_dropout',
        'eliminate_nop_cast',
        'eliminate_nop_monotone_argmax', 'eliminate_nop_pad',
        'extract_constant_to_initializer', 'eliminate_unused_initializer',
        'eliminate_nop_transpose',
        'eliminate_nop_flatten', 'eliminate_identity',
        'fuse_add_bias_into_conv',
        'fuse_consecutive_concats',
        'fuse_consecutive_log_softmax',
        'fuse_consecutive_reduce_unsqueeze', 'fuse_consecutive_squeezes',
        'fuse_consecutive_transposes', 'fuse_matmul_add_bias_into_gemm',
        'fuse_pad_into_conv', 'fuse_transpose_into_gemm', 'eliminate_duplicate_initializer'
    ]
    if not skip_fuse_bn:
        optimizers_list.append('fuse_bn_into_conv')
    if skippend_optimizers is not None:
        for opt in skipped_optimizers:
            try:
                optimizers_list.remove(opt)
            except ValurErrot:
                pass
    
    model = onnxoptimizer.optimize(model, optimizers_list, fixed_point=True)
    onnx.checker.check_model(model)
    return model
```

这个函数的功能是对原始的ONNX模型做一些图优化工作，比如merge_bn，use_add_bias_into_conv等等。我们使用`onnx.save`保存一下这个例子中图优化后的模型，可以发现它和优化前的可视化效果是一样的。

这是因为在这个模型中没有上面列举到那些可以做图优化的情况，但是当我们打印一下 ONNX 模型我们会发现 optimize 过后的 ONNX 模型多出一些 initializer 数组

![preview](https://pic1.zhimg.com/v2-4b9f5715724fc4e71ab6258141a97280_r.jpg)

这些数组存储的就是这个图中那些常量 OP 的具体值，通过这个处理我们就可以调用 `get_constant_nodes` 函数来获取 ONNX 模型的常量 OP

这个函数的详细解释如下：

```python
def get_constant_nodes(m: onnx.ModelProto) -> List[onnx.NodeProto]:
    const_nodes = []
```

