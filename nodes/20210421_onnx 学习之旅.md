# 20210421_onnx 学习之旅

## 1、ONNX opset_version

ONNX 原来有版本啊

正常情况下，支持 opset-9

## 2、tensorflow-onnx 仓库

> https://github.com/onnx/tensorflow-onnx

## 3、ONNX 相关资料

> https://zhuanlan.zhihu.com/p/346511883

### 3.1 什么是 ONNX？

开放神经网络交换 （Open Neural Network Exchange）简称 ONNX 是微软和 Facebook 提出用来表示深度学习模型的开放合适。

> IR 是intermediate representation （中间表示）

所谓开放就是 ONNX 定义了一组和环境平台无关的标准格式，来增强各种 AI 模型的可交互性。

换句话说，无论你使用何种训练框架训练模型（Tensoflow/Pytorch...），在训练完毕后你可以将这些框架的模型统一转换成 ONNX 这种统一的格式进程存储。注意，ONNX 文件中不仅仅存储了**神经网络模型的权重**，同时也存储了**模型的结构信息**以及**网络中每一层的输入输出**和一些其他的辅助信息。

> 可以使用 netron 查看网络结构

在获取到 ONNX 模型之后，模型部署人员可以将这个模型部署到兼容 ONNX 的运行环境中。这里一般还会设计到额外的模型转换工作。典型的比如在Android端利用NCNN部署ONNX格式模型，那么就需要将ONNX利用NCNN的转换工具转换到NCNN所支持的`bin`和`param`格式。

### 3.2 ProtoBuf 简介

在分析 ONNX 组织格式前我们需要了解 Protobuf。

ONNX 使用的是 Protobuf 这个序列化数据结构去存储神经网络的权重信息。Caffe 或者 Caffe2 的模型存储数据结构协议是 Protobuf。

Protobuf 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API（摘自官方介绍）。

Protobuf协议是一个以`*.proto`后缀文件为基础的，这个文件描述了用户自定义的数据结构。如果需要了解更多细节请参考0x7节的资料3，这里只是想表达ONNX是基于Protobuf来做数据存储和传输，那么自然`onnx.proto`就是ONNX格式文件了，接下来我们就分析一下ONNX格式。

### 3.3 ONNX 格式分析

这一节我们来分析一下ONNX的组织格式，上面提到ONNX中最核心的部分就是`onnx.proto`（`https://github.com/onnx/onnx/blob/master/onnx/onnx.proto`）这个文件了，它定义了ONNX这个数据协议的规则和一些其它信息。现在是2021年1月，这个文件有700多行，我们没有必要把这个文件里面的每一行都贴出来，我们只要搞清楚里面的核心部分即可。在这个文件里面以`message`关键字开头的对象是我们需要关心的。我们列一下最核心的几个对象并解释一下它们之间的关系。

- `ModelProto`
- `GraphProto`
- `NodeProto`
- `ValueInfProto`
- `TensorProto`
- `AttributeProto`

当我们加载一个 ONNX 之后，我们获得是一个 `ModelProto`，，它包含了一些版本信息，生产者信息和一个`GraphProto`。

在`GraphProto`里面又包含了四个`repeated`数组，它们分别是`node`(`NodeProto`类型)，`input`(`ValueInfoProto`类型)，`output`(`ValueInfoProto`类型)和**`initializer`(`TensorProto`类型)**。

其中，`node`中存放了模型中所有的计算节点，`input`存放了模型的输入节点，`output`存放了模型中所有的输出节点，**`initializer`存放了模型的所有权重参数**。

我们想完整的表达一个神经网络，不仅仅要知道网络的各个节点信息，还要知道它们的拓扑关系。这个拓扑关系在ONNX中是如何表示的呢？ONNX的每个计算节点都会有`input`和`output`两个数组，这两个数组是string类型，通过`input`和`output`的指向关系，我们就可以利用上述信息快速构建出一个深度学习模型的拓扑图。

> 这里要注意一下，`GraphProto`中的`input`数组不仅包含我们一般理解中的图片输入的那个节点，还包含了模型中所有的权重。例如，`Conv`层里面的`W`权重实体是保存在`initializer`中的，那么相应的会有一个同名的输入在`input`中，其背后的逻辑应该是把权重也看成模型的输入，并通过`initializer`中的权重实体来对这个输入做初始化，即一个赋值的过程。

最后，**每个计算节点中还包含了一个`AttributeProto`数组**，用来描述该节点的属性，比如`Conv`节点或者说卷积层的属性包含`group`，`pad`，`strides`等等，每一个计算节点的属性，输入输出信息都详细记录在`https://github.com/onnx/onnx/blob/master/docs/Operators.md`。

### 3.4 onnx.helper

现在我们知道 ONNX 是把一个网络的每一层或者说一个算子当作节点 `node`，使用这些 `node`去构建一个 `Graph`，即一个网络。最后将`Graph`和其它的生产者信息，版本信息等合并在一起生成一个`Model`，也即是最终的ONNX模型文件。

在构建ONNX模型的时候，`https://github.com/onnx/onnx/blob/master/onnx/helper.py`这个文件非常重要，我们可以利用它提供的`make_node`，`make_graph`，`make_tensor`等等接口完成一个ONNX模型的构建，一个示例如下：

```python
import onnx
from onnx import helper
from onnx import AttributeProto, TensorProto, GraphProto

# The protobuf definition can be found here:
# https://github.com/onnx/onnx/blob/master/onnx/onnx.proto

# Create one input (ValueInfoProto)
X = helper.make_tensor_value('X', TensorProto.FLOAT, [3,2])
pads = helper.make_tensor_value_info('pads', TensorProto.FLOAT, [1, 4])
value = helper.make_tensor_value_info('value', AttributeProto.FLOAT, [1])

# Create one output (ValueInfoProto)
Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [3, 4])

# Create a node (NodeProto) - This is based on Pad-11
node_def = helper.make_node(
	'Pad', # node name
    ['X', 'pads', 'value'],  # inputs
    ['Y'],  # outputs
    mode='constant',  # attributes
)

# Create the graph (GraphProto)
graph_def = helper.make_graph(
    [node_def],
    'test-model',
    [X, pads, value],
    [Y],
)

# Create the model (ModelProto)
model_def = helper.make_model(graph_def, producer_name='onnx-example')

print('The model is:\n{}'.format(model_def))
onnx.checker.check_model(model_def)
print('The model is checked!')
```

这个官方示例为我们演示了如何使用`onnx.helper`的`make_tensor`，`make_tensor_value_info`，`make_attribute`，`make_node`，`make_graph`，`make_node`等方法来完整构建了一个ONNX模型。需要注意的是在上面的例子中，输入数据是一个一维Tensor，初始维度为`[2]`，这也是为什么经过维度为`[1,4]`的Pad操作之后获得的输出Tensor维度为`[3,4]`。另外由于Pad操作是没有带任何权重信息的，所以当你打印ONNX模型时，`ModelProto`的`GraphProto`是没有`initializer`这个属性的。

### 3.5 onnx-simplifier

使用ONNX进行模型部署经常碰到一些因为版本兼容性，或者各种框架OP没有对齐等原因导致的各种BUG。这里以一个经典的Pytorch转ONNX的reshape问题为例子，来尝试讲解一下大老师的onnx-simplifier是怎么处理的.

当我们想把下面这段代码导出ONNX模型时：

```python
import torch 

class JustReshape(torch.nn.Module):
    def __init__(self):
        super(JustReshape, self).__init__()
        
    def forward(self, x):
        return x.view((x.shape[0], x.shape[1], x.shape[3], x.shape[2]))
    
net = JustReshape()
model_name = 'just_reshape.onnx'
dummy_input = torch.randn(2,3,4,5)
torch.onnx.export(net, dummy_input, model_name, input_names=['input'], output_names=['output'])
```

由于这个模型输入维度是固定的，所以我们期望模型是这样的：

![img](https://pic4.zhimg.com/80/v2-59b6de77babbecfaca6012c65cd69b93_720w.png)

我们期待的ONNX模型

但是，即使使用了ONNX的`polished`工具也只能获得下面的模型：

<img src="https://pic2.zhimg.com/80/v2-fc50aae30dd42d3400afc8abaaf2d53d_720w.jpg" alt="img" style="zoom:90%;" />

要解决这个问题，有两种方法

- 第一种是做一个强制类型转换，将 x.shape[0] 类似的变量强制转换成常量 int(x.shape[0])
- 第二种使用 onnx-simplifier 来解决这个问题

onnx-simplifier 的核心思想是：利用 onnxruntime 推理一遍 ONNX 的计算图，然后使用常量输出替代冗余的运算 OP。主题代码为：

```python
def simplify(model:Union[str, onnx.ModelProto], check_n:int=0, perform_optimization:bool=True, skip_fuse_bn:bool=False, input_shapes:Optional[TensorShapes]=None,skipped_optimizers: Optional[Sequence[str]] = None, skip_shape_inference=False) -> Tuple[onnx.ModelProto, bool]:
    if input_shapes is None:
        input_shapes = {}
    if type(model) == str:
        # 加载 ONNX 模型
        model = onnx.load(model)
    # 检查 ONNX 模型格式是否正确，图结构是否完整，节点是否正确
    onnx.checker.check_model(model)
    # 深拷贝一份原始 ONNX 模型
   	model_ori = copy.deepcopy(model)
    if not skip_shape_inference:
        # 获取 ONNX 模型中特征图的尺寸
        model = infer_shapes(model)
     
    # 检查输入是否有问题
    input_shapes = check_and_update_shapes(model, input_shapes)
    
    # 对原始的 ONNX 模型做一些图优化工作
    if perform_optimization:
        model = optimize(model, skip_fuse_bn, skipped_optimizers)
        
    const_nodes = get_constant_nodes(model)
    res = forward_for_node_outputs(model, const_nodes, input_shapes=input_shapes)
    const_nodes = clean_constant_nodes(const_nodes, res)
    model = eliminate_const_nodes(model, const_nodes, res)
    onnx.checker.check_model(model)
    
    if not skip_shape_inference:
        model = infer_shapes(model)
    if perform_optimization:
        model = optimize(model, skip_fuse_bn, skipped_optimizers)

    check_ok = check(model_ori, model, check_n, input_shapes=input_shapes)

    return model, check_ok
```

程序使用到了 `check_and_update_input_shapes` 接口，这个接口的代码如下，它可以用来判断输入的格式是否正确以及输入模型是否存在所有的指定输入节点

```python
def check_and_update_input_shapes(model: onnx.ModelProto, input_shapes: TensorShapes) -> TensorShapes:
    input_names = get_input_names(model)
    if None in input_shapes:
        if len(input_names) == 1:
            input_shapes[input_names[0]] = input_shapes[None]
            del input_shapes[None]
        else:
            raise RuntimeError(
                'The model has more than 1 inputs, please use the format "input_name:dim0,dim1,...,dimN" in --input-shape')
    for x in input_shapes:
        if x not in input_names:
            raise RuntimeError(
                'The model doesn\'t have input named "{}"'.format(x))
    return input_shapes
```

在确定了输入没有问题之后，程序会根据用户指定是否优化 ONNX 模型进入优化函数，函数定义如下：

```python
def optimize(model: onnx.ModelProto, skip_fuse_bn:bool, skipped_optimizers:Optional[Sequence[str]]) -> onnx.ModelProto:
   """
    :model参数: 待优化的ONXX模型.
    :return: 优化之后的ONNX模型.
    简化之前, 使用这个方法产生会在'forward_all'用到的ValueInfo
    简化之后，使用这个方法去折叠前一步产生的常量到initializer中并且消除没被使用的常量
    """
	onnx.checker.check_model(model)
    onnx.helper.strip_doc_string(model)
    optimizers_list = [
        'eliminate_deadend',
        'eliminate_nop_dropout',
        'eliminate_nop_cast',
        'eliminate_nop_monotone_argmax', 'eliminate_nop_pad',
        'extract_constant_to_initializer', 'eliminate_unused_initializer',
        'eliminate_nop_transpose',
        'eliminate_nop_flatten', 'eliminate_identity',
        'fuse_add_bias_into_conv',
        'fuse_consecutive_concats',
        'fuse_consecutive_log_softmax',
        'fuse_consecutive_reduce_unsqueeze', 'fuse_consecutive_squeezes',
        'fuse_consecutive_transposes', 'fuse_matmul_add_bias_into_gemm',
        'fuse_pad_into_conv', 'fuse_transpose_into_gemm', 'eliminate_duplicate_initializer'
    ]
    if not skip_fuse_bn:
        optimizers_list.append('fuse_bn_into_conv')
    if skippend_optimizers is not None:
        for opt in skipped_optimizers:
            try:
                optimizers_list.remove(opt)
            except ValurErrot:
                pass
    
    model = onnxoptimizer.optimize(model, optimizers_list, fixed_point=True)
    onnx.checker.check_model(model)
    return model
```

这个函数的功能是对原始的ONNX模型做一些图优化工作，比如merge_bn，use_add_bias_into_conv等等。我们使用`onnx.save`保存一下这个例子中图优化后的模型，可以发现它和优化前的可视化效果是一样的。

这是因为在这个模型中没有上面列举到那些可以做图优化的情况，但是当我们打印一下 ONNX 模型我们会发现 optimize 过后的 ONNX 模型多出一些 initializer 数组

![preview](https://pic1.zhimg.com/v2-4b9f5715724fc4e71ab6258141a97280_r.jpg)

这些数组存储的就是这个图中那些常量 OP 的具体值，通过这个处理我们就可以调用 `get_constant_nodes` 函数来获取 ONNX 模型的常量 OP

这个函数的详细解释如下：

```python
def get_constant_nodes(m: onnx.ModelProto) -> List[onnx.NodeProto]:
    const_nodes = []
    # 如果节点的 name 在 ONNX 的 GraphProto 的 initializer 数组里面，他就是静态的 tensor
    const_tensors = [x.name for x in m.graph.initializer]
    # 显示的常量 OP 也加进来
    const_tensors.extend([node.output[0] for node in m.graph.node if node.op_type=='Constant'])
    # 一些节点的输出 shape 是由输入节点决定的，我们认为这个节点的输出 shape并不是常量
    # 所以我们不需要简化这种节点
    dynamic_tensors = []
    # 判断是否为动态 OP
    def is_dynamic(node):
        if node.op_type in ['NonMaxSuppression', 'NonZero', 'Unique'] and node.input[0] not in const_tensors:
            return True
        if node.op_type in ['Reshape', 'Expand', 'Upsample', 'ConstantOfShape'] and len(node.input) > 1 and node.input[1] not in const_tensors:
            return True
        if node.op_type in ['Resize'] and ((len(node.input) > 2 and node.input[2] not in const_tensors) or (len(node.input) > 3 and node.input[3] not in const_tensors)):
            return True
        return False
    
    for node in m.graph.node:
        if any(x in dynamic_tensors for x in node.input):
            dynamic_tensors.extend(node.output)
        elif node.op_type == 'Shape':
            const_nodes.append(node)
            const_tensors.extend(node.output)
        elif is_dynamic(node):
            dynamic_tensors.extend(node.output)
        elif all([x in const_tensors for x in node.input]):
            const_nodes.append(node)
            const_tensors.extend(node.output)
    # 深拷贝
    return copy.deepcopy(const_nodes)
```

在这个例子中，打印一下执行这个获取常量 OP 函数之后，Graph 中有哪些 OP 被看成了常量 OP。

<img src="https://pic1.zhimg.com/80/v2-3661ffbff61c92adb5cf8c3450d63470_1440w.jpg" alt="img" style="zoom:50%;" />

获取模型中所有的常量 OP 之后，我们**需要把所有的静态节点扩展到 ONNX Graph 的输出节点列表**中，然后利用 onnxruntime 执行一次 forward：

```python
def forward_for_node_outputs(model:onnx.ModelProto, nodes:List[onnx.NodeProto],
                            input_shapes:Optional[TensorShapes]=None) -> Dict[str, np.ndarray]:
    if input_shapes is None:
        input_shapes = {}
    model = copy.deepcopy(model)
    # nodes 是 Graph 中所有静态 OP
    add_features_to_output(model, nodes)
    res = forward(model, input_shapes=input_shapes)
    return res
```

其中 `add_features_to_output`的定义如下：

```python
def add_features_to_output(m:onnx.ModelProto, nodes:List[onnx.NodeProto]) -> Node:
    """
    Add features to output in pb, so that ONNX Runtime will output them.
    :Param m: the model that will be run in ONNX Runtime
    :Param nodes: nodes whose outputs will be added into the Graph outputs
    """
    # ONNX 模型的 graph 扩展输出节点，获取所有静态 OP 的输出和原始输出节点的输出
    for node in nodes:
        for output in node.output:
            m.graph.output.extend([onnx.ValueInfoProto(name=output)])
 
```

最后的 forward 函数就是利用 onnxruntime 推理获得我们指定的输出节点的值。这个函数这里不进行解释。推理完成之后，进入下一个函数 `clean_constant_nodes`，和这个函数定义如下：

```python
def clean_constant_nodes(const_nodes: List[onnx.NodeProto], res: Dict[str, np.ndarray]):
    """
    It seems not needed since commit 6f2a72, but maybe it still prevents some unknown bug
    :param const_nodes: const nodes detected by `get_constant_nodes`
    :param res: The dict containing all tensors, got by `forward_all`
    :return: The constant nodes which have an output in res
    """
    return [node for node in const_nodes if node.output[0] in res]
```

这个函数是用来清洗那些没有被onnxruntime推理的静态节点，但通过上面的optimize逻辑，我们的graph中其实已经不存在这个情况了（没有被onnxruntime推理的静态节点在图优化阶段会被**优化掉**），因此这个函数理论上是可以删除的。这个地方是为了避免删除掉有可能引发其它问题就保留了。

接下来就是这个 `onnx-simplifier` 最核心的步骤了，即将常量节点从原始的 ONNX Graph 中移除，函数接口为 `eliminate_const_nodes`：

```python
def eliminate_const_nodes(model:onnx.ModelProto, const_nods:List[onnx.NodeProto],
                         res:Dict[str, np.ndarray]) -> onnx.ModelProto:
    """
    :model参数：原始ONNX模型
    :const_nodes参数：使用`get_constant_nodes`获取的静态OP
    :res参数：包括所有输出Tensor的字典
    :return：简化后的模型，所有冗余操作都已删除
    """
    for i, node in enumerate(model.graph.node):
        if node in const_nodes:
            for output in node.output:
                new_node = copy.deepcopy(node)
                new_node.name = "node_" + output
                new_node.op_type ='Constant'
                new_attr = onnx.helper.make_attribute(
                	'value',
                    onnx.numpy_helper.from_array(res[output], name=output)
                )
                # 删除
                del new_node.input[:]
                del new_node.attribute[:]
                del new_node.output[:]
                # 换血
                new_node.output.extend([output])
                new_node.attribute.extend([new_attr])
                insert_elem(mode.graph,node, i+1, new_node)
            del mode.graph.node[i]
    return model
```

运行这个函数之后我们获得的ONNX模型可视化结果是这样子的：

![img](https://pic1.zhimg.com/80/v2-3728342cdb46d90a008f5461dae6be28_1440w.jpg)

注意，这里获得的ONNX模型中虽然常量节点已经从Graph中断开了，即相当于这个DAG里面多了一些单独的点，但是这些点还是存在的。因此，我们再执行一次`optimize`就可以获得最终简化后的ONNX模型了。最终简化后的ONNX模型如下图所示：

![img](https://pic1.zhimg.com/80/v2-88f078117c8ff35e463ec0b721afb32c_1440w.jpg)



> 常用我的 onnx simplifier（简称 onnxsim） 的小伙伴可能知道，onnxsim 本身只提供 constant folding/propagation（即消除结果恒为常量的算子）的能力，而图变换（即合并 conv 和 bn 等等）的能力是由 onnxsim 调用 onnx optimizer 的各种 pass 实现的。constant folding 和图变换同时使用时，很多隐藏的优化机会会被挖掘出来，这也是 onnxsim 优化效果出色的原因之一。例如 add(add(x, 1), 2) 在变换为 add(x, add(1, 2)) 之后就可以通过 constant folding 变为 add(x, 3)，而 pad(conv(x, w, padding=0), add(1, 1)) 在经过 constant folding 变为 pad(conv(x, w, padding=0), 2) 后，就可以进一步融合成 conv(x, w, padding=2)。
>
> from -- https://zhuanlan.zhihu.com/p/350702340

## 4、ONNX 模型部署常见问题

> https://zhuanlan.zhihu.com/p/350833729

### 4.1 导出 ONNX

这里以 Pytorch 为例

```python
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = torch.load('test.pth') 
batch_size = 1
input_shape = (3,224, 224)

# set the model to inference mode
model.eval()

x = torch.randn(batch_size, *input_shape)
x = x.to(device)

export_onnx_file = "test.onnx"
torch.onnx.export(mode,
                 x,
                 export_onnx_file,
                 opset_version=10,
                 do_constant_folding=True,  # 是否执行常量折叠优化
                  input_names = ["input"],
                  output_names = ["output"],
                  dynamic_axes={"input":{0:"batch_size"},
                               "output":{0:"batch_size"}}
                 )
```

可以看到Pytorch提供了一个ONNX模型导出的专用接口，只需要配置好相关的模型和参数就可以完成自动导出ONNX模型的操作了。代码相关细节请自行查看，这里来列举几个导出ONNX模型中应该注意的问题。

#### 4.1.1 自定义 OP 问题

以 Yolov5 为例，在模型的 BackBone 部分定义了一个 Focus OP，这个 OP的代码实现为：

```python
class Focus(nn.Module):
    # Focus wh information into c-space
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super(Focus, self).__init__()
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)
        # self.contract = Contract(gain=2)

    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)
        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))
        # return self.conv(self.contract(x))
```

这个操作就是一个stride slice然后再concat的操作，类似于PixelShuffle的逆向过程。下面是把YOLOv5模型导出ONNX模型之后Focus层的可视化结果。

<img src="https://pic3.zhimg.com/80/v2-ef666e2e58a2a0380a78bb9b7ffd531e_1440w.jpg" alt="img" style="zoom:77%;" />

可以看到这个 OP 在使用 Pytorch 导出 ONNX 的过程中被拆成了很多更小的操作，这个时候 Focus OP 的问题就是推理的效率比较低并且拆成的小 OP 各个推理框架的支持程度不一致。

要解决这个问题：

1. 在前向推理框架中实现一个自定以的 Focus Op，例如[ncnn](https://github.com/Tencent/ncnn/blob/master/examples/yolov5.cpp#L24)
2. 要么将这个 OP 使用其他的操作近似代替，比如这里可以使用一个 `stride` 为 2 的卷积OP 来替代 Focus 结构，注意代替之后有可能准确率下降，需要做精度和部署友好性的平衡

综上，自定义的 OP 在导出 ONNX 进行部署是，处理考虑 ONNX 模型的执行效率问题，还要考虑框架是否支持的问题。想要快速迭代产品，建议尽量以一些经典结构为基础，尽量少引入自定义 OP。

#### 4.1.2 后处理问题

如果我们要导出检测网络的 ONNX 模型进行部署，就会碰到这个问题，**后处理部分是否要导入到 ONNX 模型？**

**我们在 Pytorch 导出 ONNX 模型时，所有的 Aten 操作都会被 ONNX 记录下来，称为一个 DAG。然后 ONNX 会根据这个 DAG 的输出节点来反推这个 DAG 有哪些节点有用的，这样获得的就是最终的 ONNX 模型。**而后处理，比如非极大值抑制也是通过 Aten 操作拼起来，所谓 ATEN 操作就是 Pytorch 中的基础算术单元比如 加减乘除，所有的 OP 以及和 Tensor 相关的操作都是基于 Aten 中的操作。

比如检测网络 Yolov3 的后处理就是 NMS，代码示例如 `https://github.com/ultralytics/yolov3/blob/master/utils/general.py#L325`。当我们完成检测网络的训练之后直接导出ONNX模型我们就会发现NMS这个实现也全部被导入了ONNX，如下图所示：

![img](https://pic2.zhimg.com/80/v2-c573db603377a05d26617c1cf2e3f361_1440w.jpg)

这个结构非常复杂，我们要在实际业务中去部署这个模型难度是很大的。另外，刚才我们提到ONNX模型只能记录Pytorch中的Aten操作，对其它的一些逻辑运算符比如`if`是无能为力的（意思是不能记录if的多个子图），而后处理过程中根据置信度阈值来筛选目标框是常规操作。如果我们在导出ONNX模型时是随机输入或者没有指定目标的图片就会导致这个ONNX记录下来的DAG可能有缺失。最后，每个人实现后处理的方式可能都是不一样的，这也增加了ONNX模型部署的难度。**为了部署的友好性和降低转换过程中的风险，后处理过程最好由读者自己完成，我们只需要导出模型的Backbone和Neck部分为ONNX**。

具体来说，我们只需要在Pytorch的代码实现中**屏蔽掉后处理部分然后导出ONNX模型**即可。这也是目前使用ONNX部署检测模型的通用方案。

所以，针对后处理问题，我们的结论就是在使用ONNX进行部署时直接屏蔽后处理，将后处理单独拿出来处理。

#### 4.1.3 胶水 OP 问题

在导出 ONNX 模型的过程中，经常会带来一些胶水 OP，比如 Gather，Shape 等等。例如

```python
import torch

class JustReshape(torch.nn.Module):
    def __init__(self):
        super(JustReshape, self).__init__()

    def forward(self, x):
        return x.view((x.shape[0], x.shape[1], x.shape[3], x.shape[2]))
  
net = JustReshape()
model_name = '../model/just_reshape.onnx'
dummy_input = torch.randn(2, 3, 4, 5)
torch.onnx.export(net, dummy_input, model_name, input_names=['input'], output_names=['output'])
```

这个时候的做法一般就是过一遍onnx-simplifer，可以去除这些胶水OP获得一个简化后的模型。

综上，我们在导出 ONNX 模型的一般流程就是：

- 去掉后处理
- 尽量不引入自定义 OP
- 导出 ONNX 模型，然后过一遍 `onnx-simplifier`

这样就可以获得一个精简的易于部署的 ONNX 模型

### 4.2 ONNX or Caffe?

我们还是以Pytorch为例，要把Pytorch模型通过TensorRT部署到GPU上，一般就是Pytorch->Caffe->TensorRT以及Pytorch->ONNX->TensorRT（当然Pytorch也是支持直接转换到TensorRT，这里不关心）。那么这里就有一个问题，**我们选择哪一条路比较好**？

们要再提一下上面那个`if`的事情了，假设现在有一个新的SOTA模型被提出，这个模型有一个自定义的OP，作者是用Pytorch的Aten操作拼的，逻辑大概是这样：

```python
result = check()
if result == 0:
 result = algorithm1(result)
else:
 result = algorithm2(result)
return result
```

然后考虑将这个模型导出ONNX或者转换为Caffe，如果是Caffe的话我们需要去实现这个自定义的OP，并将其注册到Caffe的OP管理文件中，虽然这里比较繁琐，但是我们可以将`if`操作隐藏在这个大的OP内部，这个`if`操作可以保留下来。而如果我们通过导出ONNX模型的方式`if`子图只能保留一部分，要么保留algorithm1，要么保留algorithm2对应的子图，这种情况ONNX似乎就没办法处理了。这个时候要么保存两个ONNX模型，要么修改算法逻辑绕过这个问题。从这里引申一下，如果我们碰到**有递归关系的网络，基于ONNX应当怎么部署**？ONNX还有一个缺点就是OP的细粒度太细，执行效率低，不过ONNX已经推出了多种化方法可以将OP的细粒度变粗，提高模型执行效率。目前在众多经典算法上，ONNX已经支持得非常好了。

### 4.3 一些典型的坑点及解决方法

第一节已经提到，将我们的ONNX模型过一遍onnx-simplifer之后就可以去掉胶水OP并将一些细粒度的OP进行op fuse成粗粒度的OP，并解决一部分由于Pytorch和ONNX OP实现方式不一致而导致模型变复杂的问题。除了这些问题，本节再列举一些ONNX模型部署中容易碰到的坑点，并尝试给出一些解决办法。

#### 4.3.1 预处理问题

和后处理对应的还有预处理问题，如果在 Pytorch 中使用下面代码导出 ONNX 模型

```python
import torch


class JustReshape(torch.nn.Module):
    def __init__(self):
        super(JustReshape, self).__init__()
        self.mean = torch.randn(2, 3, 4, 5)
        self.std = torch.randn(2, 3, 4, 5)

    def forward(self, x):
        x = (x - self.mean) / self.std
        return x.view((x.shape[0], x.shape[1], x.shape[3], x.shape[2]))


net = JustReshape()
model_name = '../model/just_reshape.onnx'
dummy_input = torch.randn(2, 3, 4, 5)
torch.onnx.export(net, dummy_input, model_name, input_names=['input'], output_names=['output'])
```

我们先给这个ONNX模型过一遍onnx-simplifer，然后使用Netron可视化之后模型大概长这样：

<img src="https://pic3.zhimg.com/80/v2-9192db1464b48b0313c76a1e879ac68a_1440w.jpg" alt="img" style="zoom:80%;" />

如果我们要把这个模型放到NPU上部署，如果NPU芯片不支持Sub和Div的量化计算，那么这两个操作会被回退到NPU上进行计算，这显然是不合理的，因为我们总是想网络在NPU上能一镜到底，中间断开必定会影响模型效率，所以这里的解决办法就是把预处理放在基于`nn.Module`搭建模型的代码之外，然后推理的时候先把预处理做掉即可。

#### 4.3.2 框架 OP 实现不一致问题

当从Mxnet转换模型到ONNX时，如果模型是带有PReLU OP的（在人脸识别网络很常见），就是一个大坑了。主要有两个问题，当从mxnet转为ONNX时，PReLU的slope参数维度可能会导致onnxruntime推理时失败，报错大概长这样：

```python
2）[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running PRelu node. Name:'conv_1_relu'...... Attempting to broadcast an axis by a dimension other than 1. 56 by 64 
```

这个错误产生的原因可能是MxNet的版本问题（`https://github.com/apache/incubator-mxnet/issues/17821`），这个时候的解决办法就是：修改PRelu层的slope参数的shape，不仅包括type参数，对应的slope值也要修改来和shape对应。

核心代码如下：

```python
graph.input.remove(input_map[input_name])
new_nv = helper.make_tensor_value_info(input_name, TensorProto.FLOAT, [input_dim_val,1,1])
graph.input.extend([new_nv])
```

下一个问题就是如果我们将处理好之后的ONNX通过TensorRT进行部署的话，**我们会发现TensorRT不支持PReLU这个OP，这个时候解决办法要么是TensorRT自定义PReLU插件，但是这种方法会打破TensorRT中conv+bn+relu的op fusion，速度会变慢，并且如果要做量化部署似乎是不可行的。所以这个时候一般会采用另外一种解决办法，使用relu和scale op来组合成PReLU**，如下图所示：

![img](https://pic4.zhimg.com/80/v2-fab2b7251aa428c1a62751d368e1e5df_1440w.jpg)

所以，我们在onnx模型中只需要按照这种方法将PReLU节点进行等价替换就可以了。

这个地方以PReLU列举了一个框架OP实现不一致的问题，比如大老师最新文章也介绍的就是squeeze OP在Pytorch和ONNX实现时的不一致导致ONNX模型变得很复杂，这种问题感觉是基于ONNX支持模型部署时的常见问题，虽然onnx-simplifier已经解决了一些问题，但也不能够完全解决。

#### 4.3.2 batch 维度缺失

使用 tf2onnx 工具将 TensorFlow 模型转为 ONNX 模型时，模型的输入 batch 维度没有被设置，我们需要手动添加。

```python
# 为onnx模型增加batch维度
    def set_model_input_batch(self, index=0, name=None, batch_size=4):
        model_input = None
        if name is not None:
            for ipt in self.model.graph.input:
                if ipt.name == name:
                    model_input = ipt
        else:
            model_input = self.model.graph.input[index]
        if model_input:
            tensor_dim = model_input.type.tensor_type.shape.dim
            tensor_dim[0].ClearField("dim_param")
            tensor_dim[0].dim_value = batch_size
        else:
            print('get model input failed, check index or name')
```

## 5、ONNXRuntime 介绍及用法

> https://zhuanlan.zhihu.com/p/346544539

ONNXRuntime 是微软推出的一个推理架构，可以运行ONNX模型。支持多种运行后端包括 CPU，GPU，TensorRT，DML等。

```python
import numpy as np
import onnx
import onnxruntime as ort

image = cv2.imread("image.jpg")
image = np.expand_dims(image, axis=0)

onnx_model = onnx.load_model("resnet18.onnx")
sess = ort.inferenceSession(onnx_model.SerializeToString)

sess.set_providers(['CPUExcutionProvider'])
input_name = sess.get_inputs()[0].name
output_name = sess.get_outputs()[0].name

output = sess.run([output_name], {input_name:image_data})
prob = np.squeeze(output[0])
print("predicting label: ", np.argmax(prb))
```

ONNXRuntime 在推理一个 ONNX 模型时大概分为 ：

- Session 构造
- 模型加载与初始化
- 运行阶段（和静态图架构类似）

ONNXRuntime 框架是使用 C++ 开发的，同时使用 Wrapper 技术封装了 Python 接口

### 5.1 第一阶段 Session 构造

构造阶段即创建一个 InferenceSession 对象。使用 python 前端构建 Session 对象是，python 端会通过 `onnxruntime_pybind_state.c` 调用 C++ 中的 InferenceSession 类构造函数，得到一个 InferenceSession 对象。

InferenceSession 构造阶段会进行各个成员的初始化，成员包括：

- 负责 Opkernel 管理的 KernelRegistryManager 对象
- 持有 Session 配置信息的 SessionOptions 对象
- 负责图分割的 GraphTransformerManager
- 负责log管理的 LoggingManager 
- 等等

当然，这个时候InferenceSession就是一个空壳子，只完成了对成员对象的初始构建。

### 5.2 第二阶段 模型加载与初始化

在完成 InferenceSession 对象的构造后，会将 ONNX 模型加载到 InferenceSession 中并进行进一步的初始化。

**1. 模型加载**

模型加载时，会在 C++ 后端调用对应的 Load() 函数，InferenceSession 一共提供了 8 种 Load 函数。从url，ModelProto，void* model data，model istream等读取ModelProto。InferenceSession 会对 ModelProto 进行解析然后持有其对应的Model成员。

**2. Providers 注册**

在 Load 函数结束后，InferenceSession 会调用两个函数：RegisterExecutionProviders() 和 sess->Initialize()；

RegisterExecutionProviders 函数会完成 ExecutionProvider 的注册工作。**这里解释一下ExecutionProvider，ONNXRuntime 用 Provider 表示不同的运行设备比如 CUDAProvider 等。目前ONNXRuntimev1.0支持了包括CPU，CUDA，TensorRT，MKL等七种Providers**。通过调用sess->RegisterExecutionProvider()函数，InferenceSession通过一个list持有当前运行环境中支持的ExecutionProviders。

**3. InferenceSession 初始化**

即 sess->Initialize()，这是 InferenceSession 会根据自身持有的 model 和 execution providers 进行进一步的初始化（在第一阶段 Session构造时仅仅持有了空壳子成员变量）。该步骤是 InferemceSession 初始化的核心，一系列的核心操作如内存分配，model partition，kernel 注册等都会在这个阶段完成。

1. 首先，session会根据 level 注册 graph optimization transformers，并通过 GraphTransformerManeger 成员进行持有。
2. 接下来，session 会进行 OpKernel 注册，OpKernel  即定义的各个 node 对应在不同运行设备上的计算逻辑。这个过程会将持有的各个 ExecutionProvider 上定义的所有 node 对应的 Kernel 注册到 session 中，session 通过 KernelRegistryManager 成员进行持有和管理。
3. 然后 session 会对 Graph 进行图变换，包括插入 copy 节点，cast 节点等。
4. 接下来是 model partition，也就是根据运行设备对 graph 进行切分，决定每个 node 运行在哪个 provider 上。
5. 最后，为每个 node 创建 ExecutionPlan，运行计划主要包含了各个 op 的执行顺序，内存申请管理，内存复用管理等操作。

### 5.3 第三阶段 模型运行

模型运行即I nferenceSession 每次读入一个batch的数据并进行计算得到模型的最终输出。然而其实绝大多数的工作早已经在 InferenceSession 初始化阶段完成。细看下源码就会发现 run 阶段主要是顺序调用各个 node 的对应 OpKerne l进行计算。

### 5.4 ONNXRuntime源码之OpKernel注册

> https://zhuanlan.zhihu.com/p/348387800

## 6、ONNX 中的 Pass 机制

ONNX是微软开源的一款model ir，也就是模型的中间表示。

通常情况，我们仅仅是将ONNX作为一个中间表示，转换到ONNX后往往也马上转换到另一个框架或者平台。而其实ONNX提供了一套pass机制，可以让我们在转换到ONNX后，对模型进行进一步修剪加工，让接下来的转换或部署更加便捷。

Pass机制功能强大，理论上通过Pass我们可以对模型进行任何想要的修改，给了开发者极大自由度的同时，使用起来并不不麻烦。

### 6.1 Pass 相关类详解

#### 6.1.1 Pass 基类

Pass类是所有pass的基类，主要封装了数个核心的对外接口如runPass(Graph&)函数，各个派生类需要实现该函数，该函数传入一个Graph。不同的派生pass会根据自身功能对Graph进行不同操作。Pass基类同时约定了各个Pass的一些属性如PassOptimizationType，PassEfficiency，PassAnalysisType等，稍后会对各个属性进行详细介绍。

#### 6.1.2 核心类 PredicateBasedPass

PredicateBasedPass继承自Pass类并添加了多个常用函数，是ONNX中最常用的Pass类，一般情况下如果我们需要开发自己的Pass都会继承自该类并进行进一步的功能实现。

如果继承自该类，只需要重写 `patternMatchPredicate` 函数和 `runTransform` 函数两个函数即可.

PredicateBasedPass实现了runPass函数，该函数实现遍历传入Graph中的每个Node并进行判断，如果满足判断要求则进行特定的操作，具体的操作由runTransform函数负责实现。下面是runPass的核心逻辑：

```c++
bool initialized_pass = this->initializePass(graph);
...
for (auto it = graph.begin(); it != graph.end(); ++it) {
    auto* n = *it;
    num_changes += this->DescendOnGraphAttributesAndCount(
        n, [this](Graph& g) { return _runPassInternal(g); });
    if (this->patternMatchPredicate(n)) {
      NodeDestroyType destroy_type = NodeDestroyType::DestroyZero;
      num_changes += this->runTransform(n, graph, destroy_type);

      if (destroy_type == NodeDestroyType::DestroyOne) {
        it.destroyCurrent();
      }
      if (destroy_type == NodeDestroyType::DestroyTwo) {
        it.destroyCurrent();
        it.destroyCurrent();
      }
    }
  }
...
bool finalized_pass = this->finalizePass(graph);
```

上面的代码不难看出，runPass会可以统计在传入的Graph中，发生了多少次修改。并且对于包含子图结构的Graph，也会递归运行在所有子图中的节点上。

而对于initializePass和finalizePass，可以理解为两个hook用以初始pass和进行一些收尾操作，这两个函数都是Pass基类中的虚函数，用户可以自行编写。

**patternMatchPredicate(Node\*)函数**

PredicateBasedPass中定义的接口函数，需要用户重写，用来判断当前Node是否满足运行当前pass的条件。比如说，我想实现一个移除所有dropout功能的pass，判断函数可以这么写：

```cpp
  bool patternMatchPredicate(Node* node) override {
    return (node->kind() == kDropout && node->hasAttribute(kratio));
  }
```

**runTransform((Node\*, Graph&, NodeDestroyType&)函数**

该函数是PredicateBasedPass提供的的接口函数，用户自定义pass时需要实现该函数。该函数的功能主要是定义当patternMatchPredicate函数返回True时，将对将该node进行何种操作。

需要注意的是，runTransform函数会传入一个NodeDestroyType进行判断，当传入Node需要删除时需要将NodeDestroyType置为DestroyOne或DestroyTwo，Pass会自动对Node进行删除。而如果这个pass不会对传入Node进行删除，则将其置为DestroyZero即可。

#### 6.1.3 FullGraphBasedPass类

一个更加简单的pass类，继承自Pass基类，没有实现runPass函数，通常情况下在开发特定pass功能时，我们自定义的pass继承自已经写好runPass的PredicateBasePass类。 只有在需要自由定制runPass函数的功能时我们才继承自该类。

#### 6.1.4 辅助类CountBasedPassAnalysis

该类用来统计一个predicate based pass运行结束后Grpah的状态，也是runPass的返回值。该类可以用来记录pass运行后的一些状态，比说graphChanged()返回Graph是否发生了改变： numSucceededTransforms()返回发生了多少次成功的变换; fixedPointOptimizationNeeded()返回是否需要再次运行该pass等。

#### 6.1.5 Pass种类

ONNX同时声明了多个Pass的枚举成员，用以更加方便用户声明不同类型的Pass，比如说

```cpp
enum PassOptimizationType {
  None = 0,
  Compute = 1,
  Memory = 2,
  ComputeMemory = 3,
  Stability = 4
};
enum PassType {
  Fuse = 0,
  Nop = 1,
  Seperate = 2,
  Immutable = 3,
  Other = 4
};
```

PassOptimizationType用来声明该pass的优化目的，PassType用来声明Pass的种类（干了什么）

### 6.2 Pass管理和注册

通常情况下，在继承自PredicateBasedPass并编写好自己的pass后，我们就可以直接调用pass了，但更科学的方法是用一个**全局pass注册机制**对pass进行持有，并在pass基础上进一步封装管理类提高pass调用的便捷性。

#### 6.2.1 Pass注册类GlobalPassRegistry

GlobalPassRegistry是ONNX中用来注册pass的类，该类的核心是保存一个map<std::string, std::shared_ptr<Pass>> passes用来注册pass。对外提供如find(string pass_name)等函数返回对应的pass；GetAvailablePasses()返回所有可用pass等功能。

#### 6.2.2 Pass管理类PassManager

Pass管理类通过对Pass提供进一步的封装，可以更加便捷的运行pass。

PassManager是最基本的pass管理类，包含了add和run两个接口函数，分别可以向管理类中添加pass和运行所有pass。

而GeneralPassManager是对PassManager功能的最基本实现。GeneralPassManager持有一个vector的pass进而可以做到多个特定组合pass 顺序运行的效果。

FixedPointPassManager类同样继承自PassManager基类并且和GeneralPassManager类有一样的功能，但具备了一个判断能力，即重复运行passes直到graph不在发生任何改变为止。

下面是ONNX提供的一个运行和管理pass的例子：

下面是ONNX提供的一个运行和管理pass的例子：

```cpp
GlobalPassRegistry Optimizer::passes;

Optimizer::Optimizer(
    const std::vector<std::string>& names,
    const bool fixed_point) {
  if (fixed_point) {
    this->pass_manager =
        std::shared_ptr<FixedPointPassManager>(new FixedPointPassManager());
  } else {
    this->pass_manager =
        std::shared_ptr<GeneralPassManager>(new GeneralPassManager());
  }
  for (const auto& name : names) {
    auto pass = passes.find(name);
    this->pass_manager->add(pass);
  }
}
ModelProto Optimizer::optimize(const ModelProto& mp_in) {
  std::shared_ptr<Graph> g(ImportModelProto(mp_in));
  ModelProto mp_out = PrepareOutput(mp_in);
  this->pass_manager->run(*g);
  ExportModelProto(&mp_out, g);
  return mp_out;
}

// 对外调用接口
ModelProto Optimize(
    const ModelProto& mp_in,
    const std::vector<std::string>& names) {
  Optimizer current_opt(names, false);
  return current_opt.optimize(mp_in);
}
const std::vector<std::string> GetAvailablePasses() {
  return Optimizer::passes.GetAvailablePasses();
}
```













