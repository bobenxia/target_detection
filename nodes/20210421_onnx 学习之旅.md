# 20210421_onnx 学习之旅

## 1、ONNX opset_version

ONNX 原来有版本啊

正常情况下，支持 opset-9

## 2、tensorflow-onnx 仓库

> https://github.com/onnx/tensorflow-onnx

## 3、ONNX 相关资料

> https://zhuanlan.zhihu.com/p/346511883

### 3.1 什么是 ONNX？

开放神经网络交换 （Open Neural Network Exchange）简称 ONNX 是微软和 Facebook 提出用来表示深度学习模型的开放合适。

所谓开放就是 ONNX 定义了一组和环境平台无关的标准格式，来增强各种 AI 模型的可交互性。

换句话说，无论你使用何种训练框架训练模型（Tensoflow/Pytorch...），在训练完毕后你可以将这些框架的模型统一转换成 ONNX 这种统一的格式进程存储。注意，ONNX 文件中不仅仅存储了**神经网络模型的权重**，同时也存储了**模型的结构信息**以及**网络中每一层的输入输出**和一些其他的辅助信息。

> 可以使用 netron 查看网络结构

在获取到 ONNX 模型之后，模型部署人员可以将这个模型部署到兼容 ONNX 的运行环境中。这里一般还会设计到额外的模型转换工作。典型的比如在Android端利用NCNN部署ONNX格式模型，那么就需要将ONNX利用NCNN的转换工具转换到NCNN所支持的`bin`和`param`格式。

### 3.2 ProtoBuf 简介

在分析 ONNX 组织格式前我们需要了解 Protobuf。

ONNX 使用的是 Protobuf 这个序列化数据结构去存储神经网络的权重信息。Caffe 或者 Caffe2 的模型存储数据结构协议是 Protobuf。

Protobuf 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API（摘自官方介绍）。

Protobuf协议是一个以`*.proto`后缀文件为基础的，这个文件描述了用户自定义的数据结构。如果需要了解更多细节请参考0x7节的资料3，这里只是想表达ONNX是基于Protobuf来做数据存储和传输，那么自然`onnx.proto`就是ONNX格式文件了，接下来我们就分析一下ONNX格式。

### 3.3 ONNX 格式分析

这一节我们来分析一下ONNX的组织格式，上面提到ONNX中最核心的部分就是`onnx.proto`（`https://github.com/onnx/onnx/blob/master/onnx/onnx.proto`）这个文件了，它定义了ONNX这个数据协议的规则和一些其它信息。现在是2021年1月，这个文件有700多行，我们没有必要把这个文件里面的每一行都贴出来，我们只要搞清楚里面的核心部分即可。在这个文件里面以`message`关键字开头的对象是我们需要关心的。我们列一下最核心的几个对象并解释一下它们之间的关系。

- `ModelProto`
- `GraphProto`
- `NodeProto`
- `ValueInfProto`
- `TensorProto`
- `AttributeProto`

当我们加载一个 ONNX 之后，我们获得是一个 `ModelProto`，，它包含了一些版本信息，生产者信息和一个`GraphProto`。

在`GraphProto`里面又包含了四个`repeated`数组，它们分别是`node`(`NodeProto`类型)，`input`(`ValueInfoProto`类型)，`output`(`ValueInfoProto`类型)和**`initializer`(`TensorProto`类型)**。

其中，`node`中存放了模型中所有的计算节点，`input`存放了模型的输入节点，`output`存放了模型中所有的输出节点，**`initializer`存放了模型的所有权重参数**。

我们想完整的表达一个神经网络，不仅仅要知道网络的各个节点信息，还要知道它们的拓扑关系。这个拓扑关系在ONNX中是如何表示的呢？ONNX的每个计算节点都会有`input`和`output`两个数组，这两个数组是string类型，通过`input`和`output`的指向关系，我们就可以利用上述信息快速构建出一个深度学习模型的拓扑图。

> 这里要注意一下，`GraphProto`中的`input`数组不仅包含我们一般理解中的图片输入的那个节点，还包含了模型中所有的权重。例如，`Conv`层里面的`W`权重实体是保存在`initializer`中的，那么相应的会有一个同名的输入在`input`中，其背后的逻辑应该是把权重也看成模型的输入，并通过`initializer`中的权重实体来对这个输入做初始化，即一个赋值的过程。

最后，**每个计算节点中还包含了一个`AttributeProto`数组**，用来描述该节点的属性，比如`Conv`节点或者说卷积层的属性包含`group`，`pad`，`strides`等等，每一个计算节点的属性，输入输出信息都详细记录在`https://github.com/onnx/onnx/blob/master/docs/Operators.md`。

### 3.4 onnx.helper

现在我们知道 ONNX 是把一个网络的每一层或者说一个算子当作节点 `node`，使用这些 `node`去构建一个 `Graph`，即一个网络。最后将`Graph`和其它的生产者信息，版本信息等合并在一起生成一个`Model`，也即是最终的ONNX模型文件。

在构建ONNX模型的时候，`https://github.com/onnx/onnx/blob/master/onnx/helper.py`这个文件非常重要，我们可以利用它提供的`make_node`，`make_graph`，`make_tensor`等等接口完成一个ONNX模型的构建，一个示例如下：

```python
import onnx
from onnx import helper
from onnx import AttributeProto, TensorProto, GraphProto

# The protobuf definition can be found here:
# https://github.com/onnx/onnx/blob/master/onnx/onnx.proto

# Create one input (ValueInfoProto)
X = helper.make_tensor_value('X', TensorProto.FLOAT, [3,2])
pads = helper.make_tensor_value_info('pads', TensorProto.FLOAT, [1, 4])
value = helper.make_tensor_value_info('value', AttributeProto.FLOAT, [1])

# Create one output (ValueInfoProto)
Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [3, 4])

# Create a node (NodeProto) - This is based on Pad-11
node_def = helper.make_node(
	'Pad', # node name
    ['X', 'pads', 'value'],  # inputs
    ['Y'],  # outputs
    mode='constant',  # attributes
)

# Create the graph (GraphProto)
graph_def = helper.make_graph(
    [node_def],
    'test-model',
    [X, pads, value],
    [Y],
)

# Create the model (ModelProto)
model_def = helper.make_model(graph_def, producer_name='onnx-example')

print('The model is:\n{}'.format(model_def))
onnx.checker.check_model(model_def)
print('The model is checked!')
```

这个官方示例为我们演示了如何使用`onnx.helper`的`make_tensor`，`make_tensor_value_info`，`make_attribute`，`make_node`，`make_graph`，`make_node`等方法来完整构建了一个ONNX模型。需要注意的是在上面的例子中，输入数据是一个一维Tensor，初始维度为`[2]`，这也是为什么经过维度为`[1,4]`的Pad操作之后获得的输出Tensor维度为`[3,4]`。另外由于Pad操作是没有带任何权重信息的，所以当你打印ONNX模型时，`ModelProto`的`GraphProto`是没有`initializer`这个属性的。

### 3.5 onnx-simplifier

使用ONNX进行模型部署经常碰到一些因为版本兼容性，或者各种框架OP没有对齐等原因导致的各种BUG。这里以一个经典的Pytorch转ONNX的reshape问题为例子，来尝试讲解一下大老师的onnx-simplifier是怎么处理的.

当我们想把下面这段代码导出ONNX模型时：

```python
import torch 

class JustReshape(torch.nn.Module):
    def __init__(self):
        super(JustReshape, self).__init__()
        
    def forward(self, x):
        return x.view((x.shape[0], x.shape[1], x.shape[3], x.shape[2]))
    
net = JustReshape()
model_name = 'just_reshape.onnx'
dummy_input = torch.randn(2,3,4,5)
torch.onnx.export(net, dummy_input, model_name, input_names=['input'], output_names=['output'])
```

由于这个模型输入维度是固定的，所以我们期望模型是这样的：

![img](https://pic4.zhimg.com/80/v2-59b6de77babbecfaca6012c65cd69b93_720w.png)

我们期待的ONNX模型

但是，即使使用了ONNX的`polished`工具也只能获得下面的模型：

<img src="https://pic2.zhimg.com/80/v2-fc50aae30dd42d3400afc8abaaf2d53d_720w.jpg" alt="img" style="zoom:90%;" />

要解决这个问题，有两种方法

- 第一种是做一个强制类型转换，将 x.shape[0] 类似的变量强制转换成常量 int(x.shape[0])
- 第二种使用 onnx-simplifier 来解决这个问题

onnx-simplifier 的核心思想是：利用 onnxruntime 推理一遍 ONNX 的计算图，然后使用常量输出替代冗余的运算 OP。主题代码为：

```python
def simplify(model:Union[str, onnx.ModelProto], check_n:int=0, perform_optimization:bool=True, skip_fuse_bn:bool=False, input_shapes:Optional[TensorShapes]=None,skipped_optimizers: Optional[Sequence[str]] = None, skip_shape_inference=False) -> Tuple[onnx.ModelProto, bool]:
    if input_shapes is None:
        input_shapes = {}
    if type(model) == str:
        # 加载 ONNX 模型
        model = onnx.load(model)
    # 检查 ONNX 模型格式是否正确，图结构是否完整，节点是否正确
    onnx.checker.check_model(model)
    # 深拷贝一份原始 ONNX 模型
   	model_ori = copy.deepcopy(model)
    if not skip_shape_inference:
        # 获取 ONNX 模型中特征图的尺寸
        model = infer_shapes(model)
     
    # 检查输入是否有问题
    input_shapes = check_and_update_shapes(model, input_shapes)
    
    # 对原始的 ONNX 模型做一些图优化工作
    if perform_optimization:
        model = optimize(model, skip_fuse_bn, skipped_optimizers)
        
    const_nodes = get_constant_nodes(model)
    res = forward_for_node_outputs(model, const_nodes, input_shapes=input_shapes)
    const_nodes = clean_constant_nodes(const_nodes, res)
    model = eliminate_const_nodes(model, const_nodes, res)
    onnx.checker.check_model(model)
    
    if not skip_shape_inference:
        model = infer_shapes(model)
    if perform_optimization:
        model = optimize(model, skip_fuse_bn, skipped_optimizers)

    check_ok = check(model_ori, model, check_n, input_shapes=input_shapes)

    return model, check_ok
```

程序使用到了 `check_and_update_input_shapes` 接口，这个接口的代码如下，它可以用来判断输入的格式是否正确以及输入模型是否存在所有的指定输入节点

```python
def check_and_update_input_shapes(model: onnx.ModelProto, input_shapes: TensorShapes) -> TensorShapes:
    input_names = get_input_names(model)
    if None in input_shapes:
        if len(input_names) == 1:
            input_shapes[input_names[0]] = input_shapes[None]
            del input_shapes[None]
        else:
            raise RuntimeError(
                'The model has more than 1 inputs, please use the format "input_name:dim0,dim1,...,dimN" in --input-shape')
    for x in input_shapes:
        if x not in input_names:
            raise RuntimeError(
                'The model doesn\'t have input named "{}"'.format(x))
    return input_shapes
```

在确定了输入没有问题之后，程序会根据用户指定是否优化 ONNX 模型进入优化函数，函数定义如下：

```python
def optimize(model: onnx.ModelProto, skip_fuse_bn:bool, skipped_optimizers:Optional[Sequence[str]]) -> onnx.ModelProto:
   """
    :model参数: 待优化的ONXX模型.
    :return: 优化之后的ONNX模型.
    简化之前, 使用这个方法产生会在'forward_all'用到的ValueInfo
    简化之后，使用这个方法去折叠前一步产生的常量到initializer中并且消除没被使用的常量
    """
	onnx.checker.check_model(model)
    onnx.helper.strip_doc_string(model)
    optimizers_list = [
        'eliminate_deadend',
        'eliminate_nop_dropout',
        'eliminate_nop_cast',
        'eliminate_nop_monotone_argmax', 'eliminate_nop_pad',
        'extract_constant_to_initializer', 'eliminate_unused_initializer',
        'eliminate_nop_transpose',
        'eliminate_nop_flatten', 'eliminate_identity',
        'fuse_add_bias_into_conv',
        'fuse_consecutive_concats',
        'fuse_consecutive_log_softmax',
        'fuse_consecutive_reduce_unsqueeze', 'fuse_consecutive_squeezes',
        'fuse_consecutive_transposes', 'fuse_matmul_add_bias_into_gemm',
        'fuse_pad_into_conv', 'fuse_transpose_into_gemm', 'eliminate_duplicate_initializer'
    ]
    if not skip_fuse_bn:
        optimizers_list.append('fuse_bn_into_conv')
    if skippend_optimizers is not None:
        for opt in skipped_optimizers:
            try:
                optimizers_list.remove(opt)
            except ValurErrot:
                pass
    
    model = onnxoptimizer.optimize(model, optimizers_list, fixed_point=True)
    onnx.checker.check_model(model)
    return model
```

这个函数的功能是对原始的ONNX模型做一些图优化工作，比如merge_bn，use_add_bias_into_conv等等。我们使用`onnx.save`保存一下这个例子中图优化后的模型，可以发现它和优化前的可视化效果是一样的。

这是因为在这个模型中没有上面列举到那些可以做图优化的情况，但是当我们打印一下 ONNX 模型我们会发现 optimize 过后的 ONNX 模型多出一些 initializer 数组

![preview](https://pic1.zhimg.com/v2-4b9f5715724fc4e71ab6258141a97280_r.jpg)

这些数组存储的就是这个图中那些常量 OP 的具体值，通过这个处理我们就可以调用 `get_constant_nodes` 函数来获取 ONNX 模型的常量 OP

这个函数的详细解释如下：

```python
def get_constant_nodes(m: onnx.ModelProto) -> List[onnx.NodeProto]:
    const_nodes = []
    # 如果节点的 name 在 ONNX 的 GraphProto 的 initializer 数组里面，他就是静态的 tensor
    const_tensors = [x.name for x in m.graph.initializer]
    # 显示的常量 OP 也加进来
    const_tensors.extend([node.output[0] for node in m.graph.node if node.op_type=='Constant'])
    # 一些节点的输出 shape 是由输入节点决定的，我们认为这个节点的输出 shape并不是常量
    # 所以我们不需要简化这种节点
    dynamic_tensors = []
    # 判断是否为动态 OP
    def is_dynamic(node):
        if node.op_type in ['NonMaxSuppression', 'NonZero', 'Unique'] and node.input[0] not in const_tensors:
            return True
        if node.op_type in ['Reshape', 'Expand', 'Upsample', 'ConstantOfShape'] and len(node.input) > 1 and node.input[1] not in const_tensors:
            return True
        if node.op_type in ['Resize'] and ((len(node.input) > 2 and node.input[2] not in const_tensors) or (len(node.input) > 3 and node.input[3] not in const_tensors)):
            return True
        return False
    
    for node in m.graph.node:
        if any(x in dynamic_tensors for x in node.input):
            dynamic_tensors.extend(node.output)
        elif node.op_type == 'Shape':
            const_nodes.append(node)
            const_tensors.extend(node.output)
        elif is_dynamic(node):
            dynamic_tensors.extend(node.output)
        elif all([x in const_tensors for x in node.input]):
            const_nodes.append(node)
            const_tensors.extend(node.output)
    # 深拷贝
    return copy.deepcopy(const_nodes)
```

在这个例子中，打印一下执行这个获取常量 OP 函数之后，Graph 中有哪些 OP 被看成了常量 OP。

<img src="https://pic1.zhimg.com/80/v2-3661ffbff61c92adb5cf8c3450d63470_1440w.jpg" alt="img" style="zoom:50%;" />

获取模型中所有的常量 OP 之后，我们**需要把所有的静态节点扩展到 ONNX Graph 的输出节点列表**中，然后利用 onnxruntime 执行一次 forward：

```python
def forward_for_node_outputs(model:onnx.ModelProto, nodes:List[onnx.NodeProto],
                            input_shapes:Optional[TensorShapes]=None) -> Dict[str, np.ndarray]:
    if input_shapes is None:
        input_shapes = {}
    model = copy.deepcopy(model)
    # nodes 是 Graph 中所有静态 OP
    add_features_to_output(model, nodes)
    res = forward(model, input_shapes=input_shapes)
    return res
```

其中 `add_features_to_output`的定义如下：

```python
def add_features_to_output(m:onnx.ModelProto, nodes:List[onnx.NodeProto]) -> Node:
    """
    Add features to output in pb, so that ONNX Runtime will output them.
    :Param m: the model that will be run in ONNX Runtime
    :Param nodes: nodes whose outputs will be added into the Graph outputs
    """
    # ONNX 模型的 graph 扩展输出节点，获取所有静态 OP 的输出和原始输出节点的输出
    for node in nodes:
        for output in node.output:
            m.graph.output.extend([onnx.ValueInfoProto(name=output)])
 
```

最后的 forward 函数就是利用 onnxruntime 推理获得我们指定的输出节点的值。这个函数这里不进行解释。推理完成之后，进入下一个函数 `clean_constant_nodes`，和这个函数定义如下：

```python
def clean_constant_nodes(const_nodes: List[onnx.NodeProto], res: Dict[str, np.ndarray]):
    """
    It seems not needed since commit 6f2a72, but maybe it still prevents some unknown bug
    :param const_nodes: const nodes detected by `get_constant_nodes`
    :param res: The dict containing all tensors, got by `forward_all`
    :return: The constant nodes which have an output in res
    """
    return [node for node in const_nodes if node.output[0] in res]
```

这个函数是用来清洗那些没有被onnxruntime推理的静态节点，但通过上面的optimize逻辑，我们的graph中其实已经不存在这个情况了（没有被onnxruntime推理的静态节点在图优化阶段会被**优化掉**），因此这个函数理论上是可以删除的。这个地方是为了避免删除掉有可能引发其它问题就保留了。

接下来就是这个 `onnx-simplifier` 最核心的步骤了，即将常量节点从原始的 ONNX Graph 中移除，函数接口为 `eliminate_const_nodes`：

```python
def eliminate_const_nodes(model:onnx.ModelProto, const_nods:List[onnx.NodeProto],
                         res:Dict[str, np.ndarray]) -> onnx.ModelProto:
    """
    :model参数：原始ONNX模型
    :const_nodes参数：使用`get_constant_nodes`获取的静态OP
    :res参数：包括所有输出Tensor的字典
    :return：简化后的模型，所有冗余操作都已删除
    """
    for i, node in enumerate(model.graph.node):
        if node in const_nodes:
            for output in node.output:
                new_node = copy.deepcopy(node)
                new_node.name = "node_" + output
                new_node.op_type ='Constant'
                new_attr = onnx.helper.make_attribute(
                	'value',
                    onnx.numpy_helper.from_array(res[output], name=output)
                )
                # 删除
                del new_node.input[:]
                del new_node.attribute[:]
                del new_node.output[:]
                # 换血
                new_node.output.extend([output])
                new_node.attribute.extend([new_attr])
                insert_elem(mode.graph,node, i+1, new_node)
            del mode.graph.node[i]
    return model
```

运行这个函数之后我们获得的ONNX模型可视化结果是这样子的：

![img](https://pic1.zhimg.com/80/v2-3728342cdb46d90a008f5461dae6be28_1440w.jpg)

注意，这里获得的ONNX模型中虽然常量节点已经从Graph中断开了，即相当于这个DAG里面多了一些单独的点，但是这些点还是存在的。因此，我们再执行一次`optimize`就可以获得最终简化后的ONNX模型了。最终简化后的ONNX模型如下图所示：

![img](https://pic1.zhimg.com/80/v2-88f078117c8ff35e463ec0b721afb32c_1440w.jpg)



> 常用我的 onnx simplifier（简称 onnxsim） 的小伙伴可能知道，onnxsim 本身只提供 constant folding/propagation（即消除结果恒为常量的算子）的能力，而图变换（即合并 conv 和 bn 等等）的能力是由 onnxsim 调用 onnx optimizer 的各种 pass 实现的。constant folding 和图变换同时使用时，很多隐藏的优化机会会被挖掘出来，这也是 onnxsim 优化效果出色的原因之一。例如 add(add(x, 1), 2) 在变换为 add(x, add(1, 2)) 之后就可以通过 constant folding 变为 add(x, 3)，而 pad(conv(x, w, padding=0), add(1, 1)) 在经过 constant folding 变为 pad(conv(x, w, padding=0), 2) 后，就可以进一步融合成 conv(x, w, padding=2)。
>
> from -- https://zhuanlan.zhihu.com/p/350702340

## 4、ONNX 模型部署常见问题

> https://zhuanlan.zhihu.com/p/350833729

### 4.1 导出 ONNX

这里以 Pytorch 为例

```python
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = torch.load('test.pth') 
batch_size = 1
input_shape = (3,224, 224)

# set the model to inference mode
model.eval()

x = torch.randn(batch_size, *input_shape)
x = x.to(device)

export_onnx_file = "test.onnx"
torch.onnx.export(mode,
                 x,
                 export_onnx_file,
                 opset_version=10,
                 do_constant_folding=True,  # 是否执行常量折叠优化
                  input_names = ["input"],
                  output_names = ["output"],
                  dynamic_axes={"input":{0:"batch_size"},
                               "output":{0:"batch_size"}}
                 )
```

可以看到Pytorch提供了一个ONNX模型导出的专用接口，只需要配置好相关的模型和参数就可以完成自动导出ONNX模型的操作了。代码相关细节请自行查看，这里来列举几个导出ONNX模型中应该注意的问题。

#### 4.1.1 自定义 OP 问题

以 Yolov5 为例，在模型的 BackBone 部分定义了一个 Focus OP，这个 OP的代码实现为：

```python
class Focus(nn.Module):
    # Focus wh information into c-space
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super(Focus, self).__init__()
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)
        # self.contract = Contract(gain=2)

    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)
        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))
        # return self.conv(self.contract(x))
```

这个操作就是一个stride slice然后再concat的操作，类似于PixelShuffle的逆向过程。下面是把YOLOv5模型导出ONNX模型之后Focus层的可视化结果。

<img src="https://pic3.zhimg.com/80/v2-ef666e2e58a2a0380a78bb9b7ffd531e_1440w.jpg" alt="img" style="zoom:77%;" />

可以看到这个 OP 在使用 Pytorch 导出 ONNX 的过程中被拆成了很多更小的操作，这个时候 Focus OP 的问题就是推理的效率比较低并且拆成的小 OP 各个推理框架的支持程度不一致。

要解决这个问题：

1. 在前向推理框架中实现一个自定以的 Focus Op，例如[ncnn](https://github.com/Tencent/ncnn/blob/master/examples/yolov5.cpp#L24)
2. 要么将这个 OP 使用其他的操作近似代替，比如这里可以使用一个 `stride` 为 2 的卷积OP 来替代 Focus 结构，注意代替之后有可能准确率下降，需要做精度和部署友好性的平衡

综上，自定义的 OP 在导出 ONNX 进行部署是，处理考虑 ONNX 模型的执行效率问题，还要考虑框架是否支持的问题。想要快速迭代产品，建议尽量以一些经典结构为基础，尽量少引入自定义 OP。

#### 4.1.2 后处理问题

如果我们要导出检测网络的 ONNX 模型进行部署，就会碰到这个问题，**后处理部分是否要导入到 ONNX 模型？**

**我们在 Pytorch 导出 ONNX 模型时，所有的 Aten 操作都会被 ONNX 记录下来，称为一个 DAG。然后 ONNX 会根据这个 DAG 的输出节点来反推这个 DAG 有哪些节点有用的，这样获得的就是最终的 ONNX 模型。**而后处理，比如非极大值抑制也是通过 Aten 操作拼起来，所谓 ATEN 操作就是 Pytorch 中的基础算术单元比如 加减乘除，所有的 OP 以及和 Tensor 相关的操作都是基于 Aten 中的操作。

比如检测网络 Yolov3 的后处理就是 NMS，代码示例如 `https://github.com/ultralytics/yolov3/blob/master/utils/general.py#L325`。当我们完成检测网络的训练之后直接导出ONNX模型我们就会发现NMS这个实现也全部被导入了ONNX，如下图所示：

![img](https://pic2.zhimg.com/80/v2-c573db603377a05d26617c1cf2e3f361_1440w.jpg)

这个结构非常复杂，我们要在实际业务中去部署这个模型难度是很大的。另外，刚才我们提到ONNX模型只能记录Pytorch中的Aten操作，对其它的一些逻辑运算符比如`if`是无能为力的（意思是不能记录if的多个子图），而后处理过程中根据置信度阈值来筛选目标框是常规操作。如果我们在导出ONNX模型时是随机输入或者没有指定目标的图片就会导致这个ONNX记录下来的DAG可能有缺失。最后，每个人实现后处理的方式可能都是不一样的，这也增加了ONNX模型部署的难度。**为了部署的友好性和降低转换过程中的风险，后处理过程最好由读者自己完成，我们只需要导出模型的Backbone和Neck部分为ONNX**。

具体来说，我们只需要在Pytorch的代码实现中**屏蔽掉后处理部分然后导出ONNX模型**即可。这也是目前使用ONNX部署检测模型的通用方案。

所以，针对后处理问题，我们的结论就是在使用ONNX进行部署时直接屏蔽后处理，将后处理单独拿出来处理。

#### 4.1.3 胶水 OP 问题

在导出 ONNX 模型的过程中，经常会带来一些胶水 OP，比如 Gather，Shape 等等。例如

```python
import torch

class JustReshape(torch.nn.Module):
    def __init__(self):
        super(JustReshape, self).__init__()

    def forward(self, x):
        return x.view((x.shape[0], x.shape[1], x.shape[3], x.shape[2]))
  
net = JustReshape()
model_name = '../model/just_reshape.onnx'
dummy_input = torch.randn(2, 3, 4, 5)
torch.onnx.export(net, dummy_input, model_name, input_names=['input'], output_names=['output'])
```

这个时候的做法一般就是过一遍onnx-simplifer，可以去除这些胶水OP获得一个简化后的模型。

综上，我们在导出 ONNX 模型的一般流程就是：

- 去掉后处理
- 尽量不引入自定义 OP
- 导出 ONNX 模型，然后过一遍 `onnx-simplifier`

这样就可以获得一个精简的易于部署的 ONNX 模型

### 4.2 ONNX or Caffe?

我们还是以Pytorch为例，要把Pytorch模型通过TensorRT部署到GPU上，一般就是Pytorch->Caffe->TensorRT以及Pytorch->ONNX->TensorRT（当然Pytorch也是支持直接转换到TensorRT，这里不关心）。那么这里就有一个问题，**我们选择哪一条路比较好**？

们要再提一下上面那个`if`的事情了，假设现在有一个新的SOTA模型被提出，这个模型有一个自定义的OP，作者是用Pytorch的Aten操作拼的，逻辑大概是这样：

```python
result = check()
if result == 0:
 result = algorithm1(result)
else:
 result = algorithm2(result)
return result
```

然后考虑将这个模型导出ONNX或者转换为Caffe，如果是Caffe的话我们需要去实现这个自定义的OP，并将其注册到Caffe的OP管理文件中，虽然这里比较繁琐，但是我们可以将`if`操作隐藏在这个大的OP内部，这个`if`操作可以保留下来。而如果我们通过导出ONNX模型的方式`if`子图只能保留一部分，要么保留algorithm1，要么保留algorithm2对应的子图，这种情况ONNX似乎就没办法处理了。这个时候要么保存两个ONNX模型，要么修改算法逻辑绕过这个问题。从这里引申一下，如果我们碰到**有递归关系的网络，基于ONNX应当怎么部署**？ONNX还有一个缺点就是OP的细粒度太细，执行效率低，不过ONNX已经推出了多种化方法可以将OP的细粒度变粗，提高模型执行效率。目前在众多经典算法上，ONNX已经支持得非常好了。

### 4.3 一些典型的坑点及解决方法

第一节已经提到，将我们的ONNX模型过一遍onnx-simplifer之后就可以去掉胶水OP并将一些细粒度的OP进行op fuse成粗粒度的OP，并解决一部分由于Pytorch和ONNX OP实现方式不一致而导致模型变复杂的问题。除了这些问题，本节再列举一些ONNX模型部署中容易碰到的坑点，并尝试给出一些解决办法。

#### 4.3.1 预处理问题

和后处理对应的还有预处理问题，如果在 Pytorch 中使用下面代码导出 ONNX 模型

```python
import torch


class JustReshape(torch.nn.Module):
    def __init__(self):
        super(JustReshape, self).__init__()
        self.mean = torch.randn(2, 3, 4, 5)
        self.std = torch.randn(2, 3, 4, 5)

    def forward(self, x):
        x = (x - self.mean) / self.std
        return x.view((x.shape[0], x.shape[1], x.shape[3], x.shape[2]))


net = JustReshape()
model_name = '../model/just_reshape.onnx'
dummy_input = torch.randn(2, 3, 4, 5)
torch.onnx.export(net, dummy_input, model_name, input_names=['input'], output_names=['output'])
```

我们先给这个ONNX模型过一遍onnx-simplifer，然后使用Netron可视化之后模型大概长这样：

<img src="https://pic3.zhimg.com/80/v2-9192db1464b48b0313c76a1e879ac68a_1440w.jpg" alt="img" style="zoom:80%;" />

如果我们要把这个模型放到NPU上部署，如果NPU芯片不支持Sub和Div的量化计算，那么这两个操作会被回退到NPU上进行计算，这显然是不合理的，因为我们总是想网络在NPU上能一镜到底，中间断开必定会影响模型效率，所以这里的解决办法就是把预处理放在基于`nn.Module`搭建模型的代码之外，然后推理的时候先把预处理做掉即可。

#### 4.3.2 框架 OP 实现不一致问题

当从Mxnet转换模型到ONNX时，如果模型是带有PReLU OP的（在人脸识别网络很常见），就是一个大坑了。主要有两个问题，当从mxnet转为ONNX时，PReLU的slope参数维度可能会导致onnxruntime推理时失败，报错大概长这样：

```python
2）[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running PRelu node. Name:'conv_1_relu'...... Attempting to broadcast an axis by a dimension other than 1. 56 by 64 
```

这个错误产生的原因可能是MxNet的版本问题（`https://github.com/apache/incubator-mxnet/issues/17821`），这个时候的解决办法就是：修改PRelu层的slope参数的shape，不仅包括type参数，对应的slope值也要修改来和shape对应。

核心代码如下：

```python
graph.input.remove(input_map[input_name])
new_nv = helper.make_tensor_value_info(input_name, TensorProto.FLOAT, [input_dim_val,1,1])
graph.input.extend([new_nv])
```

下一个问题就是如果我们将处理好之后的ONNX通过TensorRT进行部署的话，**我们会发现TensorRT不支持PReLU这个OP，这个时候解决办法要么是TensorRT自定义PReLU插件，但是这种方法会打破TensorRT中conv+bn+relu的op fusion，速度会变慢，并且如果要做量化部署似乎是不可行的。所以这个时候一般会采用另外一种解决办法，使用relu和scale op来组合成PReLU**，如下图所示：

![img](https://pic4.zhimg.com/80/v2-fab2b7251aa428c1a62751d368e1e5df_1440w.jpg)

所以，我们在onnx模型中只需要按照这种方法将PReLU节点进行等价替换就可以了。

这个地方以PReLU列举了一个框架OP实现不一致的问题，比如大老师最新文章也介绍的就是squeeze OP在Pytorch和ONNX实现时的不一致导致ONNX模型变得很复杂，这种问题感觉是基于ONNX支持模型部署时的常见问题，虽然onnx-simplifier已经解决了一些问题，但也不能够完全解决。

#### 4.3.2 batch 维度缺失

使用 tf2onnx 工具将 TensorFlow 模型转为 ONNX 模型时，模型的输入 batch 维度没有被设置，我们需要手动添加。

```python
# 为onnx模型增加batch维度
    def set_model_input_batch(self, index=0, name=None, batch_size=4):
        model_input = None
        if name is not None:
            for ipt in self.model.graph.input:
                if ipt.name == name:
                    model_input = ipt
        else:
            model_input = self.model.graph.input[index]
        if model_input:
            tensor_dim = model_input.type.tensor_type.shape.dim
            tensor_dim[0].ClearField("dim_param")
            tensor_dim[0].dim_value = batch_size
        else:
            print('get model input failed, check index or name')
```

### 4.4 ONNXRuntime 介绍及用法

ONNXRuntime 是微软推出的一个推理架构，可以运行ONNX模型。支持多种运行后端包括 CPU，GPU，TensorRT，DML等。

```python
import numpy as np
import onnx
import onnxruntime as ort

image = cv2.imread("image.jpg")
image = np.expand_dims(image, axis=0)

onnx_model = onnx.load_model("resnet18.onnx")
sess = ort.inferenceSession(onnx_model.SerializeToString)
sess.set_providers(['CPUExcutionProvider'])
input_name = sess.get_inputs()[0].name
output_name = sess.get_outputs()[0].name

output = sess.run([output_name], {input_name:image_data})
prob = np.squeeze(output[0])
print("predicting label: ", np.argmax(prb))
```

























